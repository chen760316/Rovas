"""
ugly outliersçš„ä¿®å¤
åˆ†ç±»å™¨ä¸ºlinearæ ¸çš„svmåˆ†ç±»å™¨
ä¸€ã€åŽ»é™¤è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼å’Œåˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽæœªèƒ½æé«˜æµ‹è¯•é›†ä¸Šåˆ†ç±»å‡†ç¡®åº¦çš„å¯èƒ½åŽŸå› ï¼š
1ã€æ•°æ®é›†çš„å¤æ‚æ€§ï¼šå¼‚å¸¸å€¼çš„æ¯”ä¾‹è¾ƒå°ï¼Œæˆ–è€…æ•°æ®æœ¬èº«çš„åˆ†å¸ƒå’Œç‰¹å¾è¾ƒä¸ºå¤æ‚ï¼Œåˆ é™¤è¿™äº›æ ·æœ¬å¯¹æ•´ä½“å‡†ç¡®åº¦çš„æå‡å¯èƒ½ä¸æ˜¾è‘—
2ã€æ ·æœ¬çš„ä»£è¡¨æ€§: å¦‚æžœæ•°æ®é›†ä¸­çš„å¤§å¤šæ•°æ ·æœ¬æ˜¯æ­£å¸¸çš„æˆ–åˆç†çš„ï¼Œå³ä½¿åˆ é™¤äº†å°‘é‡å¼‚å¸¸æ ·æœ¬ï¼Œæ¨¡åž‹çš„å­¦ä¹ æ•ˆæžœå’Œå‡†ç¡®åº¦æå‡ä¹Ÿå¯èƒ½æœ‰é™
3ã€æ¨¡åž‹çš„è¡¨çŽ°ä¸Žæ•°æ®çš„å…³ç³»ï¼šå¦‚æžœæ¨¡åž‹çš„è¶…å‚æ•°ï¼ˆå¦‚ C å’Œ gammaï¼‰æ²¡æœ‰ç»è¿‡ä¼˜åŒ–ï¼Œæˆ–æ•°æ®ç‰¹å¾ä¸å……åˆ†ï¼Œé‡æ–°è®­ç»ƒçš„æ¨¡åž‹å¯èƒ½ä¸ä¼šæ¯”åŽŸå§‹æ¨¡åž‹æœ‰æ˜¾è‘—çš„æ”¹è¿›
4ã€å¼‚å¸¸å€¼çš„ç±»åž‹: å¦‚æžœå¼‚å¸¸å€¼å¯¹æ¨¡åž‹çš„å½±å“ä¸å¤§ï¼Œæˆ–è€…å¼‚å¸¸å€¼åœ¨æµ‹è¯•é›†ä¸­çš„è¡¨çŽ°ä¸Žè®­ç»ƒé›†ä¸­çš„è¡¨çŽ°ç›¸ä¼¼ï¼Œåˆ é™¤è¿™äº›å¼‚å¸¸å€¼å¯èƒ½ä¸ä¼šå¯¹æ¨¡åž‹çš„æ€»ä½“æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“
5ã€æ ·æœ¬é”™è¯¯: è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯æ ·æœ¬å¯èƒ½æ˜¯ç”±æ ‡ç­¾é”™è¯¯å¼•èµ·çš„ï¼Œè€Œä¸æ˜¯æ¨¡åž‹æ— æ³•å¤„ç†çš„æ•°æ®ç‰¹æ€§ã€‚å¦‚æžœæ ‡ç­¾æœ¬èº«ä¸å‡†ç¡®ï¼Œåˆ é™¤è¿™äº›é”™è¯¯æ ·æœ¬å¯èƒ½ä¸ä¼šæé«˜æ¨¡åž‹çš„æ€§èƒ½
6ã€è®­ç»ƒæ ·æœ¬çš„æ•°é‡: åˆ é™¤æ ·æœ¬å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒé›†æ ·æœ¬é‡å‡å°‘ã€‚å¦‚æžœåˆ é™¤çš„æ ·æœ¬æ¯”ä¾‹è¾ƒé«˜ï¼Œå¯èƒ½ä¼šå½±å“è®­ç»ƒé›†çš„ä»£è¡¨æ€§ï¼Œè¿›è€Œå½±å“æ¨¡åž‹çš„æ€§èƒ½ã€‚
7ã€è¿‡æ‹Ÿåˆé—®é¢˜: å¦‚æžœåœ¨å¤„ç†å¼‚å¸¸å€¼å’Œé”™è¯¯æ ·æœ¬æ—¶è¿‡åº¦è°ƒæ•´è®­ç»ƒæ•°æ®ï¼Œå¯èƒ½å¯¼è‡´æ¨¡åž‹è¿‡æ‹Ÿåˆï¼Œä»Žè€Œåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—æå‡ã€‚
8ã€è¯„ä¼°æ ‡å‡†: ä½¿ç”¨åˆ†ç±»å‡†ç¡®åº¦ä½œä¸ºæ€§èƒ½è¯„ä¼°æ ‡å‡†å¯èƒ½ä¸æ€»æ˜¯æœ€åˆé€‚çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é›†ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚è€ƒè™‘ä½¿ç”¨å…¶ä»–è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚F1åˆ†æ•°ã€ROC-AUCç­‰ï¼Œæ¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡åž‹æ€§èƒ½ã€‚
"""
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

def calculate_made(data):
    median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42

# section æ•°æ®é¢„å¤„ç†
file_path = "../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŽŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
# print("X_train åŽŸå§‹ç´¢å¼•:", train_indices)
# print("X_test åŽŸå§‹ç´¢å¼•:", test_indices)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
# å°† X å’Œ y ç»„åˆä¸ºä¸€ä¸ª numpy æ•°ç»„
combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# åˆ›å»ºæ–°çš„ DataFrame
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ•´æ•°ç¼–ç 
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# SECTION Mð‘œ (ð‘¡, D)
# subsection é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOAD
clf_gold = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf_gold.fit(X_train, y=None)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
clf = clf_gold
train_scores = clf.decision_function(X_train)
train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))

# SECTION SVMæ¨¡åž‹çš„å®žçŽ°
# svm_model = svm.SVC()
svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
# è¢«SVMæ¨¡åž‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]

# SUBSECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šè®¾ç½®é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
bad_samples = np.where(hinge_loss > 1)[0]
soft_outliers = np.where((hinge_loss > 0) & (hinge_loss <= 1))[0]
correct_samples = np.where(hinge_loss == 0)[0]
print("æŸå¤±å‡½æ•°é«˜äºŽæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", bad_samples)

# subsection åˆ¤å®šè®­ç»ƒæ•°æ®ä¸­å¼‚å¸¸å€¼å¯èƒ½å¯¼è‡´åˆ†ç±»é”™è¯¯çš„æ ·æœ¬
# è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¯¼è‡´SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
inter_outliers = list(set(train_outliers_index) & set(bad_samples))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä½†æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®ä¸è¶³é˜ˆå€¼1
inter_soft_outliers = list(set(train_outliers_index) & set(soft_outliers))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä¸”æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®è¶…è¿‡é˜ˆå€¼1
inter_correct_class = list(set(train_outliers_index) & set(correct_samples))
# è®­ç»ƒæ•°æ®ä¸­è¢«SVMè¯¯åˆ†ç±»çš„æ ·æœ¬ä¸Žè®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼çš„äº¤é›†
intersection = np.intersect1d(bad_samples, wrong_classified_indices)
# è®­ç»ƒæ•°æ®ä¸­è¢«SVMè¯¯åˆ†ç±»çš„æ ·æœ¬ä¸­æœªè¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬
diff_elements = np.setdiff1d(wrong_classified_indices, intersection)
print("åˆ†ç±»é”™è¯¯çš„æ ·æœ¬ä¸­æœªè¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ç´¢å¼•ï¼š", diff_elements)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°é«˜äºŽé˜ˆå€¼1çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_outliers)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°åœ¨0å’Œé˜ˆå€¼1ä¹‹é—´çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_soft_outliers)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°ä¸º0çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_correct_class)

# SECTION åŽŸå§‹æ•°æ®ä¸­çš„svmåœ¨å„ç±»è¯„ä»·æŒ‡æ ‡ä¸‹çš„è¡¨çŽ°
print("*" * 100)

# subsection è®¡ç®—SVMçš„åˆ†ç±»å‡†ç¡®åº¦
# å‡†ç¡®åº¦æ˜¯æŒ‡æ¨¡åž‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹
print("åŽŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŽŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))

# subsection è®¡ç®— F1 åˆ†æ•°
# F1åˆ†æ•°æ˜¯ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡çš„è°ƒå’Œå¹³å‡å€¼ï¼Œé€‚ç”¨äºŽç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# ç²¾ç¡®çŽ‡ï¼ˆPrecisionï¼‰æ˜¯æ­£ç¡®åˆ†ç±»ä¸ºæ­£ç±»çš„æ¯”ä¾‹ï¼Œè€Œå¬å›žçŽ‡ï¼ˆRecallï¼‰æ˜¯æ‰€æœ‰å®žé™…æ­£ç±»ä¸­è¢«æ­£ç¡®åˆ†ç±»çš„æ¯”ä¾‹ã€‚F1åˆ†æ•°ç»¼åˆè€ƒè™‘äº†è¿™ä¸¤ä¸ªæŒ‡æ ‡ã€‚
# average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºŽå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºŽéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
# average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºŽç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
# average=None: è¿”å›žæ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºŽè¯¦ç»†åˆ†æžæ¯ä¸ªç±»åˆ«çš„è¡¨çŽ°ã€‚
f1_train = f1_score(y_train, svm_model.predict(X_train), average='weighted')
f1_test = f1_score(y_test, svm_model.predict(X_test), average='weighted')
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train))
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test))

# subsection è®¡ç®—ROC-AUCåˆ†æ•°
# å¯¹äºŽäºŒåˆ†ç±»é—®é¢˜ï¼Œy_score æ˜¯æ¨¡åž‹å¯¹æ ·æœ¬ä¸ºæ­£ç±»çš„æ¦‚çŽ‡ä¼°è®¡
# ROCæ›²çº¿ç»˜åˆ¶çš„æ˜¯ä¸åŒé˜ˆå€¼ä¸‹çš„çœŸé˜³æ€§çŽ‡ï¼ˆå¬å›žçŽ‡ï¼‰ä¸Žå‡é˜³æ€§çŽ‡ï¼ˆ1 - ç‰¹å¼‚æ€§ï¼‰çš„å…³ç³»ã€‚
# AUCï¼ˆArea Under the Curveï¼‰åˆ™æ˜¯ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œå€¼è¶ŠæŽ¥è¿‘1è¡¨ç¤ºæ¨¡åž‹çš„æ€§èƒ½è¶Šå¥½
# ç¡®ä¿ä½¿ç”¨ predict_proba èŽ·å–é¢„æµ‹æ¦‚çŽ‡
y_prob_train = svm_model.predict_proba(X_train)  # è¿”å›žæ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡
y_prob_test = svm_model.predict_proba(X_test)
roc_auc_train = roc_auc_score(y_train, y_prob_train, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
roc_auc_test = roc_auc_score(y_test, y_prob_test, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_train))
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

# subsection è®¡ç®—ç²¾ç¡®çŽ‡
# ç²¾ç¡®çŽ‡æ˜¯æŒ‡æ¨¡åž‹é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­æœ‰å¤šå°‘æ˜¯çœŸæ­£çš„æ­£ç±»ã€‚é€‚ç”¨äºŽå…³æ³¨å‡é˜³æ€§è¾ƒå°‘çš„åœºæ™¯ã€‚
# average='micro': è®¡ç®—å…¨å±€ç²¾ç¡®çŽ‡ã€‚å¯¹æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æžœè¿›è¡Œæ±‡æ€»ï¼Œç„¶åŽè®¡ç®—ç²¾ç¡®çŽ‡ã€‚
# average='macro': è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œç„¶åŽå–è¿™äº›ç²¾ç¡®çŽ‡çš„ç®€å•å¹³å‡ã€‚é€‚ç”¨äºŽä¸å…³æ³¨ç±»åˆ«æ ·æœ¬é‡çš„æƒ…å†µã€‚
# average='weighted': è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œç„¶åŽæŒ‰ç±»åˆ«æ ·æœ¬é‡åŠ æƒå¹³å‡ã€‚é€‚ç”¨äºŽç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average=None: è¿”å›žæ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œå¯ä»¥å¸®åŠ©ä½ è¯¦ç»†äº†è§£æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»æ€§èƒ½ã€‚
from sklearn.metrics import precision_score
# è®¡ç®—ç²¾ç¡®çŽ‡
precision_train = precision_score(y_train, svm_model.predict(X_train), average='weighted')
precision_test = precision_score(y_test, svm_model.predict(X_test), average='weighted')
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_train))
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_test))

# subsection å¬å›žçŽ‡ (Recall)
# å¬å›žçŽ‡æ˜¯æŒ‡æ‰€æœ‰çœŸæ­£çš„æ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ç±»çš„æ¯”ä¾‹ã€‚é€‚ç”¨äºŽå…³æ³¨å‡é˜´æ€§è¾ƒå°‘çš„åœºæ™¯ã€‚
# average='micro': è®¡ç®—å…¨å±€ç²¾ç¡®çŽ‡ã€‚å¯¹æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æžœè¿›è¡Œæ±‡æ€»ï¼Œç„¶åŽè®¡ç®—ç²¾ç¡®çŽ‡ã€‚
# average='macro': è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œç„¶åŽå–è¿™äº›ç²¾ç¡®çŽ‡çš„ç®€å•å¹³å‡ã€‚é€‚ç”¨äºŽä¸å…³æ³¨ç±»åˆ«æ ·æœ¬é‡çš„æƒ…å†µã€‚
# average='weighted': è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œç„¶åŽæŒ‰ç±»åˆ«æ ·æœ¬é‡åŠ æƒå¹³å‡ã€‚é€‚ç”¨äºŽç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average=None: è¿”å›žæ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®çŽ‡ï¼Œå¯ä»¥å¸®åŠ©ä½ è¯¦ç»†äº†è§£æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»æ€§èƒ½ã€‚
from sklearn.metrics import recall_score
# è®¡ç®—å¬å›žçŽ‡
recall_train = recall_score(y_train, svm_model.predict(X_train), average='weighted')
recall_test = recall_score(y_test, svm_model.predict(X_test), average='weighted')
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_train))
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_test))

# subsection æ··æ·†çŸ©é˜µ (Confusion Matrix)
# æ··æ·†çŸ©é˜µæä¾›äº†çœŸæ­£ä¾‹ã€å‡æ­£ä¾‹ã€çœŸè´Ÿä¾‹å’Œå‡è´Ÿä¾‹çš„è¯¦ç»†è®¡æ•°ï¼Œå¸®åŠ©ç†è§£æ¨¡åž‹çš„åˆ†ç±»è¡¨çŽ°ã€‚
from sklearn.metrics import confusion_matrix
# è®¡ç®—æ··æ·†çŸ©é˜µ
conf_matrix_train = confusion_matrix(y_train, svm_model.predict(X_train))
conf_matrix_test = confusion_matrix(y_test, svm_model.predict(X_test))
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_train)
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_test)

# subsection PRæ›²çº¿ (Precision-Recall Curve)
# PRæ›²çº¿å±•ç¤ºäº†ç²¾ç¡®çŽ‡ä¸Žå¬å›žçŽ‡ä¹‹é—´çš„æƒè¡¡ï¼Œç‰¹åˆ«é€‚ç”¨äºŽå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
# è®¡ç®— PR æ›²çº¿
y_pred_train = svm_model.predict(X_train)
y_pred_test = svm_model.predict(X_test)
precision_train, recall_train, _, _ = precision_recall_fscore_support(y_train, y_pred_train, average='macro')
precision_test, recall_test, _, _ = precision_recall_fscore_support(y_test, y_pred_test, average='macro')
# precision_train, recall_train, _ = precision_recall_curve(y_train, svm_model.decision_function(X_train))
# precision_test, recall_test, _ = precision_recall_curve(y_test, svm_model.decision_function(X_test))
# ç»˜åˆ¶ PR æ›²çº¿
plt.figure()
plt.plot(recall_train, precision_train, color='blue', label='Train PR curve')
plt.plot(recall_test, precision_test, color='red', label='Test PR curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
# Adding a grid can help in visualization
plt.grid(True)
plt.show()

print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­åˆ†ç±»é”™è¯¯ï¼ˆhingeæŸå¤±å‡½æ•°é«˜äºŽ1ï¼‰ä¸”è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
print("*" * 100)

# ç”Ÿæˆå¸ƒå°”ç´¢å¼•ï¼Œä¸ºè¦åˆ é™¤çš„è¡Œåˆ›å»ºå¸ƒå°”å€¼æ•°ç»„
mask = np.ones(len(X_train), dtype=bool)
mask[inter_outliers] = False
# ä½¿ç”¨å¸ƒå°”ç´¢å¼•åˆ é™¤é‚£äº›æ—¢è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œåˆä½¿å¾—hingeæŸå¤±é«˜äºŽ1çš„æ ·æœ¬
X_train_split = X_train[mask]
y_train_split = y_train[mask]
# é‡æ–°è®­ç»ƒSVMæ¨¡åž‹
svm_model_split = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_split.fit(X_train_split, y_train_split)
# ä½¿ç”¨è®­ç»ƒå¥½çš„svmæ¨¡åž‹é¢„æµ‹
y_train_pred = svm_model_split.predict(X_train_split)
y_test_pred = svm_model_split.predict(X_test)

# subsection è®¡ç®— accuracyåˆ†æ•°
print("*" * 100)
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_split, y_train_pred)))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, y_test_pred)))
print("*" * 100)

# subsection è®¡ç®— F1 åˆ†æ•°
f1_train_split = f1_score(y_train_split, svm_model_split.predict(X_train_split), average='weighted')
f1_test_split = f1_score(y_test, svm_model_split.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_split))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_split))

# subsection è®¡ç®—ROC-AUCåˆ†æ•°
y_prob_train_split = svm_model_split.predict_proba(X_train_split)  # è¿”å›žæ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡
y_prob_test = svm_model_split.predict_proba(X_test)
roc_auc_train = roc_auc_score(y_train_split, y_prob_train_split, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
roc_auc_test = roc_auc_score(y_test, y_prob_test, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

# subsection è®¡ç®—ç²¾ç¡®çŽ‡
from sklearn.metrics import precision_score
precision_train = precision_score(y_train_split, svm_model_split.predict(X_train_split), average='weighted')
precision_test = precision_score(y_test, svm_model_split.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_test))

# subsection å¬å›žçŽ‡ (Recall)
from sklearn.metrics import recall_score
recall_train = recall_score(y_train_split, svm_model_split.predict(X_train_split), average='weighted')
recall_test = recall_score(y_test, svm_model_split.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_test))

# subsection æ··æ·†çŸ©é˜µ (Confusion Matrix)
from sklearn.metrics import confusion_matrix
conf_matrix_train = confusion_matrix(y_train_split, svm_model_split.predict(X_train_split))
conf_matrix_test = confusion_matrix(y_test, svm_model_split.predict(X_test))
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_train)
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_test)

# subsection PRæ›²çº¿ (Precision-Recall Curve)
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
y_pred_train_split = svm_model_split.predict(X_train_split)
y_pred_test = svm_model_split.predict(X_test)
precision_train, recall_train, _, _ = precision_recall_fscore_support(y_train_split, y_pred_train_split, average='macro')
precision_test, recall_test, _, _ = precision_recall_fscore_support(y_test, y_pred_test, average='macro')
plt.figure()
plt.plot(recall_train, precision_train, color='blue', label='Train PR curve')
plt.plot(recall_test, precision_test, color='red', label='Test PR curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# subsection è®¡ç®— Matthews Correlation Coefficient (MCC)åˆ†æ•°
from sklearn.metrics import matthews_corrcoef

mcc_train = matthews_corrcoef(y_train_split, y_train_pred)
mcc_test = matthews_corrcoef(y_test, y_test_pred)
print(f'è®­ç»ƒé›†çš„Matthews Correlation Coefficient: {mcc_train}')
print(f'æµ‹è¯•é›†çš„Matthews Correlation Coefficient: {mcc_test}')

# subsection è®¡ç®— Balanced Accuracyåˆ†æ•°
from sklearn.metrics import balanced_accuracy_score

balanced_acc_train = balanced_accuracy_score(y_train_split, y_train_pred)
balanced_acc_test = balanced_accuracy_score(y_test, y_test_pred)
print(f'è®­ç»ƒé›†çš„Balanced Accuracy: {balanced_acc_train}')
print(f'æµ‹è¯•é›†çš„Balanced Accuracy: {balanced_acc_test}')

# subsection è®¡ç®— G-Meanåˆ†æ•°
def calculate_gmean(y_true, y_pred):
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    # ä»Žæ··æ·†çŸ©é˜µä¸­æå– TPã€TNã€FPã€FN
    TP = cm[1, 1]
    TN = cm[0, 0]
    FP = cm[0, 1]
    FN = cm[1, 0]
    # è®¡ç®—çµæ•åº¦å’Œç‰¹å¼‚æ€§
    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0
    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0
    # è®¡ç®— G-Mean
    gmean = np.sqrt(sensitivity * specificity)
    return gmean

g_mean_train = calculate_gmean(y_train_split, y_train_pred)
g_mean_test = calculate_gmean(y_test, y_test_pred)
print(f'è®­ç»ƒé›†çš„g_meanåˆ†æ•°: {g_mean_train}')
print(f'æµ‹è¯•é›†çš„g_meanåˆ†æ•°: {g_mean_test}')

# subsection è®¡ç®— F-beta Scoreåˆ†æ•°
from sklearn.metrics import fbeta_score

# è®¾ç½® beta å‚æ•°
beta = 2  # ä¸¾ä¾‹ï¼Œbeta=2 è¡¨ç¤ºå¬å›žçŽ‡çš„æƒé‡æ˜¯ç²¾ç¡®åº¦çš„ 2 å€
f_beta_train = fbeta_score(y_train_split, y_train_pred, beta=beta)
f_beta_test = fbeta_score(y_test, y_test_pred, beta=beta)
print(f'è®­ç»ƒé›†çš„F-beta Score: {f_beta_train}')
print(f'æµ‹è¯•é›†çš„F-beta Score: {f_beta_test}')

# subsection è®¡ç®— Average Precision (AP)åˆ†æ•°
from sklearn.metrics import average_precision_score

# è®¡ç®— Average Precision
ap_train = average_precision_score(y_train_split, y_train_pred)
ap_test = average_precision_score(y_test, y_test_pred)
print(f'è®­ç»ƒé›†çš„Average Precision: {ap_train}')
print(f'æµ‹è¯•é›†çš„Average Precision: {ap_test}')

# subsection è®¡ç®— Area Under Precision-Recall Curve (PR AUC)é¢ç§¯
from sklearn.metrics import average_precision_score

pr_auc_train = average_precision_score(y_train_split, y_train_pred)
pr_auc_test = average_precision_score(y_test, y_test_pred)
print(f'è®­ç»ƒé›†çš„Area Under Precision-Recall Curve (PR AUC): {pr_auc_train}')
print(f'æµ‹è¯•é›†çš„Area Under Precision-Recall Curve (PR AUC): {pr_auc_test}')

print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­åˆ†ç±»é”™è¯¯ï¼ˆhingeæŸå¤±å‡½æ•°é«˜äºŽ1ï¼‰çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
print("*" * 100)

# subsection è®¡ç®— accuracyåˆ†æ•°
mask_h = np.ones(len(X_train), dtype=bool)
mask_h[bad_samples] = False
X_train_h = X_train[mask_h]
y_train_h = y_train[mask_h]
svm_model_h = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_h.fit(X_train_h, y_train_h)
print("*" * 100)
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_h, svm_model_h.predict(X_train_h))))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_h.predict(X_test))))
print("*" * 100)

# subsection è®¡ç®— F1 åˆ†æ•°
f1_train_h = f1_score(y_train_h, svm_model_h.predict(X_train_h), average='weighted')
f1_test_h = f1_score(y_test, svm_model_h.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_h))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_h))

# subsection è®¡ç®—ROC-AUCåˆ†æ•°
y_prob_train_h = svm_model_h.predict_proba(X_train_h)  # è¿”å›žæ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡
y_prob_test = svm_model_h.predict_proba(X_test)
roc_auc_train = roc_auc_score(y_train_h, y_prob_train_h, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
roc_auc_test = roc_auc_score(y_test, y_prob_test, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

# subsection è®¡ç®—ç²¾ç¡®çŽ‡
from sklearn.metrics import precision_score
precision_train = precision_score(y_train_h, svm_model_h.predict(X_train_h), average='weighted')
precision_test = precision_score(y_test, svm_model_h.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_test))

# subsection å¬å›žçŽ‡ (Recall)
from sklearn.metrics import recall_score
recall_train = recall_score(y_train_h, svm_model_h.predict(X_train_h), average='weighted')
recall_test = recall_score(y_test, svm_model_h.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_test))

# subsection æ··æ·†çŸ©é˜µ (Confusion Matrix)
from sklearn.metrics import confusion_matrix
conf_matrix_train = confusion_matrix(y_train_h, svm_model_h.predict(X_train_h))
conf_matrix_test = confusion_matrix(y_test, svm_model_h.predict(X_test))
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_train)
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_test)

# subsection PRæ›²çº¿ (Precision-Recall Curve)
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
y_pred_train_h = svm_model_h.predict(X_train_h)
y_pred_test = svm_model_h.predict(X_test)
precision_train, recall_train, _, _ = precision_recall_fscore_support(y_train_h, y_pred_train_h, average='macro')
precision_test, recall_test, _, _ = precision_recall_fscore_support(y_test, y_pred_test, average='macro')
plt.figure()
plt.plot(recall_train, precision_train, color='blue', label='Train PR curve')
plt.plot(recall_test, precision_test, color='red', label='Test PR curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
plt.grid(True)
plt.show()

print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM

print("*" * 100)

# subsection è®¡ç®— accuracyåˆ†æ•°
mask_o = np.ones(len(X_train), dtype=bool)
mask_o[train_outliers_index] = False
X_train_o = X_train[mask_o]
y_train_o = y_train[mask_o]
svm_model_o = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_o.fit(X_train_o, y_train_o)
print("*" * 100)
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_o, svm_model_o.predict(X_train_o))))
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_o.predict(X_test))))
print("*" * 100)

# subsection è®¡ç®— F1 åˆ†æ•°
f1_train_o = f1_score(y_train_o, svm_model_o.predict(X_train_o), average='weighted')
f1_test_o = f1_score(y_test, svm_model_o.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_o))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_o))

# subsection è®¡ç®—ROC-AUCåˆ†æ•°
y_prob_train_o = svm_model_o.predict_proba(X_train_o)  # è¿”å›žæ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡
y_prob_test = svm_model_o.predict_proba(X_test)
roc_auc_train = roc_auc_score(y_train_o, y_prob_train_o, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
roc_auc_test = roc_auc_score(y_test, y_prob_test, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

# subsection è®¡ç®—ç²¾ç¡®çŽ‡
from sklearn.metrics import precision_score
precision_train = precision_score(y_train_o, svm_model_o.predict(X_train_o), average='weighted')
precision_test = precision_score(y_test, svm_model_o.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»ç²¾ç¡®çŽ‡ï¼š" + str(precision_test))

# subsection å¬å›žçŽ‡ (Recall)
from sklearn.metrics import recall_score
recall_train = recall_score(y_train_o, svm_model_o.predict(X_train_o), average='weighted')
recall_test = recall_score(y_test, svm_model_o.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_train))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å¬å›žçŽ‡ï¼š" + str(recall_test))

# subsection æ··æ·†çŸ©é˜µ (Confusion Matrix)
from sklearn.metrics import confusion_matrix
conf_matrix_train = confusion_matrix(y_train_o, svm_model_o.predict(X_train_o))
conf_matrix_test = confusion_matrix(y_test, svm_model_o.predict(X_test))
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_train)
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»æ··æ·†çŸ©é˜µï¼š")
print(conf_matrix_test)

# subsection PRæ›²çº¿ (Precision-Recall Curve)
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import precision_recall_fscore_support
y_pred_train_o = svm_model_o.predict(X_train_o)
y_pred_test = svm_model_o.predict(X_test)
precision_train, recall_train, _, _ = precision_recall_fscore_support(y_train_o, y_pred_train_o, average='macro')
precision_test, recall_test, _, _ = precision_recall_fscore_support(y_test, y_pred_test, average='macro')
plt.figure()
plt.plot(recall_train, precision_train, color='blue', label='Train PR curve')
plt.plot(recall_test, precision_test, color='red', label='Test PR curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
plt.grid(True)
plt.show()

print("*" * 100)