"""
æ–¹æ¡ˆä¸€æ•ˆæœè¾ƒå¥½ï¼Œæ–¹æ¡ˆäºŒæ•ˆæœä¸å¥½
é‡‡ç”¨KNNä¿®æ”¹å¼‚å¸¸å€¼çš„æ ‡ç­¾
é‡‡ç”¨ç»Ÿè®¡æ–¹æ³•ä¿®å¤å¼‚å¸¸å€¼çš„ç‰¹å¾
ä½¿ç”¨SVMåˆ†ç±»å™¨è¿›è¡Œå®éªŒ
"""
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.ensemble import RandomForestClassifier

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section åœ¨ç‰¹å¾ä¸­æ³¨å…¥é«˜æ–¯éšæœºå™ªå£°
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.2, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
feature_names = data.columns.values.tolist()
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)
# print("è®­ç»ƒé›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", train_noise)
# print("æµ‹è¯•é›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", test_noise)

# SECTION Mğ‘œ (ğ‘¡, D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOAD
epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
clf_gold = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf_gold.fit(X_train, y=None)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹ã€‚
#  ç»è¿‡æ£€éªŒï¼ŒåŠ å…¥é«˜æ–¯å™ªå£°ä¼šå½±å“å¼‚å¸¸å€¼åˆ¤åˆ«

# subsection ä»åŸå§‹æ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºçš„å¼‚å¸¸å€¼

out_clf = clf_gold

# ä»åŸå§‹è®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

train_scores = out_clf.decision_function(X_train)
train_pred_labels, train_confidence = out_clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index)/len(X_train))

# ä»åŸå§‹æµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

test_scores = out_clf.decision_function(X_test)
test_pred_labels, test_confidence = out_clf.predict(X_test, return_confidence=True)
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
test_outliers_index = []
print("æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test))
for i in range(len(X_test)):
    if test_pred_labels[i] == 1:
        test_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index)
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index))
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index)/len(X_test))

# subsection ä»åŠ å™ªæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºçš„å¼‚å¸¸å€¼

clf_gold_noise = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf_gold_noise.fit(X_train_copy, y=None)
out_clf_noise = clf_gold_noise

# ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

train_scores_noise = out_clf_noise.decision_function(X_train_copy)
train_pred_labels_noise, train_confidence_noise = out_clf_noise.predict(X_train_copy, return_confidence=True)
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
train_outliers_index_noise = []
print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
for i in range(len(X_train_copy)):
    if train_pred_labels_noise[i] == 1:
        train_outliers_index_noise.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index_noise))
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index_noise)/len(X_train_copy))

# ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

test_scores_noise = out_clf_noise.decision_function(X_test_copy)
test_pred_labels_noise, test_confidence_noise = out_clf_noise.predict(X_test_copy, return_confidence=True)
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
test_outliers_index_noise = []
print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
for i in range(len(X_test_copy)):
    if test_pred_labels_noise[i] == 1:
        test_outliers_index_noise.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index_noise))
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index_noise)/len(X_test_copy))


# SECTION éšæœºæ£®æ—æ¨¡å‹çš„å®ç°

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„éšæœºæ£®æ—æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)

# # è®¾ç½®éšæœºæ£®æ—å‚æ•°
# params = {
#     'n_estimators': [50, 100, 150],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'max_features': ['auto', 'sqrt', 'log2']
# }
#
# # ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
# grid_search = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=5, scoring='accuracy')
# grid_search.fit(X_train, y_train)
#
# # è·å–æœ€ä½³å‚æ•°
# best_params = grid_search.best_params_
#
# # ä½¿ç”¨æœ€ä½³å‚æ•°å®šä¹‰æ–°çš„éšæœºæ£®æ—æ¨¡å‹
# rf_clf = RandomForestClassifier(random_state=42, **best_params)
#
# # è·å–æœ€ä½³æ¨¡å‹
# rf_classifier = grid_search.best_estimator_

rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42)
rf_classifier.fit(X_train, y_train)
train_label_pred = rf_classifier.predict(X_train)
test_label_pred = rf_classifier.predict(X_test)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„éšæœºæ£®æ—æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)

# # ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
# grid_search = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=5, scoring='accuracy')
# grid_search.fit(X_train, y_train)
#
# # è·å–æœ€ä½³æ¨¡å‹
# rf_classifier_noise = grid_search.best_estimator_

rf_classifier_noise = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42)
rf_classifier_noise.fit(X_train_copy, y_train)
train_label_pred_noise = rf_classifier_noise.predict(X_train_copy)
test_label_pred_noise = rf_classifier_noise.predict(X_test_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section è¯†åˆ«X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„

# å¼‚å¸¸æ£€æµ‹å™¨æ£€æµ‹å‡ºçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
train_outliers_noise = train_indices[train_outliers_index_noise]
test_outliers_noise = test_indices[test_outliers_index_noise]
outliers_noise = np.union1d(train_outliers_noise, test_outliers_noise)

# åœ¨åŠ å™ªæ•°æ®é›†D'ä¸Šè®­ç»ƒçš„éšæœºæ£®æ—æ¨¡å‹ï¼Œå…¶åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
train_wrong_clf_noise = train_indices[wrong_classified_train_indices_noise]
test_wrong_clf_noise = test_indices[wrong_classified_test_indices_noise]
wrong_clf_noise = np.union1d(train_wrong_clf_noise, test_wrong_clf_noise)

# outlierså’Œåˆ†é”™æ ·æœ¬çš„å¹¶é›†
train_union = np.union1d(train_outliers_noise, train_wrong_clf_noise)
test_union = np.union1d(test_outliers_noise, test_wrong_clf_noise)

# åŠ å™ªæ•°æ®é›†D'ä¸Šéœ€è¦ä¿®å¤çš„å€¼
# éœ€è¦ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_repair_indices = np.union1d(outliers_noise, wrong_clf_noise)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

# # section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆknnæ–¹æ³•ï¼‰
# #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰
#
# # subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾
#
# knn = KNeighborsClassifier(n_neighbors=3)
# knn.fit(X_copy_inners, y_inners)
#
# # é¢„æµ‹å¼‚å¸¸å€¼
# y_pred = knn.predict(X_copy_repair)
#
# # æ›¿æ¢å¼‚å¸¸å€¼
# y[X_copy_repair_indices] = y_pred
# y_train = y[train_indices]
# y_test = y[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
#
# rf_classifier_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42)
# rf_classifier_repair.fit(X_train_copy, y_train)
# # svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# # svm_repair.fit(X_train_copy, y_train)
# y_train_pred = rf_classifier_repair.predict(X_train_copy)
# y_test_pred = rf_classifier_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# section æ–¹æ¡ˆäºŒï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œç‰¹å¾ä¿®å¤ï¼ˆç»Ÿè®¡æ–¹æ³•ä¿®å¤ï¼‰
#  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

rf_classifier_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42)
rf_classifier_repair.fit(X_train_copy, y_train)

# subsection ç¡®å®šæœ‰å½±å“åŠ›çš„ç‰¹å¾
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)

# ç‰¹å¾æ•°å–4æˆ–6
i = 16
np.random.seed(1)
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_
explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=feature_names,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)
# predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
predict_fn = lambda x: rf_classifier.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
important_features = []
for feature_set in top_features:
    feature_long = feature_set[0]
    for feature in feature_names:
        if set(feature).issubset(set(feature_long)):
            important_features.append(feature)
            break
top_k_indices = [feature_names.index(feature_name) for feature_name in important_features]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# subsection ç¡®å®šå¼‚å¸¸å€¼ï¼Œä½¿ç”¨2MADeæ–¹æ³•ç¡®å®šæœ‰å½±å“åŠ›çš„åˆ—ä¸­çš„å¼‚å¸¸å€¼ï¼Œå°†å¼‚å¸¸å€¼çš„éƒ¨åˆ†è®¾ç½®ä¸ºnp.nan
for i in range(X_copy.shape[1]):
    if i in top_k_indices:
        column_data = X_copy[:, i]
        mean = np.mean(column_data)
        # å°†æ‰€æœ‰éœ€è¦ä¿®å¤çš„è¡Œå¯¹åº”çš„åˆ—ä½ç½®çš„å…ƒç´ æ›¿æ¢ä¸ºå‡å€¼
        intersection = X_copy_repair_indices
        X_copy[intersection, i] = mean

# subsection ä½¿ç”¨knnä¿®å¤æ‰€æœ‰è¢«æ ‡è®°ä¸ºnançš„å¼‚å¸¸ç‰¹å¾

X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]

# subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹

y_train_pred = rf_classifier_repair.predict(X_train_copy)
y_test_pred = rf_classifier_repair.predict(X_test_copy)

print("*" * 100)
# è®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«éšæœºæ£®æ—æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))