## 一、可用的实验指标

1、**accuracy（分类准确度）**：分类准确度是正确分类的样本数与总样本数的比率，**适用于类别分布相对均匀的情况**。

![image-20240919170710735](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919170710735.png)

2、**F1分数**：F1分数是精确率和召回率的调和平均值，综合考虑了这两者的表现。**特别适用于类别不平衡的情况**，F1分数能更好地反映模型性能。

![image-20240919170648213](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919170648213.png)

3、**ROC-AUC 分数**：**ROC（Receiver Operating Characteristic）曲线绘制了假正例率（FPR）与真正例率（TPR）之间的关系。AUC（Area Under the Curve）表示 ROC 曲线下的面积，值范围在 0 到 1 之间**。**用途：AUC 值越接近 1，模型性能越好，适用于二分类问题。**

4、**precision（分类精确度）**：精确度是指所有被模型预测为正例的样本中，实际为正例的比例。**高精确度表示假正例少，适用于需要降低误报的情况**。

![image-20240919170626346](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919170626346.png)

5、**recall（召回率）**：召回率是指所有实际为正例的样本中，被模型正确预测为正例的比例。**高召回率表示模型能找到大部分正例，适用于需要降低漏报的情况**。

![image-20240919170851311](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919170851311.png)

6、**Confusion Matrix（混淆矩阵）**：混淆矩阵是一个表格，显示了分类模型的预测结果与实际标签之间的关系，组成包含 TP、TN、FP、FN 的计数。**用于直观地评估模型性能，帮助理解错误分类的类型**。

![image-20240919171031089](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919171031089.png)

7、**Precision-Recall Curve （PR曲线）**：PR 曲线绘制了精确度与召回率之间的关系，**适用于不平衡数据集**。可以用于**评估不同阈值下的模型性能，特别是在正类样本较少的情况下**。

8、**Area Under Precision-Recall Curve (PR AUC)**：PR AUC 表示 PR 曲线下的面积，范围在 0 到 1 之间。**越接近 1，模型在平衡精确度和召回率方面的性能越好**。

9、**Average Precision (AP)**：AP 是精确度和召回率的加权平均，综合考虑了不同召回率下的精确度。**通过对 PR 曲线下的每个小区间求和来计算。用途：常用于多类分类任务，特别是检测任务**。AP 是一个重要的评估指标，能够**全面反映模型在不同召回率下的表现**。计算 AP 时，关注精确度与召回率的平衡，尤其是在**处理不平衡数据**时。

10、Matthews Correlation Coefficient (MCC)（不常用）：MCC 是一个用于二分类问题的指标，考虑了所有四个分类结果（TP、TN、FP、FN）。它提供了一个综合的性能衡量，范围在 -1 到 1 之间。**MCC 特别适用于类别不平衡的情况，因为它对所有类别的贡献是均等的**。MCC 为 1 表示完美预测，为 0 表示随机预测，为 -1 表示完全不一致。

![image-20240919171750288](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919171750288.png)

11、F-beta Score（不常用）：F-beta 分数是精确度和召回率的调和平均数，但与 F1 分数不同的是，**F-beta 允许根据具体需求调整精确度和召回率的权重**。β 值决定了两个指标的重要性。当 β > 1 时，召回率比精确度更重要。当 β < 1 时，精确度比召回率更重要。**适用于需要在精确度和召回率之间进行权衡的场景**。

![image-20240919171847386](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919171847386.png)

12、Balanced Accuracy（不常用）：平衡准确率是针对类别不平衡问题的修改版本，它是各类真正例率（召回率）的平均值。其中，Sensitivity（灵敏度）是正类的召回率，Specificity（特异性）是负类的召回率。**适用于类别分布不均衡的情况，提供比简单准确率更有意义的评估**。

![image-20240919171943985](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919171943985.png)

13、G-Mean（不常用）：G-Mean 是真正例率和假正例率的几何平均，强调在处理不平衡数据时的模型性能。**适合于需要保证模型对各类预测的平衡性能，尤其是在不平衡数据集上**。

![image-20240919172025052](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919172025052.png)

14、**基于Wilcoxon-Holm方法的临界差异图（CD图）**：CD 图 是一种可视化工具，**通常用于比较多个算法或模型的性能**，特别是在机器学习和统计学中。**它主要用于显示不同算法之间的显著性差异，并帮助研究者判断哪些算法在特定任务上表现更好**。

- **Wilcoxon 检验**：这是一种非参数统计检验方法，用于比较两个相关样本的差异。与 t 检验不同，**它不需要数据符合正态分布**。
- **Holm 方法**：**是针对多个假设检验的修正方法，旨在控制错误发现率（FDR）**。它通过逐步调整 p 值来提高检验的功效

结合 Wilcoxon 检验和 Holm 方法后，可以比较多个模型的性能，以确定它们之间的统计显著性。CD 图广泛应用于：

- **算法比较**：在机器学习实验中，研究人员可以通过 CD 图轻松比较多种算法的效果。
- **参数调优**：可以帮助判断不同超参数设置下的模型表现。
- **可视化研究结果**：为研究论文或报告提供直观的结果展示。

![image-20240919172636469](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919172636469.png)

15、**假阳性率（FPR）**：假阳性率是一个用于评估分类模型性能的重要指标，特别是在二分类任务中。它表示模型将负类样本错误地预测为正类的比例。**低假阳性率**：表示模型在识别负类样本时表现良好，误报较少。**高假阳性率**：表示模型经常将负类样本错误地预测为正类，可能导致不必要的后续处理或干扰。

![image-20240919173336456](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919173336456.png)

16、**Rank Power (RP)**：是一种用于**评估排序模型或推荐系统性能的指标**。它主要用于判断一个模型在给定查询下的结果排名是否合理，特别是在信息检索、推荐系统和排序学习等领域。Rank Power 指标**关注模型在预测时是否能将相关性更高的项目排在更前面**。它通过比较模型的排名结果与理想排名之间的一致性来进行评估。通过判断前n个异常候选中实际有多少异常值，可以衡量不同异常检测算法的性能。

17、Correlation coefficient：是相关性的数值度量，即，两个变量之间的统计关系。例如，斯皮尔曼等级相似性或皮尔逊相关性。**更重要的是放在可能的离群值排名在顶部**。

18、Log-Loss (Logarithmic Loss)（不常用）: 用于评估分类模型输出的概率与实际标签之间的差距。它特别适用于二分类和多分类问题。Log-Loss 的值越小，模型的预测越准确。它惩罚错误的概率预测，特别是当模型对不正确的类赋予高概率时。

![image-20240919182925717](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919182925717.png)19、19、Brier Score（不常用): Brier Score 是一种衡量预测概率与实际结果之间差异的指标。它是基于均方误差的，适用于二分类和多分类问题。Brier Score 的值范围在 0 到 1 之间，值越小表示预测越好。由于是均方误差，Brier Score 对于概率预测的准确性更为敏感。

![image-20240919183033828](C:\Users\kenis\AppData\Roaming\Typora\typora-user-images\image-20240919183033828.png)

20、Kullback-Leibler Divergence (KL Divergence)（不常用）: 用来衡量模型预测分布与实际分布之间的差异，适用于评估概率预测。

21、标准化均方误差（SMSE）(不常用):对于连续特征，我们计算估计值与原始地面真值之间的标准化均方误差（SMSE）

22、其他的性能评测指标：例如**CPU利用率、执行时间、训练时间、测试时间、计算时间、内存使用、鲁棒性**。

## 二、所调研基准中使用的指标

1、ADBench基准使用了**ROC-AUC** 分数、**Area Under Precision-Recall Curve** (**PR AUC**)、**基于Wilcoxon-Holm方法的临界差异图（CD图）**[34，70]用于**统计学比较AD方法组**（p ≤ 0.05）

2、Generalized Out-of-Distribution中使用了**F-scores，ROC-AUC，PR AUC**指标，以及**结合下游任务性能**的指标。此外，本工作认为离群检测器的评估还可以**通过其支持的主要任务的性能来评估**。例如，**如果使用离群值检测器来净化具有噪声标签的数据集，则在经净化的数据集上训练的分类器的性能可以指示离群值检测器的质量**。

3、Machine Learning for Anomaly Detection中提到使用最多的性能指标是**召回率（recall）**，它测量被正确分类的异常。此外，116篇论文使用**假阳性率（FPR）**作为性能指标，该指标测量被错误分类的异常。**ROC 曲线** 是以假阳性率（FPR）为横轴，真阳性率（TPR）为纵轴绘制的曲线，**ROC曲线是用于有效评估入侵检测系统性能的最强指标之一**。此外，**准确度（Accuracy），精度和F-分数经常被研究人员作为性能指标**。除了性能指标之外，许多论文还提出了**计算性能指标，例如CPU利用率、执行时间、训练时间、测试时间和计算时间**。

4、Evaluation of Machine Learning Algorithms选用了**准确度（Accuracy）、精确度（Precision）、召回率(Recall)、F1评分和受试者工作特征（ROC）曲线**这些评价指标。在12个参与对比的算法上**随机森林（RF）算法**达到最佳性能。

5、A Comparison of Outlier Detection Techniques的评价指标为**AUC、Precision、Average precision (AP)、Rank Power (RP)、相关系数**。

6、A comparative evaluation of outlier detection algorithms选择了**ROC曲线下面积**和**精确度-召回率（PR）曲线**。为了全面概述这些方法，我们还对每种方法的**训练时间，预测时间，内存使用和鲁棒性**进行了基准测试。

7、Progress in Outlier Detection Techniques: A Survey中的评估指标：**Precision、Recall、Average precision、ROC、AUC、Correlation coefficient、Rank power (RP)**。**AUC完全忽略了分数之间的微小变化，只考虑排名**。**与PR曲线下区域等技术相比，它对于不平衡的类问题也较差且不完美，而PR曲线下区域等技术在突出微小的检测变化方面显示出更好的可能性**。然而，尽管存在这些缺点，**AUC，ROC和精确召回仍然作为评估许多离群值检测问题的事实上的标准**。

8、Human-in-the-loop Outlier Detection中的评估指标：**精确度（precision）和召回率（recall）**。

9、COPOD: Copula-Based Outlier Detection中的评估指标：**ROC-AUC**和**AVERAGE PRECISION（AP）**指标

10、outlier detection in high dimensional data中的评估指标：在离群点检测中，假设异常实例的比例远远小于正常实例。因此，我们使用**精确度和召回率**作为算法性能的衡量标准。更准确地说，我们通过**F1分数**将联合收割机的精确度和召回率合并为一个值。

11、ECOD:Unsupervised Outlier Detection中的评估指标：在每个实验中，60%的数据用于训练，剩余的40%用于测试。**使用受试者工作特征下的面积（ROC）和平均精度（AP），通过取10个独立试验的平均评分来评价性能**。这两种度量在离群点挖掘研究中得到了广泛的应用[52]、[53]、[54]、[55]。**我们报告了原始比较结果和临界差异（CD）图，以显示统计学差异[56]，[57]**，其中通过Wilcoxon符号秩检验和霍尔姆's校正可视化了统计学比较。

12、A robust SVM-based approach中的评估指标：为了比较分类器，我们使用了公认的分类性能指标：**准确度（ACC）和曲线下面积（AUC）**。**评测方法**：**我们遵循十重交叉验证程序（TFCV）来获得这些指标**。它包括将数据集随机划分为10个子集。在每次迭代中，这些子集中的九个构成训练集，剩余的分区是测试集。分类器的性能由独立的测试集进行评估。

13、Robust Variational Autoencoders中的评估指标：在OD实验中，我们使用**平均精度（AVPR）**，根据每种方法的离群值得分计算。**AVPR是精确率-召回率曲线下面积的度量，因此越高越好**。在修复实验中，根据特征类型需要不同的度量。**对于连续特征，我们计算估计值与原始地面真值之间的标准化均方误差（SMSE）**，该标准化均方误差由地面真值的经验方差归一化**对于分类特征，我们计算基础真值的独热表示与每个类别估计的概率之间的Brier得分**。

## 三、所调研基准中的实验设计

## （一）ADBench中的实验设计

### 1、实验设计角度

#### 1）角度1：地面实况标签的可用性（监督）

动机：除了未标记的样本之外，**在真实世界的应用中，人们可以访问有限数量的标记的异常**，由领域专家或人工参与技术（如主动学习）识别出的一些异常[5，7，78，189]。

本文设计：我们首先对现有的无监督异常检测方法进行基准测试，然后根据[127，131，205]中的**设置评估具有不同监督级别的半监督和全监督方法，以提供公平的比较**。例如，**标记的异常γl = 10%意味着训练集中10%的异常是已知的，而其他样本保持未标记**。

思考：

> 1、设置基准时考虑无监督，半监督和监督的基准，设置异常比例

#### 2）角度2：异常值类型

动机：广泛的公共数据集可用于基准测试，但**它们通常由不同类型异常的混合物组成**，这使得理解AD算法在特定类型异常方面的优缺点具有挑战性[55，166]。为了更好地理解异常类型的影响，**我们在公共数据集的基础上，通过注入特定类型的异常来创建合成数据集，以分析AD算法的响应**。

本文设计：**在ADBench中，我们通过注入特定类型的异常，从基准数据集创建逼真的合成数据集**。一些现有的工作，**如PyOD [198]，通过假设它们的数据分布来生成完全合成的异常，这无法创建复杂的异常**。我们遵循并丰富了[166]中的方法，以**生成“真实的”合成数据**；**我们的方法支持更多类型的异常生成**。其核心思想是建立一个生成模型（例如，[166]、Sparx [191]和ADBench中使用的高斯混合模型GMM），并丢弃其原始异常，因为我们不知道它们的类型。然后，我们可以生成正常的样本和不同类型的异常的定义的基础上，通过调整生成模型。正常样本的生成在所有设置中都是相同的，如果没有注明的话，我们在下面提供了四种异常类型的生成过程（详细信息也请参见我们的代码库）。

> 1、**局部异常**：局部异常是指偏离其局部邻域的异常[22]。我们遵循GMM过程[118，166]来生成合成正态样本，然后**通过缩放参数α = 5来缩放协方差矩阵= α以生成局部异常**。
>
> 2、**全局异常**：全局异常与正态数据[68]有更大的不同，**由均匀分布生成**，其中边界被定义为输入特征的最小值和最大值，例如，第k个特征Xk和α = 1.1控制异常的异常度。
>
> 3、**依赖异常**：依赖异常是指不遵循正常数据遵循的依赖结构的样本[117]，即，依赖性异常的输入特征被假定为彼此独立。Vine Copula [1]方法用于对原始数据的依赖性结构进行建模，其中通过去除建模的依赖性将生成的异常的概率密度函数设置为完全独立（参见[117]）。我们使用核密度估计（KDE）[61]来估计特征的概率密度函数并生成正常样本。
>
> 4、**异常群**：也称为群异常[93]，表现出类似的特征[42，99]。我们将正常样本的平均特征向量缩放α = 5，即，µ = α

在ADBench中，我们分析了上述所有四种异常类型下的算法性能（§4.3）。

思考：

> 1、四类离群值下Rovas分别能检测出的异常比例？
>
> 2、这四类离群值中ugly outliers修复对分类器分类效果影响的不同？

#### 3）角度3：噪声和损坏数据的模型鲁棒性

动机：在现实世界的应用中，输入数据可能在一定程度上受到噪声和损坏[42，55，60，124]。然而，这一重要观点在现有的基准测试中尚未得到充分研究，我们尝试通过在三种噪声和损坏设置下评估AD算法来理解这一点。

> 1、**重复的异常**。在许多应用中，由于记录错误等原因，某些异常可能会在数据中重复多次[83]。重复异常的存在也被称为“异常屏蔽”[55、60、100]，这对许多AD算法[25]提出了挑战，例如，基于密度的KNN [11，144]。此外，异常频率的变化也会影响检测方法的行为[42]。**因此，我们通过将数据分为训练集和测试集来模拟此设置，然后在两个集中复制异常（特征和标签）多达6次，并观察AD算法如何变化**。
>
> 2、**不相关的特征**。表格数据可能包含由测量噪声或不一致的测量单位[28，55]引起的不相关特征，其中**这些噪声维度可能隐藏异常数据的特征，并因此使检测过程更加困难**[128，150]。**我们添加不相关特征达到总输入特征的50%**（即，通过从随机选择的第k个输入特征Xk的Unif min Xk，max Xk中生成均匀噪声特征，同时保持标签正确，对问题定义中的d）进行分类，并总结算法性能变化。
>
> 3、**注释错误**。虽然现有研究[131，152]探讨了未标记样本中的异常污染，但我们进一步讨论了标记污染对算法性能的更普遍影响，其中**考虑了正常样本和异常之间的标记翻转[122，202]（高达总标记的50%）。请注意，此设置不影响未受监督的方法，因为它们不使用任何标签**。**讨论标注错误是有意义的，因为手动标注或一些自动标注技术在被视为完美的同时总是有噪声**。

### 2、评估指标

1、我们通过两个广泛使用的指标来评价不同的AD方法：**AUCROC（受试者操作特征曲线下的面积）和AUCPR（精确度-回忆曲线下的面积）**。此外，**基于Wilcoxon-Holm方法的临界差异图（CD图）**[34，70]用于统计学比较AD方法组（p ≤ 0.05）。

2、模型训练和推理的时间

### 3、实验结果

#### 1）角度1：地面实况标签的可用性（监督）

1、没有一种无监督方法在统计上优于其他方法

2、当标签信息有限时，半监督方法优于监督方法

3、最新的网络架构（如Transformer）和新兴的集成方法在AD中具有竞争力的性能

#### 2）角度2：异常值类型

1、无监督算法的性能高度依赖于其假设和潜在异常类型的对齐。例如局部异常因子（LOF）在统计上优于局部异常的其他无监督方法，并且使用第k个（全局）最近邻的距离作为异常分数的KNN是全局异常的统计最佳检测器

2、关于异常类型的先验知识的“力量”可能超过部分标签的使用。

#### 3）角度3：噪声和损坏数据的模型鲁棒性

1、**无监督方法更容易受到重复异常的影响**。几乎所有无监督方法都受到重复异常的严重影响。其AUCROC随重复的增加而成比例地恶化。一种解释是，无监督方法通常假设基础数据是不平衡的，只有一小部分异常它们依赖于这个假设来检测异常。随着异常重复次数的增加，底层数据变得更加平衡，并且违反了异常的少数假设，从而导致无监督方法的退化。显然，在标签的帮助下，更平衡的数据集不会显着影响半监督和全监督方法的性能。

2、**由于特征选择，不相关的特征对监督方法的影响很小**。例如，像XGBoost这样的集成树可以过滤不相关的特征。此外，监督方法（和一些半监督方法，如DevNet）的鲁棒性能表明，标签信息可以有利于特征选择。

3、半监督和全监督方法都对轻微的注释错误表现出很大的弹性。

## （二）《outlier detection in high dimensional data》中的实验设计

### 1、实验设计角度

#### 1）角度1：不同的异常比例

**本文算法使用的关键参数是污染率，即数据中异常的假定比例。每种算法都根据给定的污染率将数据中的一些点标记为离群值**。由于在实践中，真正的污染率是未知的，我们测试的方法在一个范围内的污染值。

## （三）《ECOD:Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions》中的实验设计

### 1、实验设计角度

#### 1）角度1：ECOD的有效性

探索不同尾部概率的使用如何影响ECOD的性能。

#### 2）角度2：ECOD与基准算法在AUC和AP评测指标上的差异

#### 3）角度3：ECOD的运行效率和可扩展性

**运行效率偏向算法计算速度，可扩展性偏向于特征维度扩展对算法带来的影响**。

### （五）《A robust SVM-based approach with feature selection and outliers detection for classification problems》中的实验设计

### 1、实验设计角度

#### 1）角度1：将本文提出算法与基准算法在实验指标上进行比较

#### 2）角度2：通过施加不同的异常值和扰动比例，验证本文算法和基准算法的性能变化

#### 3）角度3：时间消耗的对比

## （六）《Robust Variational Autoencoders for Outlier Detection and Repair of Mixed-Type Data》中的实验设计

### 1、实验设计角度

#### 1）角度1：向数据集中人为加入元组异常和单元格异常

对于每个数据集，随机选择一个单元格子集进行破坏，遵循两步过程：a）随机选择数据中的一定百分比的行进行破坏; b）对于这些选择的行中的每一行，随机破坏20%的特征，在每个选择行中破坏不同的特征集。

#### 2）角度2：向数据集中加入不同比例的异常（区分连续和分类特征）

**对于连续特征，在标准化数据之前执行噪声处理**。探讨了四种不同的噪声分布：**高斯噪声、拉普拉斯噪声、对数正态噪声、两个高斯噪声分量的混合**

**对于分类特征，噪声过程基于潜在的边缘（离散）分布**。我们通过从tempered categorical distribution（在重要性抽样中也称为幂启发式）中采样（并排除当前的干净类别），将单元格值替换为脏值