

# Rovas实验框架

## 一、实验设置

### 1.1 实验数据集

真实异常检测数据集（主要来自ADBench，以及其他基准中使用的数据集）

合成异常检测数据集（合成自ADBench，Pyod提供的合成方法）

真实多分类数据集（部分异常检测数据集对应的原始多分类数据集，采集自UCI/Kaggle/OpenML）

**合成多分类数据集（合成方法待定，可以考虑高斯过程）**

**加噪多分类数据集（方法待定，在多分类数据集中添加噪声/异常值）**

不同异常比例的数据集（采集自LMU中不同异常比例的真实异常检测数据集）

不同异常类型的数据集（将原始异常全部删除，抽取一定比例的剩余值，转化为不同类型异常值，使用这部分异常值替换采样的正常值，参考ADbench中的实现）

### 1.2 实验基准算法

无监督算法（基于距离的，基于密度的，基于统计的，基于模型的，基于学习的（偏向深度和主动学习）的）

半监督算法（深度和主动学习的开源算法）

监督算法（基于深度的开源算法）

集成算法（基于bagging/boosting等）

### 1.3 实验评测指标

accuracy（分类准确度是正确分类的样本数与总样本数的比率，**适用于类别分布相对均匀的情况**）

precision（精确度是指所有被模型预测为正例的样本中，实际为正例的比例）

recall（召回率是指所有实际为正例的样本中，被模型正确预测为正例的比例）

F1分数（F1分数是精确率和召回率的调和平均值，**适用于类别不平衡的情况**）

ROC-AUC分数（AUC 值越接近 1，模型性能越好，适用于**二分类问题**）

PR AUC（PR 曲线绘制了精确度与召回率之间的关系，**适用于不平衡数据集**。可以用于**评估不同阈值下的模型性能，特别是在正类样本较少的情况下**。PR AUC 表示 PR 曲线下的面积，范围在 0 到 1 之间。**越接近 1，模型在平衡精确度和召回率方面的性能越好**。）

AP（**AP 是精确度和召回率的加权平均，综合考虑了不同召回率下的精确度**。用途：常用于多类分类任务，特别是检测任务。**AP 是一个重要的评估指标，能够全面反映模型在不同召回率下的表现。计算 AP 时，关注精确度与召回率的平衡，尤其是在处理不平衡数据时**。）

CD图（CD 图 是一种可视化工具，**通常用于比较多个算法或模型的性能**，特别是在机器学习和统计学中。**它主要用于显示不同算法之间的显著性差异，并帮助研究者判断哪些算法在特定任务上表现更好**）

Rank Power (RP)（是一种用于**评估排序模型或推荐系统性能的指标**。它主要用于判断一个模型在给定查询下的结果排名是否合理。Rank Power 指标**关注模型在预测时是否能将相关性更高的项目排在更前面**。通过判断前n个异常候选中实际有多少异常值，可以衡量不同异常检测算法的性能）

其他的性能评测指标（例如**CPU利用率、执行时间、训练时间、测试时间、计算时间、内存使用、鲁棒性**）

### 1.4 实验配置

实验环境（待定）

算法超参数设置（默认）

算法实现语言



## 二、实验设计

### 2.1 Rovas和传统离群值检测算法在离群值检测指标上的比较

#### 2.1.1 实验一（Rovas/基准算法性能对比）

将Rovas与基准在异常检测数据集上的**异常检测性能（Accuracy/PRAUC等指标）进行对比**（作表）

#### 2.1.2 实验二（Rovas/基准算法对不同种类，不同比例异常值的检测性能）

**改变数据集中的异常值种类**，探索Rovas和基准对不同种类异常值（good/bad/ugly，或adbench中对异常值的分类）的检测效果。**改变异常值比例**，检测Rovas和基准算法的鲁棒性。（作CD图）

#### 2.1.3 实验三（Rovas/ADbench探索性试验）可不加

good/bad/ugly的outliers和adbench中区分的四类异常值的区别和联系



### 2.2 Rovas算法自身性能的评估

#### 2.2.1 实验一（Rovas/基准算法可扩展性）

随着数据集规模（行/列）的增大，Rovas运行时间变化（可加入基准算法的时间变化，但估计比Rovas快）

#### 2.2.2 实验二（Rovas谓词评估）

评估Rovas规则中的机器学习/聚合/损失谓词（暂无评估指标，可参照谓词设定目标的实现程度，以及谓词的执行时间）



### 2.3 Rovas修复对下游分类任务的影响

#### 2.3.1 实验一（OHunt修复效果测试）

对比下游任务分类器（如SVM/RF分类器）**在原始数据集（含outliers），与ugly outliers修复后的数据集上，分类器性能(Accuracy/PRAUC等指标)的差异**

#### 2.3.2 实验二（OHunt修复方法对比）

对比不同异常值修复策略（如**基于统计修复**/**直接删除**/**训练机器学习模型用于异常修复**）的不同效果(Accuracy/PRAUC等指标)

#### 2.3.3 实验三（OHunt鲁棒性测试）

对比在**不同异常比例**的原始数据集中，经过Rovas修复策略进行修复后，**能保证分类器在原始数据集上分类效果（Accuracy/PRAUC等指标）随异常比例提高而降低，在修复后的数据集上，分类效果能稳定在一定水平范围内**。
