"""
æ–¹æ¡ˆä¸€ã€äºŒæ•ˆæœè¾ƒå¥½
é€‰ç”¨ä¸åŒçš„æ•°æ®é›†å±•å¼€å®éªŒ
é‡‡ç”¨KNNä¿®æ”¹å¼‚å¸¸å€¼çš„æ ‡ç­¾
é‡‡ç”¨ç»Ÿè®¡æ–¹æ³•ä¿®å¤å¼‚å¸¸å€¼çš„ç‰¹å¾
ä½¿ç”¨SVMåˆ†ç±»å™¨è¿›è¡Œå®éªŒ
"""
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†

# choice drybeanæ•°æ®é›†
file_path = "../../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
categorical_features = [0, 6]

# choice obesityæ•°æ®é›†
# file_path = "../../kaggle_datasets/Obesity_prediction/obesity_data.csv"
# label_col_name = "ObesityCategory"
# data = pd.read_csv(file_path)
# enc = LabelEncoder()
# data[label_col_name] = enc.fit_transform(data[label_col_name])
# categorical_features = [1]
# categorical_names = {}
# feature_names = data.columns[:-1].tolist()
# # å¯¹å­—ç¬¦ä¸²åˆ—è¿›è¡Œæ•°å€¼æ˜ å°„
# for feature in categorical_features:
#     le = LabelEncoder()
#     le.fit(data.iloc[:, feature])
#     data.iloc[:, feature] = le.transform(data.iloc[:, feature])
#     categorical_names[feature] = le.classes_
# data[feature_names] = data[feature_names].astype(float)
# num_col = data.shape[1]
# X = data.values[:, 0:num_col-1]
# y = data.values[:, num_col-1]

# choice wine-qualityæ•°æ®é›† (svmæ¨¡å‹ä¸‹åˆ†ç±»æ•ˆæœä¸å¥½)
# file_path = "../../UCI_datasets/wine+quality/winequality-white.csv"
# label_col_name = "quality"
# data = pd.read_csv(file_path, sep=';')
# enc = LabelEncoder()
# data[label_col_name] = enc.fit_transform(data[label_col_name])
# feature_names = data.columns[:-1].tolist()
# data[feature_names] = data[feature_names].astype(float)
# data[label_col_name] = data[label_col_name].astype(int)
# num_col = data.shape[1]
# X = data.values[:, 0:num_col-1]
# y = data.values[:, num_col-1]
# categorical_features = []

# choice apple_qualityæ•°æ®é›†ï¼ˆä½¿ç”¨SVMçš„é»˜è®¤æ¨¡å‹ï¼Œæ–¹å·®ç¡®å®šé‡è¦ç‰¹å¾ï¼Œä¿®å¤ç‰¹å¾ä¸­çš„å¼‚å¸¸æœ€æœ‰æ•ˆï¼‰
# file_path = "../../kaggle_datasets/Apple_Quality/apple_quality.csv"
# data = pd.read_csv(file_path)
# # åˆ é™¤idåˆ—
# data = data.drop(data.columns[0], axis=1)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['Quality'] = enc.fit_transform(data['Quality'])
# categorical_features = []
# X = data.values[:, :-1]
# y = data.values[:, -1]

# choice balitaæ•°æ®é›† (ä½¿ç”¨SVMçš„é»˜è®¤æ¨¡å‹ï¼Œæ–¹å·®ç¡®å®šé‡è¦ç‰¹å¾ã€‚ç‰¹å¾æ•°è¶Šå°‘ï¼Œä¿®å¤ç‰¹å¾ä¸­å¼‚å¸¸æ•ˆæœè¶Šå¥½ï¼Œå¯ä»¥è€ƒè™‘ç‰¹å¾æ•°è¾ƒå°‘çš„æ•°æ®é›†)
# file_path = "../../kaggle_datasets/balita/data_balita.csv"
# data = pd.read_csv(file_path)
# # ä» data ä¸­éšæœºæŠ½å– 10% çš„æ ·æœ¬
# data = data.sample(frac=0.1, random_state=42)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['Nutrition_Status'] = enc.fit_transform(data['Nutrition_Status'])
# data['Gender'] = enc.fit_transform(data['Gender'])
# categorical_features = [0, 1]
# X = data.values[:, :-1]
# y = data.values[:, -1]

# choice Irisæ•°æ®é›† (ä½¿ç”¨SVMçš„é»˜è®¤æ¨¡å‹ï¼Œæ–¹å·®ç¡®å®šé‡è¦ç‰¹å¾ã€‚ç‰¹å¾æ•°è¶Šå°‘ï¼Œä¿®å¤ç‰¹å¾ä¸­å¼‚å¸¸æ•ˆæœè¶Šå¥½ï¼Œå¯ä»¥è€ƒè™‘ç‰¹å¾æ•°è¾ƒå°‘çš„æ•°æ®é›†)
# file_path = "../../kaggle_datasets/Iris_Species/Iris.csv"
# data = pd.read_csv(file_path)
# # åˆ é™¤idåˆ—
# data = data.drop(data.columns[0], axis=1)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['Species'] = enc.fit_transform(data['Species'])
# categorical_features = []
# X = data.values[:, :-1]
# y = data.values[:, -1]

# choice Obesityæ•°æ®é›†
# file_path = "../../kaggle_datasets/Obesity_prediction/obesity_data.csv"
# data = pd.read_csv(file_path)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['ObesityCategory'] = enc.fit_transform(data['ObesityCategory'])
# data['Gender'] = enc.fit_transform(data['Gender'])
# categorical_features = [0, 1, 5]
# X = data.values[:, :-1]
# y = data.values[:, -1]

# choice Wine datasetæ•°æ®é›† (æ•ˆæœä¸å¥½)
# file_path = "../../kaggle_datasets/Wine dataset/Wine dataset.csv"
# data = pd.read_csv(file_path)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['class'] = enc.fit_transform(data['class'])
# categorical_features = [5, 13]
# X = data.values[:, 1:]
# y = data.values[:, 0]

# choice adultæ•°æ®é›† (SVMæ‹Ÿåˆå¤§æ•°æ®é›†é€Ÿåº¦å¾ˆæ…¢ï¼Œå¯ä»¥å¯¹æ•°æ®é›†æˆªå–åè®­ç»ƒsvm)
# file_path = "../../nosiy_datasets/adult/adult.csv"
# data = pd.read_csv(file_path)
# # ä» data ä¸­éšæœºæŠ½å– 20% çš„æ ·æœ¬
# data = data.sample(frac=0.2, random_state=42)
# enc = LabelEncoder()
# # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
# data['Income'] = enc.fit_transform(data['Income'])
# categorical_features = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
# feature_names = data.columns[:-1].tolist()
# for i in categorical_features:
#     column_name = feature_names[i]
#     data[column_name] = enc.fit_transform(data[column_name])
# X = data.values[:, :-1]
# y = data.values[:, -1]

# section æ•°æ®ç‰¹å¾ç¼©æ”¾
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)

# section å‘æ•°æ®é›†ä¸­åŠ å™ª
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=all_columns)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)
# print("è®­ç»ƒé›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", train_noise)
# print("æµ‹è¯•é›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", test_noise)

# section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)

# choice å€ŸåŠ©PCAæ–¹æ³•æ‰¾åˆ°å¯¹åˆ†ç±»å™¨æœ‰é‡è¦å½±å“çš„ç‰¹å¾ (æ•ˆæœä¸å¥½)
from sklearn.decomposition import PCA

svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_model = svm.SVC()
svm_model.fit(X_train_copy, y_train)
pca = PCA(n_components=0.95)  # ä¿ç•™ 95% çš„æ–¹å·®
X_train_copy_copy = np.copy(X_train_copy)
X_pca = pca.fit_transform(X_train_copy_copy)
components = pca.components_
feature_importance = np.abs(components).sum(axis=0)  # è®¡ç®—ç‰¹å¾é‡è¦æ€§
# æŒ‘é€‰æœ€é‡è¦çš„ k ä¸ªç‰¹å¾
k = 6  # é€‰æ‹©å‰ 10 ä¸ªç‰¹å¾
top_k_indices = np.argsort(feature_importance)[-k:]  # æŒ‘é€‰æœ€é‡è¦çš„ k ä¸ªç‰¹å¾å¯¹åº”çš„ç´¢å¼•
print("æœ€é‡è¦çš„ k ä¸ªç‰¹å¾çš„ç´¢å¼•:", top_k_indices)

# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
# i = X_train_copy.shape[1]
# np.random.seed(1)
# categorical_names = {}
# svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
# # svm_model = svm.SVC()
# svm_model.fit(X_train_copy, y_train)
#
# for feature in categorical_features:
#     le = LabelEncoder()
#     le.fit(data_copy.iloc[:, feature])
#     data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
#     categorical_names[feature] = le.classes_
#
# explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
#                                                    categorical_features=categorical_features,
#                                                    categorical_names=categorical_names, kernel_width=3)
#
# predict_fn = lambda x: svm_model.predict_proba(x)
# exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
# top_features = exp.as_list()
# important_features = []
# for feature_set in top_features:
#     feature_long = feature_set[0]
#     for feature in feature_names:
#         if set(feature).issubset(set(feature_long)):
#             important_features.append(feature)
#             break
#
# top_k_indices = [feature_names.index(feature_name) for feature_name in important_features]
# print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# choice æ— æ¨¡å‹(éå‚æ•°)æ–¹æ³•ä¸­çš„Permutation Feature Importance-slearn(æ•ˆæœæœªçŸ¥)ï¼ˆé€Ÿåº¦æ…¢ï¼‰
# from sklearn.inspection import permutation_importance
# top_k_svm = 4
# svm_model = svm.SVC()
# svm_model.fit(X_train_copy, y_train)
# result = permutation_importance(svm_model, X_train_copy, y_train, n_repeats=10,random_state=42)
# permutation_importance = result.importances_mean
# top_k_indices = np.argpartition(-permutation_importance, top_k_svm)[:top_k_svm]
# print("Permutation Feature Importance-slearnæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_svm, top_k_indices))

# choice ä½¿ç”¨SelectFromModelæ–¹æ³•(æ•ˆæœä¸å¥½)
# from sklearn.feature_selection import SelectFromModel
# svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_model.fit(X_train_copy, y_train)
# sfm = SelectFromModel(svm_model, threshold='mean', prefit=True)
# X_selected = sfm.transform(X_copy)
# # è·å–é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
# top_k_indices = sfm.get_support(indices=True)
# # æ‰“å°é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
# print("SelectFromModelé€‰æ‹©å™¨é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:", top_k_indices)

# choice ä½¿ç”¨ç³»æ•°æ–¹æ³•ï¼ˆæ•ˆæœä¸å¥½ï¼‰
# # æ‰¾åˆ°è®­ç»ƒæ•°æ®ä¸­æœ‰å½±å“åŠ›çš„ç‰¹å¾ï¼ˆæœ€ä½³ç‰¹è¯æ•°æ˜¯4ï¼Œ7ï¼Œ8ï¼Œ10ï¼‰
# top_k_svm = 6
# svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_model.fit(X_train_copy, y_train)
# # æå–ç³»æ•°
# feature_importances_coef = np.abs(svm_model.coef_[0])
#
# # å¯¹ç³»æ•°è¿›è¡Œæ’åº
# top_k_indices = np.argpartition(-feature_importances_coef, top_k_svm)[:top_k_svm]
# print("SVMæ¨¡å‹é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•æ˜¯ï¼š", top_k_indices)

# choice å€ŸåŠ©æ–¹å·®åˆ¤åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾(æ•ˆæœè¿˜å¯ä»¥)
# if X_copy.shape[1] < 8:
#     top_k_var = X_copy.shape[1] // 2 + 1
# else:
#     top_k_var = X_copy.shape[1] // 2
# svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_model.fit(X_train_copy, y_train)
# variances = np.var(X_copy, axis=0)
# top_k_indices = np.argsort(variances)[-top_k_var:][::-1]
# print("æ–¹å·®æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_var, top_k_indices))

# choice ä½¿ç”¨RFEé€’å½’åœ°è®­ç»ƒæ¨¡å‹å¹¶åˆ é™¤æœ€ä¸é‡è¦çš„ç‰¹å¾ (æ•ˆæœä¸å¥½)
# from sklearn.feature_selection import RFE
#
# # åˆ›å»ºæ¨¡å‹
# svm_model = svm.SVC(kernel='linear', C=1.0)
# selector = RFE(svm_model, n_features_to_select=6)  # é€‰æ‹©å‰5ä¸ªç‰¹å¾
# selector = selector.fit(X_train_copy, y_train)
# # è·å–è¢«é€‰æ‹©çš„ç‰¹å¾
# top_k_indices = np.where(selector.support_)[0]
# # è®­ç»ƒsvmæ¨¡å‹
# svm_model.fit(X_train_copy, y_train)
# print("é€‰æ‹©çš„ç‰¹å¾ï¼š", top_k_indices)

# section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„

# choice ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
# decision_values = svm_model.decision_function(X_copy)
# predicted_labels = np.argmax(decision_values, axis=1)
# # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
# num_samples = X_copy.shape[0]
# num_classes = svm_model.classes_.shape[0]
# hinge_losses = np.zeros((num_samples, num_classes))
# hinge_loss = np.zeros(num_samples)
# for i in range(num_samples):
#     correct_class = int(y[i])
#     for j in range(num_classes):
#         if j != correct_class:
#             loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
#             hinge_losses[i, j] = loss_j
#     hinge_loss[i] = np.max(hinge_losses[i])
#
# # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
# ugly_outlier_candidates = np.where(hinge_loss > 1)[0]
# # print("Dä¸­æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", ugly_outlier_candidates)

# choice ä½¿ç”¨svmæ¨¡å‹é¢„æµ‹ç»“æœ
y_p = svm_model.predict(X_copy)
ugly_outlier_candidates = np.where(y != y_p)[0]

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„

outlier_feature_indices = {}
threshold = 0.01
for column_indice in top_k_indices:
    select_feature = feature_names[column_indice]
    select_column_data = data_copy[select_feature].values
    max_value = np.max(select_column_data)
    min_value = np.min(select_column_data)
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
    # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    outlier_feature_indices[column_indice] = intersection
# print(outlier_feature_indices)

# SECTION SVMæ¨¡å‹çš„å®ç°

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
svm_clf = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_clf = svm.SVC()
svm_clf.fit(X_train, y_train)
train_label_pred = svm_clf.predict(X_train)
test_label_pred = svm_clf.predict(X_test)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
train_label_pred_noise = svm_model.predict(X_train_copy)
test_label_pred_noise = svm_model.predict(X_test_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆknnæ–¹æ³•ï¼‰
#  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

# subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾

# ç¡®å®šæ•°æ®é›†Dä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„å’Œæ­£å¸¸å…ƒç»„
outlier_tuple_set = set()
for value in outlier_feature_indices.values():
    outlier_tuple_set.update(value)
X_copy_repair_indices = list(outlier_tuple_set)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_copy_inners, y_inners)

# é¢„æµ‹å¼‚å¸¸å€¼
y_pred = knn.predict(X_copy_repair)

# æ›¿æ¢å¼‚å¸¸å€¼
y[X_copy_repair_indices] = y_pred
y_train = y[train_indices]
y_test = y[test_indices]

# subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹

svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair = svm.SVC()
svm_repair.fit(X_train_copy, y_train)
y_train_pred = svm_repair.predict(X_train_copy)
y_test_pred = svm_repair.predict(X_test_copy)

print("*" * 100)
# è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆäºŒï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œç‰¹å¾ä¿®å¤ï¼ˆç»Ÿè®¡æ–¹æ³•ä¿®å¤ï¼‰
# #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

# # subsection æŒ‰ç…§ç‰¹å¾ä¸­çš„å¼‚å¸¸å€¼è¿›è¡Œä¿®å¤
#
# for key, value in outlier_feature_indices.items():
#     column_data = X_copy[:, key]
#     mean = np.mean(column_data)
#     X_copy[value, key] = mean
#
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹
#
# # svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy, y_train)
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))