"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´, M) â†’ ugly(ğ‘¡)çš„å…·ä½“å®ç°
åˆ†ç±»å™¨ä¸ºä¸æ˜¯linearæ ¸çš„svmåˆ†ç±»å™¨
"""
# unsupervised methods
from collections import Counter

from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section é¢„å¤„ç†æ•°æ®é›†
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
# ä½¿ç”¨å¤„ç†åçš„Xå’Œyç»„åˆæˆæ–°çš„data_copy
column_names = data.columns.tolist()
# å°† X å’Œ y ç»„åˆä¸ºä¸€ä¸ª numpy æ•°ç»„
combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# åˆ›å»ºæ–°çš„ DataFrame
data_copy = pd.DataFrame(combined_array, columns=column_names)
# å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ•´æ•°ç¼–ç 
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼ˆåœ¨è®­ç»ƒæ•°æ®Dçš„æ‰€æœ‰å…ƒç»„ä¸­æ‰¾å¼‚å¸¸å€¼ï¼‰
threshold = 0.01
col_indices = 3
row_indices = 10
select_feature = feature_names[col_indices]
# è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
select_column_data = data_copy[select_feature].values
# # ç¡®å®šè®­ç»ƒé›†çš„è¡Œæ•°
# num_rows_X_train = X_train.shape[0]
# # æˆªå– select_column_data ä¸­X_trainä¸‹æ ‡å¯¹åº”çš„å…ƒç´ 
# select_column_data_trimmed = select_column_data[train_indices]
# æ‰¾åˆ°æ‰€é€‰åˆ—çš„æœ€å¤§å€¼å’Œæœ€å°å€¼
max_value = np.max(select_column_data)
min_value = np.min(select_column_data)
# æ‰¾åˆ°t.Aå¯¹åº”çš„å€¼
t_value = data_copy.iloc[row_indices, col_indices]
# å¯¹æ•°æ®è¿›è¡Œæ’åº
# sorted_data = np.sort(select_column_data)
sorted_indices = np.argsort(select_column_data)
sorted_data = select_column_data[sorted_indices]
# æ‰¾åˆ°æœ€æ¥è¿‘çš„æ¯” t_value å¤§çš„å€¼å’Œæ¯” t_value å°çš„å€¼
greater_than_t_value = sorted_data[sorted_data > t_value]
less_than_t_value = sorted_data[sorted_data < t_value]
# æ‰¾åˆ°ä¸t_valueæœ€æ¥è¿‘çš„æ›´å¤§çš„å€¼å’Œæ›´å°çš„å€¼
if greater_than_t_value.size > 0:
    closest_greater = greater_than_t_value[0]  # æœ€è¿‘çš„å¤§äº t_value çš„å€¼
else:
    closest_greater = t_value
if less_than_t_value.size > 0:
    closest_less = less_than_t_value[-1]  # æœ€è¿‘çš„å°äº t_value çš„å€¼
else:
    closest_less = t_value
# åˆ¤æ–­t.Aæ˜¯å¦æ˜¯å¼‚å¸¸å€¼
if max_value == t_value:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold)
elif min_value == t_value:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", closest_greater - t_value > threshold)
else:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold and t_value - closest_less > threshold)
# æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
outliers = []
outliers_index = []
# æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
if len(sorted_data) > 1:
    if (sorted_data[1] - sorted_data[0] >= threshold):
        outliers.append(sorted_data[0])
        outliers_index.append(sorted_indices[0])
    if (sorted_data[-1] - sorted_data[-2] >= threshold):
        outliers.append(sorted_data[-1])
        outliers_index.append(sorted_indices[-1])
# æ£€æŸ¥ä¸­é—´å…ƒç´ 
for i in range(1, len(sorted_data) - 1):
    current_value = sorted_data[i]
    left_value = sorted_data[i - 1]
    right_value = sorted_data[i + 1]
    if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
        outliers.append(current_value)
        outliers_index.append(sorted_indices[i])
# æ‰€æœ‰æ•°æ®Dä¸‹å¯¹åº”çš„ä¸‹æ ‡ç´¢å¼•
outliers_index_numpy = np.array(outliers_index)
print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼çš„ç´¢å¼•ä¸ºï¼š", outliers_index)
print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼ä¸ºï¼š", outliers)

# SECTION è°“è¯loss(M, D, ğ‘¡)çš„å®ç°

#  SVMæ¨¡å‹è®­ç»ƒå’Œåˆ†ç±»å‡†ç¡®åº¦
# svm_model = svm.SVC()
svm_model = svm.SVC(probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
# è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]

# SUBSECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šè®¾ç½®é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
# åœ¨è®­ç»ƒæ•°æ®ä¸­åˆ¤å®šçš„è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬çš„ç´¢å¼•
bad_samples = np.where(hinge_loss > 1)[0]
print("æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", bad_samples)

# subsection åŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
print("*" * 100)
print("åŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
print("*" * 100)

# section è°“è¯ Mğ‘ (ğ‘…, ğ´, M) çš„å®ç°
# SUBSECTION å€ŸåŠ©æ–¹å·®åˆ¤åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾
top_k_var = 6
variances = np.var(X_train, axis=0)
top_k_indices_var = np.argsort(variances)[-top_k_var:][::-1]
print("æ–¹å·®æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_var, top_k_indices_var))

# SUBSECTION sklearnåº“çš„SelectKBesté€‰æ‹©å™¨ï¼Œå€ŸåŠ©Fisheræ£€éªŒç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
top_k_fisher = 6
selector = SelectKBest(score_func=f_classif, k=top_k_fisher)
y_trans_fisher = y_train.reshape(-1)
X_new = selector.fit_transform(X_train, y_trans_fisher)
# è·å–è¢«é€‰ä¸­çš„ç‰¹å¾çš„ç´¢å¼•
selected_feature_indices = selector.get_support(indices=True)
print("SelectKBesté€‰æ‹©å™¨å€ŸåŠ©Fisheræ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_fisher, selected_feature_indices))

# SUBSECTION æ— æ¨¡å‹(éå‚æ•°)æ–¹æ³•ä¸­çš„Permutation Feature Importance-slearn(å¾ˆè€—æ—¶)
# top_k_svm = 6
# result = permutation_importance(svm_model, X_train, y_train, n_repeats=10,random_state=42)
# permutation_importance = result.importances_mean
# top_k_permutation = np.argpartition(-permutation_importance, top_k_svm)[:top_k_svm]
# print("Permutation Feature Importance-slearnæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_svm, top_k_permutation))

# SUBSECTION LIME(Local Interpretable Model-Agnostic Explanation)ï¼Œéœ€è¦å€ŸåŠ©XGBoost
i = 16
top_k_svm = 6
np.random.seed(1)
explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)
# predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
predict_fn = lambda x: svm_model.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
important_features = []
for feature_set in top_features:
    feature_long = feature_set[0]
    for feature in feature_names:
        if set(feature).issubset(set(feature_long)):
            important_features.append(feature)
            break
important_feature_indices = [feature_names.index(feature_name) for feature_name in important_features]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_svm, important_feature_indices))

# section ç¡®å®šæœ€é‡è¦çš„ç‰¹å¾

# subsection ä»…é‡‡ç”¨åŸºäºæ–¹å·®çš„æ–¹æ³•
important_features_var = top_k_indices_var

# subsection ä»…é‡‡ç”¨åŸºäºFisheræ£€éªŒçš„æ–¹æ³•
important_features_fisher = selected_feature_indices

# subsection ä»…é‡‡ç”¨Permutation Importanceçš„æ–¹æ³•
# important_features_perm = top_k_permutation

# subsection ä»…é‡‡ç”¨åŸºäºLIMEçš„æ–¹æ³•
important_features_lime = important_feature_indices

# subsection å°†ä¸Šè¿°æ‰€æœ‰æ–¹æ³•è¿”å›çš„é‡è¦ç‰¹å¾æŒ‰å‡ºç°æ¬¡æ•°ç”±é«˜åˆ°ä½å¾—åˆ°æœ€é‡è¦çš„Kä¸ªç‰¹å¾
# å°†æ‰€æœ‰ ndarrays åˆå¹¶åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
# all_indices = np.concatenate([important_features_var, important_features_fisher,
#                               important_features_perm, important_features_lime])
all_indices = np.concatenate([important_features_var, important_features_fisher, important_features_lime])
# ç»Ÿè®¡æ¯ä¸ªç‰¹å¾å‡ºç°çš„æ¬¡æ•°
index_counts = Counter(all_indices)
# æŒ‰å‡ºç°æ¬¡æ•°ä»é«˜åˆ°ä½æ’åº
sorted_indices = sorted(index_counts.items(), key=lambda x: x[1], reverse=True)
# æå–æ’åºåçš„ç‰¹å¾ç´¢å¼•
sorted_feature_indices = [index for index, count in sorted_indices]
# é€‰æ‹©å‰ k ä¸ªæœ€é‡è¦çš„ç‰¹å¾
k = 6
important_features_mix = sorted_feature_indices[:k]
print("æ‰€æœ‰ç‰¹å¾ç´¢å¼•åŠå…¶å‡ºç°æ¬¡æ•°:", index_counts)
print("æŒ‰é¢‘æ¬¡æ’åºçš„ç‰¹å¾ç´¢å¼•:", sorted_indices)
print("é¢‘æ¬¡è®¡æ•°æ–¹æ³•é€‰æ‹©çš„æœ€é‡è¦çš„å‰ {} ä¸ªç‰¹å¾ç´¢å¼•: {}".format(k, important_features_mix))

# section ç¡®å®šæ‰€é€‰å±æ€§Aä¸‹å…ƒç»„tæ˜¯å¦ä¸ºugly outliers
# åœ¨åŸå§‹æ•°æ®Dä¸­çš„ä¸‹æ ‡ç´¢å¼•
intersection = np.intersect1d(outliers_index_numpy, train_indices[bad_samples])
influential_features = important_features_mix
if np.isin(row_indices, intersection) and np.isin(col_indices, influential_features):
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹æ˜¯ugly outliers")
elif np.isin(row_indices, intersection) and np.logical_not(col_indices, influential_features):
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹ä¸æ˜¯ugly outliersï¼Œæ˜¯bad outliers")
else:
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹æ—¢ä¸æ˜¯ugly outliersï¼Œä¹Ÿä¸æ˜¯bad outliers")

if np.isin(col_indices, influential_features):
    print("Aç‰¹å¾æ˜¯æœ‰å½±å“åŠ›çš„ç‰¹å¾")
else:
    print("Aç‰¹å¾ä¸æ˜¯æœ‰å½±å“åŠ›çš„ç‰¹å¾")

# section ç¡®å®šæ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„æ‰€æœ‰ugly outliers
ugly_outlier_list = {}
for column in influential_features:
    select_feature = feature_names[column]
    # è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
    select_column_data = data_copy[select_feature].values
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°æ‰€é€‰å±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    # åœ¨åŸå§‹æ•°æ®é›†Dä¸‹çš„ä¸‹æ ‡ç´¢å¼•ï¼ŒåŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†
    intersection = np.intersect1d(outliers_index_numpy, train_indices[bad_samples])
    ugly_outlier_list[column] = intersection

for idx, key in enumerate(ugly_outlier_list):
    value = ugly_outlier_list[key]
    print(f"ç¬¬ {key} åˆ—çš„ugly outliersä¸º: {value}")