"""
ğ‘…(ğ‘¡) âˆ§ Mğ‘œ (ğ‘¡, D) âˆ§ ğ‘‹1 â†’ ugly(ğ‘¡)çš„å…·ä½“å®ç°
åˆ†ç±»å™¨ä¸ºlinearæ ¸çš„svmåˆ†ç±»å™¨
"""
from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

def calculate_made(data):
    median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42

# section æ•°æ®é¢„å¤„ç†
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
# print("X_train åŸå§‹ç´¢å¼•:", train_indices)
# print("X_test åŸå§‹ç´¢å¼•:", test_indices)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
# å°† X å’Œ y ç»„åˆä¸ºä¸€ä¸ª numpy æ•°ç»„
combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# åˆ›å»ºæ–°çš„ DataFrame
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ•´æ•°ç¼–ç 
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# SECTION Mğ‘œ (ğ‘¡, D)
# subsection é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOAD
clf_gold = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf_gold.fit(X_train, y=None)

# subsection é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨DeepSAD
clf_deep = DeepSAD(epochs=1, hidden_dims=20,
                   device=device,
                   random_state=42)
anom_id = np.where(y_train == 1)[0]
known_anom_id = np.random.choice(anom_id, 10, replace=False)
y_semi = np.zeros_like(y_train, dtype=int)
y_semi[known_anom_id] = 1
clf_deep.fit(X_train, y_semi)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
clf = clf_gold
train_scores = clf.decision_function(X_train)
train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))

# SECTION è°“è¯loss(M, D, ğ‘¡)çš„å®ç°
# SUBSECTION SVMæ¨¡å‹çš„å®ç°
# svm_model = svm.SVC()
svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
# è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]

# SUBSECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šè®¾ç½®é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
bad_samples = np.where(hinge_loss > 1)[0]
print("æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", bad_samples)

# subsection åŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
print("*" * 100)
print("åŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
print("*" * 100)

# SECTION Mğ‘ (ğ‘…, ğ´, M)
top_k_svm = 6
# æå–ç³»æ•°
feature_importances_coef = np.abs(svm_model.coef_[0])
# å¯¹ç³»æ•°è¿›è¡Œæ’åº
top_k_indices = np.argpartition(-feature_importances_coef, top_k_svm)[:top_k_svm]
print("SVMæ¨¡å‹é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•æ˜¯ï¼š", top_k_indices)
important_features_coef = top_k_indices

# section imbalanced(ğ·, ğ‘…, ğ‘¡ .ğ´, ğ›¿)ï¼Œé€‚ç”¨äºæ•´ä¸ªè¾“å…¥æ•°æ®ğ·
from sklearn.preprocessing import MinMaxScaler
# è®¾ç½®åˆ†ç»„çš„é—´éš”
interval = 0.01
# åˆå§‹åŒ–MinMaxScaler
scaler = MinMaxScaler()
col_indices = 10
row_indices = 100
train_row_number = X_train.shape[0]
select_feature = feature_names[col_indices]
data_imbalance = pd.read_excel(file_path)
data_imbalance[data.columns] = scaler.fit_transform(data[data.columns])
ta = data_imbalance.iloc[row_indices, col_indices]
# å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
digitized = np.digitize(data_imbalance[select_feature], bins)
# ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
unique_bins, counts = np.unique(digitized, return_counts=True)
# æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
ta_bin = np.digitize([ta], bins)[0]
# æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
ta_count = counts[unique_bins == ta_bin][0]
# è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
median_imbalance, made_imbalance = calculate_made(counts)
lower_threshold = median_imbalance - 2 * made_imbalance
upper_threshold = median_imbalance + 2 * made_imbalance
if ta_count < lower_threshold or ta_count > upper_threshold:
    print("æ‰€é€‰åˆ—Aåœ¨æ‰€é€‰å…ƒç»„tå¤„æ˜¯ä¸å¹³è¡¡çš„")
else:
    print("æ‰€é€‰åˆ—Aåœ¨æ‰€é€‰å…ƒç»„tå¤„æ˜¯å¹³è¡¡çš„")

# section SDomain(ğ·, ğ‘…, ğ´, ğœ)
from sklearn.preprocessing import MinMaxScaler
# è®¾ç½®åˆ†ç»„çš„é—´éš”
interval = 0.01
col_indices = 10
selected_bins = 0
columns_bins = {}
columns_bins_count = []
# åˆå§‹åŒ–MinMaxScaler
scaler = MinMaxScaler()
select_feature = feature_names[col_indices]
data_minmax = pd.read_excel(file_path)
data_minmax[data.columns] = scaler.fit_transform(data[data.columns])
# å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
# ç»Ÿè®¡æ¯åˆ—æ•°æ®å æ®äº†å¤šå°‘ä¸ªé—´éš”
for column in data_minmax.columns:
    digitized = np.digitize(data_minmax[column], bins)
    unique_bins, counts = np.unique(digitized, return_counts=True)
    print(f"åˆ— '{column}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
    columns_bins[column] = len(unique_bins)
    columns_bins_count.append(len(unique_bins))
    if column == select_feature:
        selected_bins = len(unique_bins)
median, made = calculate_made(np.array(columns_bins_count))
lower_threshold = median - 2 * made
upper_threshold = median + 2 * made
if selected_bins < lower_threshold:
    print("æ‰€é€‰åˆ—Açš„æ´»åŠ¨åŸŸè¿‡å°")
else:
    print("æ‰€é€‰åˆ—Açš„æ´»åŠ¨åŸŸæ­£å¸¸")

# section ç¡®å®šæ‰€é€‰å±æ€§Aä¸‹å…ƒç»„tæ˜¯å¦ä¸ºugly outliers
# å¼‚å¸¸æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å¼‚å¸¸å€¼ (è®­ç»ƒé›†ä¸­æ‰€æœ‰çš„outliers)
outliers_index_numpy = np.array(train_outliers_index)
# å¼‚å¸¸å€¼ä¸­å¯¼è‡´SVMåˆ†ç±»é”™è¯¯çš„éƒ¨åˆ†ï¼ˆç­›é€‰å‡ºè®­ç»ƒé›†ä¸­æ‰€æœ‰çš„bad outliersï¼‰
intersection = np.intersect1d(outliers_index_numpy, bad_samples)
# æ˜ å°„å›åŸå§‹æ•°æ®é›†å¯¹åº”çš„å…ƒç»„ç´¢å¼•
initial_intersection = train_indices[intersection]
# åœ¨è¿™éƒ¨åˆ†åˆ†ç±»é”™è¯¯çš„å¼‚å¸¸å€¼ä¸­ç»§ç»­å¼•å…¥è°“è¯ (åœ¨bad outliersçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥åŒºåˆ†å‡ºugly outliers)
influential_features = important_features_coef
# subsection åˆ¤æ–­å“ªäº›influential featuresçš„æ´»åŠ¨åŸŸè¿‡å° ï¼ˆSDomain(ğ·, ğ‘…, ğ´, ğœ)ï¼‰
feature_with_small_domain = []
for feature in influential_features:
    select_feature = feature_names[feature]
    digitized = np.digitize(data_minmax[select_feature], bins)
    unique_bins, counts = np.unique(digitized, return_counts=True)
    selected_bins = len(unique_bins)
    if selected_bins < lower_threshold:
        feature_with_small_domain.append(feature)
print("æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸­æ´»åŠ¨åŸŸè¿‡å°çš„ç‰¹å¾ç´¢å¼•ä¸ºï¼š", feature_with_small_domain)
# subsection åˆ¤æ–­æ´»åŠ¨åŸŸè¿‡å°çš„ç‰¹å¾ä¸­å“ªäº›å…ƒç»„ä¸ºä¸å¹³è¡¡çš„å…ƒç»„ï¼ˆimbalanced(ğ·, ğ‘…, ğ‘¡ .ğ´, ğ›¿)ï¼‰
selected_tuples = []
for small_feature in feature_with_small_domain:
    for row_index in original_indices:
        ta = data_imbalance.iloc[row_index, small_feature]
        select_feature = feature_names[small_feature]
        # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
        # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
        bins = np.arange(0, 1.01, interval)
        digitized = np.digitize(data_imbalance[select_feature], bins)
        # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
        unique_bins, counts = np.unique(digitized, return_counts=True)
        # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
        ta_bin = np.digitize([ta], bins)[0]
        # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
        ta_count = counts[unique_bins == ta_bin][0]
        # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
        median, made = calculate_made(counts)
        lower_threshold = median - 2 * made
        upper_threshold = median + 2 * made
        if ta_count < lower_threshold or ta_count > upper_threshold:
            selected_tuples.append(row_index)
# ugly_outliers = set(selected_tuples).union(set(initial_intersection))
ugly_outliers = set(selected_tuples).intersection(set(initial_intersection))
print("bad outliersä¸ºï¼š", set(initial_intersection))
print("ugly outliersä¸ºï¼š", ugly_outliers)