"""
ugly outliersçš„ä¿®å¤
åˆ†ç±»å™¨ä¸ºlinearæ ¸çš„svmåˆ†ç±»å™¨
è¾“å…¥æ•°æ®é›†ä¸ºçœŸå®žæ•°æ®é›†
é€‚ç”¨äºŽå¤šåˆ†ç±»çš„æƒ…å†µ
ä¸€ã€åŽ»é™¤è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼å’Œåˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽæœªèƒ½æé«˜æµ‹è¯•é›†ä¸Šåˆ†ç±»å‡†ç¡®åº¦çš„å¯èƒ½åŽŸå› ï¼š
1ã€æ•°æ®é›†çš„å¤æ‚æ€§ï¼šå¼‚å¸¸å€¼çš„æ¯”ä¾‹è¾ƒå°ï¼Œæˆ–è€…æ•°æ®æœ¬èº«çš„åˆ†å¸ƒå’Œç‰¹å¾è¾ƒä¸ºå¤æ‚ï¼Œåˆ é™¤è¿™äº›æ ·æœ¬å¯¹æ•´ä½“å‡†ç¡®åº¦çš„æå‡å¯èƒ½ä¸æ˜¾è‘—
2ã€æ ·æœ¬çš„ä»£è¡¨æ€§: å¦‚æžœæ•°æ®é›†ä¸­çš„å¤§å¤šæ•°æ ·æœ¬æ˜¯æ­£å¸¸çš„æˆ–åˆç†çš„ï¼Œå³ä½¿åˆ é™¤äº†å°‘é‡å¼‚å¸¸æ ·æœ¬ï¼Œæ¨¡åž‹çš„å­¦ä¹ æ•ˆæžœå’Œå‡†ç¡®åº¦æå‡ä¹Ÿå¯èƒ½æœ‰é™
3ã€æ¨¡åž‹çš„è¡¨çŽ°ä¸Žæ•°æ®çš„å…³ç³»ï¼šå¦‚æžœæ¨¡åž‹çš„è¶…å‚æ•°ï¼ˆå¦‚ C å’Œ gammaï¼‰æ²¡æœ‰ç»è¿‡ä¼˜åŒ–ï¼Œæˆ–æ•°æ®ç‰¹å¾ä¸å……åˆ†ï¼Œé‡æ–°è®­ç»ƒçš„æ¨¡åž‹å¯èƒ½ä¸ä¼šæ¯”åŽŸå§‹æ¨¡åž‹æœ‰æ˜¾è‘—çš„æ”¹è¿›
4ã€å¼‚å¸¸å€¼çš„ç±»åž‹: å¦‚æžœå¼‚å¸¸å€¼å¯¹æ¨¡åž‹çš„å½±å“ä¸å¤§ï¼Œæˆ–è€…å¼‚å¸¸å€¼åœ¨æµ‹è¯•é›†ä¸­çš„è¡¨çŽ°ä¸Žè®­ç»ƒé›†ä¸­çš„è¡¨çŽ°ç›¸ä¼¼ï¼Œåˆ é™¤è¿™äº›å¼‚å¸¸å€¼å¯èƒ½ä¸ä¼šå¯¹æ¨¡åž‹çš„æ€»ä½“æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“
5ã€æ ·æœ¬é”™è¯¯: è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯æ ·æœ¬å¯èƒ½æ˜¯ç”±æ ‡ç­¾é”™è¯¯å¼•èµ·çš„ï¼Œè€Œä¸æ˜¯æ¨¡åž‹æ— æ³•å¤„ç†çš„æ•°æ®ç‰¹æ€§ã€‚å¦‚æžœæ ‡ç­¾æœ¬èº«ä¸å‡†ç¡®ï¼Œåˆ é™¤è¿™äº›é”™è¯¯æ ·æœ¬å¯èƒ½ä¸ä¼šæé«˜æ¨¡åž‹çš„æ€§èƒ½
6ã€è®­ç»ƒæ ·æœ¬çš„æ•°é‡: åˆ é™¤æ ·æœ¬å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒé›†æ ·æœ¬é‡å‡å°‘ã€‚å¦‚æžœåˆ é™¤çš„æ ·æœ¬æ¯”ä¾‹è¾ƒé«˜ï¼Œå¯èƒ½ä¼šå½±å“è®­ç»ƒé›†çš„ä»£è¡¨æ€§ï¼Œè¿›è€Œå½±å“æ¨¡åž‹çš„æ€§èƒ½ã€‚
7ã€è¿‡æ‹Ÿåˆé—®é¢˜: å¦‚æžœåœ¨å¤„ç†å¼‚å¸¸å€¼å’Œé”™è¯¯æ ·æœ¬æ—¶è¿‡åº¦è°ƒæ•´è®­ç»ƒæ•°æ®ï¼Œå¯èƒ½å¯¼è‡´æ¨¡åž‹è¿‡æ‹Ÿåˆï¼Œä»Žè€Œåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—æå‡ã€‚
8ã€è¯„ä¼°æ ‡å‡†: ä½¿ç”¨åˆ†ç±»å‡†ç¡®åº¦ä½œä¸ºæ€§èƒ½è¯„ä¼°æ ‡å‡†å¯èƒ½ä¸æ€»æ˜¯æœ€åˆé€‚çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é›†ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ã€‚è€ƒè™‘ä½¿ç”¨å…¶ä»–è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚F1åˆ†æ•°ã€ROC-AUCç­‰ï¼Œæ¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡åž‹æ€§èƒ½ã€‚
äºŒã€åœ¨ç±»åˆ«ä¸å‡è¡¡çš„å‰æä¸‹ï¼Œä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡æœ€åˆé€‚ï¼š
1ã€F1-Score: ç»“åˆäº†ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡çš„è°ƒå’Œå¹³å‡æ•°ï¼Œé€‚åˆä¸å‡è¡¡æ•°æ®ï¼Œå› ä¸ºå®ƒå¯¹æ­£è´Ÿæ ·æœ¬çš„æ¯”ä¾‹ä¸æ•æ„Ÿã€‚ç‰¹åˆ«é€‚åˆå½“ä½ éœ€è¦å¹³è¡¡ç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡æ—¶ä½¿ç”¨ã€‚
2ã€ç²¾ç¡®çŽ‡-å¬å›žçŽ‡æ›²çº¿ï¼ˆPrecision-Recall Curveï¼‰: åœ¨ç±»åˆ«ä¸å‡è¡¡æ—¶æ¯” ROC æ›²çº¿æ›´å…·ä¿¡æ¯é‡ï¼Œå› ä¸ºå®ƒç›´æŽ¥å…³æ³¨æ­£ç±»çš„è¡¨çŽ°ã€‚
3ã€ROC-AUC: å°½ç®¡ ROC-AUC åœ¨ç±»åˆ«ä¸å‡è¡¡æ—¶å¯èƒ½ä¼šé«˜ä¼°æ¨¡åž‹æ€§èƒ½ï¼Œä½†å®ƒä»ç„¶èƒ½æä¾›æ•´ä½“æ€§èƒ½çš„è‰¯å¥½æ¦‚è§ˆï¼Œå°¤å…¶æ˜¯å½“ä½ å¯¹å‡é˜³æ€§çŽ‡å’ŒçœŸæ­£çŽ‡çš„æƒè¡¡æ„Ÿå…´è¶£æ—¶ã€‚
4ã€åŠ æƒå¹³å‡æŒ‡æ ‡: ä½¿ç”¨åŠ æƒ F1-score æˆ–åŠ æƒç²¾ç¡®çŽ‡å’Œå¬å›žçŽ‡ï¼Œå¯ä»¥å¯¹æ¯ä¸ªç±»åˆ«çš„è¡¨çŽ°è¿›è¡ŒåŠ æƒï¼Œä»Žè€Œé€‚åº”ç±»åˆ«ä¸å‡è¡¡çš„æƒ…å†µã€‚
ä¸‰ã€å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å’Œç¼–ç æ˜¯æœ‰å¿…è¦çš„
æ ‡å‡†åŒ–ï¼šç¡®ä¿ç‰¹å¾åœ¨ç›¸ä¼¼çš„å°ºåº¦èŒƒå›´å†…ï¼Œå‡å°‘æ¨¡åž‹è®­ç»ƒçš„å¤æ‚åº¦ï¼Œæé«˜æ”¶æ•›é€Ÿåº¦ã€‚
åˆ†ç±»ç‰¹å¾ç¼–ç ï¼šå°†åˆ†ç±»æ•°æ®è½¬åŒ–ä¸ºæ•°å€¼å½¢å¼ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæ­£ç¡®åœ°å¤„ç†è¿™äº›ç‰¹å¾ã€‚
è®­ç»ƒæ—¶é—´ï¼šæœªç»æ ‡å‡†åŒ–å’Œç¼–ç çš„æ•°æ®å¯èƒ½ä¼šå¯¼è‡´ SVM è®­ç»ƒè¿‡ç¨‹å˜æ…¢ï¼Œå› ä¸ºæ¨¡åž‹éœ€è¦åœ¨æ›´å¤æ‚çš„ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ã€‚
"""
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

def calculate_made(data):
    median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸Žä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42

# section æ•°æ®é¢„å¤„ç†

# subsection çœŸå®žæ•°æ®é›†ä¸”å¯¹æ•°æ®é›†çš„ç‰¹å¾è¿›è¡Œäº†å¤„ç†
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŽŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
# print("X_train åŽŸå§‹ç´¢å¼•:", train_indices)
# print("X_test åŽŸå§‹ç´¢å¼•:", test_indices)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
# å°† X å’Œ y ç»„åˆä¸ºä¸€ä¸ª numpy æ•°ç»„
combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# åˆ›å»ºæ–°çš„ DataFrame
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ•´æ•°ç¼–ç 
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# subsection çœŸå®žæ•°æ®é›†ä¸”å¯¹æ•°æ®é›†çš„ç‰¹å¾æ²¡æœ‰è¿›è¡Œæ ‡å‡†åŒ–å’Œç¼–ç å¤„ç†
# file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
# data = pd.read_excel(file_path)
# enc = LabelEncoder()
# data['Class'] = enc.fit_transform(data['Class'])
# X = data.values[:, :-1]
# y = data.values[:, -1]
# # è®°å½•åŽŸå§‹ç´¢å¼•
# original_indices = np.arange(len(X))
# X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
# class_names = enc.classes_
# feature_names = data.columns.values.tolist()
# # åˆ›å»ºæ–°çš„ DataFrame
# data_copy = data.copy()

# subsection çœŸå®žæ•°æ®é›†ä¸”åœ¨æ•°æ®é›†ä¸­äººä¸ºå¼•å…¥å™ªå£°
# file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
# data = pd.read_excel(file_path)
# enc = LabelEncoder()
# data['Class'] = enc.fit_transform(data['Class'])
# X = data.values[:, :-1]
# y = data.values[:, -1]
# # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
# X = StandardScaler().fit_transform(X)
# # åŠ å…¥éšæœºå™ªå£°
# noise_level = 0.1
# # è®¡ç®—å™ªå£°æ•°é‡
# n_samples = X.shape[0]
# n_noise = int(noise_level * n_samples)
# # éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
# noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# # æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
# X[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# # print("Data with noise:\n", X.head())
# # è®°å½•åŽŸå§‹ç´¢å¼•
# original_indices = np.arange(len(X))
# X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
# feature_names = data.columns.values.tolist()
# combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# # åˆ›å»ºæ–°çš„ DataFrame
# data_copy = pd.DataFrame(combined_array, columns=feature_names)
# # è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŽŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
# train_noise = np.intersect1d(train_indices, noise_indices)
# # æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŽŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
# test_noise = np.intersect1d(test_indices, noise_indices)
# print("è®­ç»ƒé›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", train_noise)
# print("æµ‹è¯•é›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", test_noise)

# SECTION Mð‘œ (ð‘¡, D)
# subsection é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOAD
clf_gold = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf_gold.fit(X_train, y=None)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
clf = clf_gold
train_scores = clf.decision_function(X_train)
train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
train_outliers_index = []
train_outliers_confidence = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
        train_outliers_confidence.append(train_confidence[i])
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼çš„ç½®ä¿¡åº¦ï¼š", train_outliers_confidence)

# SECTION SVMæ¨¡åž‹çš„å®žçŽ°
# svm_model = svm.SVC()
svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
# è¢«SVMæ¨¡åž‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]
# æµ‹è¯•æ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
wrong_test_indices = np.where(y_test != svm_model.predict(X_test))[0]

# SUBSECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šè®¾ç½®é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
bad_samples = np.where(hinge_loss > 1)[0]
soft_outliers = np.where((hinge_loss > 0) & (hinge_loss <= 1))[0]
correct_samples = np.where(hinge_loss == 0)[0]
# print("æŸå¤±å‡½æ•°é«˜äºŽæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", bad_samples)

# subsection åˆ¤å®šè®­ç»ƒæ•°æ®ä¸­å¼‚å¸¸å€¼å¯èƒ½å¯¼è‡´åˆ†ç±»é”™è¯¯çš„æ ·æœ¬
# è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¯¼è‡´SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
inter_outliers = list(set(train_outliers_index) & set(bad_samples))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä½†æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®ä¸è¶³é˜ˆå€¼1
inter_soft_outliers = list(set(train_outliers_index) & set(soft_outliers))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä¸”æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®è¶…è¿‡é˜ˆå€¼1
inter_correct_class = list(set(train_outliers_index) & set(correct_samples))
# è®­ç»ƒæ•°æ®ä¸­è¢«SVMè¯¯åˆ†ç±»çš„æ ·æœ¬ä¸Žè®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼çš„äº¤é›†
intersection = np.intersect1d(bad_samples, wrong_classified_indices)
# è®­ç»ƒæ•°æ®ä¸­è¢«SVMè¯¯åˆ†ç±»çš„æ ·æœ¬ä¸­æœªè¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬
diff_elements = np.setdiff1d(wrong_classified_indices, intersection)
print("åˆ†ç±»é”™è¯¯çš„æ ·æœ¬ä¸­æœªè¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ç´¢å¼•ï¼š", diff_elements)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°é«˜äºŽé˜ˆå€¼1çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_outliers)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°åœ¨0å’Œé˜ˆå€¼1ä¹‹é—´çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_soft_outliers)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°ä¸º0çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_correct_class)

# SECTION åŽŸå§‹æ•°æ®ä¸­çš„svmåœ¨å„ç±»è¯„ä»·æŒ‡æ ‡ä¸‹çš„è¡¨çŽ°
print("*" * 100)

# subsection è®¡ç®—SVMçš„åˆ†ç±»å‡†ç¡®åº¦
# å‡†ç¡®åº¦æ˜¯æŒ‡æ¨¡åž‹æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹
print("åŽŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŽŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))

# subsection è®¡ç®— F1 åˆ†æ•°
f1_train = f1_score(y_train, svm_model.predict(X_train), average='weighted')
f1_test = f1_score(y_test, svm_model.predict(X_test), average='weighted')
print("åŽŸå§‹è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train))
print("åŽŸå§‹æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test))

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­åˆ†ç±»é”™è¯¯ï¼ˆhingeæŸå¤±å‡½æ•°é«˜äºŽ1ï¼‰ä¸”è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
print("*" * 100)
# subsection è®¡ç®— accuracyåˆ†æ•°
# ç”Ÿæˆå¸ƒå°”ç´¢å¼•ï¼Œä¸ºè¦åˆ é™¤çš„è¡Œåˆ›å»ºå¸ƒå°”å€¼æ•°ç»„
mask = np.ones(len(X_train), dtype=bool)
mask[inter_outliers] = False
# ä½¿ç”¨å¸ƒå°”ç´¢å¼•åˆ é™¤é‚£äº›æ—¢è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œåˆä½¿å¾—hingeæŸå¤±é«˜äºŽ1çš„æ ·æœ¬
X_train_split = X_train[mask]
y_train_split = y_train[mask]
# é‡æ–°è®­ç»ƒSVMæ¨¡åž‹
svm_model_split = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_split.fit(X_train_split, y_train_split)
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_split, svm_model_split.predict(X_train_split))))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_split.predict(X_test))))

print("åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_train[wrong_classified_indices], svm_model.predict(X_train[wrong_classified_indices]))))
print("åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_test[wrong_test_indices], svm_model.predict(X_test[wrong_test_indices]))))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_train[wrong_classified_indices], svm_model_split.predict(X_train[wrong_classified_indices]))))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_test[wrong_test_indices], svm_model_split.predict(X_test[wrong_test_indices]))))

# subsection è®¡ç®— F1 åˆ†æ•°
print("*" * 100)
f1_train_split = f1_score(y_train_split, svm_model_split.predict(X_train_split), average='weighted')
f1_test_split = f1_score(y_test, svm_model_split.predict(X_test), average='weighted')
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_split))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_split))

print("åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_train[wrong_classified_indices], svm_model.predict(X_train[wrong_classified_indices]), average='weighted')))
print("åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_test[wrong_test_indices], svm_model.predict(X_test[wrong_test_indices]), average='weighted')))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_train[wrong_classified_indices], svm_model_split.predict(X_train[wrong_classified_indices]), average='weighted')))
print("åŽ»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_test[wrong_test_indices], svm_model_split.predict(X_test[wrong_test_indices]), average='weighted')))

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­hingeæŸå¤±å‡½æ•°é«˜äºŽ1çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
print("*" * 100)

# subsection è®¡ç®— accuracyåˆ†æ•°
mask_h = np.ones(len(X_train), dtype=bool)
mask_h[bad_samples] = False
X_train_h = X_train[mask_h]
y_train_h = y_train[mask_h]
svm_model_h = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_h.fit(X_train_h, y_train_h)
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_h, svm_model_h.predict(X_train_h))))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_h.predict(X_test))))

print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽåŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_train[wrong_classified_indices], svm_model_h.predict(X_train[wrong_classified_indices]))))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_test[wrong_test_indices], svm_model_h.predict(X_test[wrong_test_indices]))))

# subsection è®¡ç®— F1 åˆ†æ•°
print("*" * 100)
f1_train_h = f1_score(y_train_h, svm_model_h.predict(X_train_h), average='weighted')
f1_test_h = f1_score(y_test, svm_model_h.predict(X_test), average='weighted')
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_h))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽåŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_h))

print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_train[wrong_classified_indices], svm_model_h.predict(X_train[wrong_classified_indices]), average='weighted')))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_test[wrong_test_indices], svm_model_h.predict(X_test[wrong_test_indices]), average='weighted')))

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
print("*" * 100)

# subsection è®¡ç®— accuracyåˆ†æ•°
mask_o = np.ones(len(X_train), dtype=bool)
mask_o[train_outliers_index] = False
X_train_o = X_train[mask_o]
y_train_o = y_train[mask_o]
svm_model_o = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_o.fit(X_train_o, y_train_o)
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_o, svm_model_o.predict(X_train_o))))
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_o.predict(X_test))))

print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_train[wrong_classified_indices], svm_model_o.predict(X_train[wrong_classified_indices]))))
print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" +
      str(accuracy_score(y_test[wrong_test_indices], svm_model_o.predict(X_test[wrong_test_indices]))))

# subsection è®¡ç®— F1 åˆ†æ•°
print("*" * 100)
f1_train_o = f1_score(y_train_o, svm_model_o.predict(X_train_o), average='weighted')
f1_test_o = f1_score(y_test, svm_model_o.predict(X_test), average='weighted')
print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_train_o))
print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_test_o))

print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_train[wrong_classified_indices], svm_model_o.predict(X_train[wrong_classified_indices]), average='weighted')))
print("èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬åŽï¼Œé‡æ–°è®­ç»ƒçš„SVMï¼Œåœ¨åŽŸSVMæ¨¡åž‹åˆ†é”™çš„æµ‹è¯•æ•°æ®ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" +
      str(f1_score(y_test[wrong_test_indices], svm_model_o.predict(X_test[wrong_test_indices]), average='weighted')))