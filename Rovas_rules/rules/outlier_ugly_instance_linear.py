"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´, M) â†’ ugly(ğ‘¡)çš„å…·ä½“å®ç°
åˆ†ç±»å™¨ä¸ºlinearæ ¸çš„svmåˆ†ç±»å™¨
"""
# unsupervised methods
from collections import Counter

from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section é¢„å¤„ç†æ•°æ®é›†
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.5, random_state=1)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
# ä½¿ç”¨å¤„ç†åçš„Xå’Œyç»„åˆæˆæ–°çš„data_copy
column_names = data.columns.tolist()
# å°† X å’Œ y ç»„åˆä¸ºä¸€ä¸ª numpy æ•°ç»„
combined_array = np.hstack((X, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# åˆ›å»ºæ–°çš„ DataFrame
data_copy = pd.DataFrame(combined_array, columns=column_names)
# å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œæ•´æ•°ç¼–ç 
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°

threshold = 0.01
col_indices = 3
row_indices = 10
select_feature = feature_names[col_indices]
# è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
select_column_data = data_copy[select_feature].values
# æ‰¾åˆ°æ‰€é€‰åˆ—çš„æœ€å¤§å€¼å’Œæœ€å°å€¼
max_value = np.max(select_column_data)
min_value = np.min(select_column_data)
# æ‰¾åˆ°t.Aå¯¹åº”çš„å€¼
t_value = data_copy.iloc[row_indices, col_indices]
# å¯¹æ•°æ®è¿›è¡Œæ’åº
# sorted_data = np.sort(select_column_data)
sorted_indices = np.argsort(select_column_data)
sorted_data = select_column_data[sorted_indices]
# æ‰¾åˆ°æœ€æ¥è¿‘çš„æ¯” t_value å¤§çš„å€¼å’Œæ¯” t_value å°çš„å€¼
greater_than_t_value = sorted_data[sorted_data > t_value]
less_than_t_value = sorted_data[sorted_data < t_value]
# æ‰¾åˆ°ä¸t_valueæœ€æ¥è¿‘çš„æ›´å¤§çš„å€¼å’Œæ›´å°çš„å€¼
if greater_than_t_value.size > 0:
    closest_greater = greater_than_t_value[0]  # æœ€è¿‘çš„å¤§äº t_value çš„å€¼
else:
    closest_greater = t_value
if less_than_t_value.size > 0:
    closest_less = less_than_t_value[-1]  # æœ€è¿‘çš„å°äº t_value çš„å€¼
else:
    closest_less = t_value
# åˆ¤æ–­t.Aæ˜¯å¦æ˜¯å¼‚å¸¸å€¼
if max_value == t_value:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold)
elif min_value == t_value:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", closest_greater - t_value > threshold)
else:
    print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold and t_value - closest_less > threshold)
# æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
outliers = []
outliers_index = []
# æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
if len(sorted_data) > 1:
    if (sorted_data[1] - sorted_data[0] >= threshold):
        outliers.append(sorted_data[0])
        outliers_index.append(sorted_indices[0])
    if (sorted_data[-1] - sorted_data[-2] >= threshold):
        outliers.append(sorted_data[-1])
        outliers_index.append(sorted_indices[-1])
# æ£€æŸ¥ä¸­é—´å…ƒç´ 
for i in range(1, len(sorted_data) - 1):
    current_value = sorted_data[i]
    left_value = sorted_data[i - 1]
    right_value = sorted_data[i + 1]
    if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
        outliers.append(current_value)
        outliers_index.append(sorted_indices[i])
outliers_index_numpy = np.array(outliers_index)
# åœ¨æ•´ä¸ªæ•°æ®é›†Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼çš„ç´¢å¼•ä¸ºï¼š", outliers_index)
print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼ä¸ºï¼š", outliers)

# SECTION è°“è¯loss(M, D, ğ‘¡)çš„å®ç°

#  SVMæ¨¡å‹è®­ç»ƒå’Œåˆ†ç±»å‡†ç¡®åº¦
# svm_model = svm.SVC()
svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
# è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# åœ¨è®­ç»ƒæ•°æ®ä¸­è¢«è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]

# SUBSECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šè®¾ç½®é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
# åœ¨è®­ç»ƒæ•°æ®ä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
bad_samples = np.where(hinge_loss > 1)[0]
print("æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", bad_samples)

# subsection åŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
print("*" * 100)
print("åŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
print("*" * 100)

# section è°“è¯ Mğ‘ (ğ‘…, ğ´, M) çš„å®ç°
# SUBSECTION å€ŸåŠ©æ–¹å·®åˆ¤åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾
top_k_var = 6
variances = np.var(X_train, axis=0)
top_k_indices_var = np.argsort(variances)[-top_k_var:][::-1]
print("æ–¹å·®æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_var, top_k_indices_var))

# SUBSECTION sklearnåº“çš„SelectKBesté€‰æ‹©å™¨ï¼Œå€ŸåŠ©Fisheræ£€éªŒç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
top_k_fisher = 6
selector = SelectKBest(score_func=f_classif, k=top_k_fisher)
y_trans_fisher = y_train.reshape(-1)
X_new = selector.fit_transform(X_train, y_trans_fisher)
# è·å–è¢«é€‰ä¸­çš„ç‰¹å¾çš„ç´¢å¼•
selected_feature_indices = selector.get_support(indices=True)
print("SelectKBesté€‰æ‹©å™¨å€ŸåŠ©Fisheræ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_fisher, selected_feature_indices))

# SUBSECTION å€ŸåŠ©CARTå†³ç­–æ ‘ç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
# top_k_cart = 6
# classifier = DecisionTreeClassifier()
# classifier.fit(X_train, y_train)
# # è·å–ç‰¹å¾é‡è¦æ€§å¾—åˆ†
# feature_importance = classifier.feature_importances_
# # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
# sorted_indices = np.argsort(feature_importance)[::-1]
# # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
# top_k_features = sorted_indices[:top_k_cart]
# print("CARTå†³ç­–æ ‘æ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_cart, top_k_features))

# SUBSECTION sklearnåº“SelectFromModelé€‰æ‹©å™¨(é€‚ç”¨äºSVMçº¿æ€§æ ¸)
# ä½¿ç”¨SelectFromModelæ¥é€‰æ‹©é‡è¦ç‰¹å¾
sfm = SelectFromModel(svm_model, threshold='mean', prefit=True)
X_selected = sfm.transform(X_train)
# è·å–é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
selected_idx = sfm.get_support(indices=True)
# æ‰“å°é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
print("SelectFromModelé€‰æ‹©å™¨é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:", selected_idx)

# SUBSECTION åŸºäºSVMæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§(ä¸€èˆ¬æƒ…å†µä¸‹SVMçš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°æ–¹æ³•æ›´å…·å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§)
# å¦‚æœä½ çš„æ¨¡å‹ä¸æ˜¯çº¿æ€§SVMï¼Œåˆ™éœ€è¦ç¡®ä¿ä½¿ç”¨çš„æ˜¯çº¿æ€§æ ¸ SVMï¼Œå¦åˆ™ coef_ å¯èƒ½ä¸å­˜åœ¨
top_k_svm = 6
# æå–ç³»æ•°
feature_importances_coef = np.abs(svm_model.coef_[0])
# å¯¹ç³»æ•°è¿›è¡Œæ’åº
top_k_indices = np.argpartition(-feature_importances_coef, top_k_svm)[:top_k_svm]
print("SVMæ¨¡å‹é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•æ˜¯ï¼š", top_k_indices)

# SUBSECTION æ— æ¨¡å‹(éå‚æ•°)æ–¹æ³•ä¸­çš„Permutation Feature Importance-slearn
result = permutation_importance(svm_model, X_train, y_train, n_repeats=10,random_state=42)
permutation_importance = result.importances_mean
top_k_permutation = np.argpartition(-permutation_importance, top_k_svm)[:top_k_svm]
print("Permutation Feature Importance-slearnæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_svm, top_k_permutation))

# SUBSECTION LIME(Local Interpretable Model-Agnostic Explanation)
i = 16
np.random.seed(1)
explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)
# predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
predict_fn = lambda x: svm_model.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
important_features = []
for feature_set in top_features:
    feature_long = feature_set[0]
    for feature in feature_names:
        if set(feature).issubset(set(feature_long)):
            important_features.append(feature)
            break
# lime_importances = np.zeros(X_train.shape[1])
# for i in range(X_train.shape[0]):
#     exp = explainer.explain_instance(X_train[i], svm_model.predict_proba, num_features=X_train.shape[1])
#     for feature, importance in exp.as_list():
#         idx = data.columns[:-1].get_loc(feature)
#         lime_importances[idx] += np.abs(importance)
important_feature_indices = [feature_names.index(feature_name) for feature_name in important_features]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_svm, important_feature_indices))

# SUBSECTION SHAP (SHapley Additive exPlanations) æä¾›æ¯ä¸ªç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„è´¡çŒ®åº¦ï¼Œé€‚ç”¨äºä»»ä½•æ¨¡å‹ï¼ŒåŒ…æ‹¬éçº¿æ€§æ ¸ SVMï¼ˆæ‰§è¡Œå¤ªæ…¢äº†ï¼‰
# # ä»åŸå§‹æ•°æ®é›†ä¸­é‡‡æ ·
# np.random.seed(42)  # For reproducibility
# indices = np.random.choice(X_train.shape[0], size=100, replace=False)
# background_data = X_train[indices]
# # åˆ›å»ºæ¨¡å‹è§£é‡Šå™¨
# explainer = shap.KernelExplainer(svm_model.predict_proba, background_data)
# shap_values = explainer.shap_values(X_train)
# # è®¡ç®—ç‰¹å¾é‡è¦æ€§
# importance = np.mean(np.abs(shap_values[0]), axis=0)
# # å¯¹ç‰¹å¾é‡è¦æ€§è¿›è¡Œæ’åº
# top_k_indices = np.argpartition(-importance, top_k_svm)[:top_k_svm]
# print("SHAPé€‰æ‹©çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•: {}".format(top_k_svm, top_k_indices))

# section ç¡®å®šæœ€é‡è¦çš„ç‰¹å¾

# subsection ä»…é‡‡ç”¨åŸºäºæ–¹å·®çš„æ–¹æ³•
important_features_var = top_k_indices_var

# subsection ä»…é‡‡ç”¨åŸºäºFisheræ£€éªŒçš„æ–¹æ³•
important_features_fisher = selected_feature_indices

# subsection ä»…é‡‡ç”¨sklearnåº“SelectFromModelé€‰æ‹©å™¨
important_features_sel = selected_idx

# subsection ä»…é‡‡ç”¨åŸºäºæ¨¡å‹ç³»æ•°çš„æ–¹æ³•
important_features_coef = top_k_indices

# subsection ä»…é‡‡ç”¨Permutation Importanceçš„æ–¹æ³•
important_features_perm = top_k_permutation

# subsection ä»…é‡‡ç”¨åŸºäºLIMEçš„æ–¹æ³•
important_features_lime = important_feature_indices

# subsection å°†åŸºäºç³»æ•°çš„æ–¹æ³•ï¼ŒPermutation Feature Importanceè¾“å‡ºçš„ç‰¹å¾é‡è¦æ€§åŠ æƒ
# å½’ä¸€åŒ–ç‰¹å¾é‡è¦æ€§
scaler = StandardScaler()
feature_importances = np.array([feature_importances_coef, permutation_importance]).T
feature_importances_normalized = scaler.fit_transform(feature_importances)
# è®¡ç®—åŠ æƒå¹³å‡
weights = [0.7, 0.3]
feature_importance_combined = np.dot(feature_importances_normalized, weights)
# Step 4: é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
top_k = 6
important_features_mix = np.argpartition(-feature_importance_combined, top_k)[:top_k]
print("ç‰¹å¾åŠ æƒæ–¹æ³•é€‰æ‹©çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•: {}".format(top_k, important_features_mix))

# subsection å°†ä¸Šè¿°æ‰€æœ‰æ–¹æ³•è¿”å›çš„é‡è¦ç‰¹å¾æŒ‰å‡ºç°æ¬¡æ•°ç”±é«˜åˆ°ä½å¾—åˆ°æœ€é‡è¦çš„Kä¸ªç‰¹å¾
# å°†æ‰€æœ‰ ndarrays åˆå¹¶åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­
all_indices = np.concatenate([important_features_var, important_features_fisher, important_features_sel,
                              important_features_coef, important_features_perm, important_features_lime])
# ç»Ÿè®¡æ¯ä¸ªç‰¹å¾å‡ºç°çš„æ¬¡æ•°
index_counts = Counter(all_indices)
# æŒ‰å‡ºç°æ¬¡æ•°ä»é«˜åˆ°ä½æ’åº
sorted_indices = sorted(index_counts.items(), key=lambda x: x[1], reverse=True)
# æå–æ’åºåçš„ç‰¹å¾ç´¢å¼•
sorted_feature_indices = [index for index, count in sorted_indices]
# é€‰æ‹©å‰ k ä¸ªæœ€é‡è¦çš„ç‰¹å¾
k = 6
top_k_indices = sorted_feature_indices[:k]
print("æ‰€æœ‰ç‰¹å¾ç´¢å¼•åŠå…¶å‡ºç°æ¬¡æ•°:", index_counts)
print("æŒ‰é¢‘æ¬¡æ’åºçš„ç‰¹å¾ç´¢å¼•:", sorted_indices)
print("é¢‘æ¬¡è®¡æ•°æ–¹æ³•é€‰æ‹©çš„æœ€é‡è¦çš„å‰ {} ä¸ªç‰¹å¾ç´¢å¼•: {}".format(k, top_k_indices))

# section ç¡®å®šæ‰€é€‰å±æ€§Aä¸‹å…ƒç»„tæ˜¯å¦ä¸ºugly outliers
intersection = np.intersect1d(outliers_index_numpy, train_indices[bad_samples])
influential_features = important_features_coef
if np.isin(row_indices, intersection) and np.isin(col_indices, influential_features):
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹æ˜¯ugly outliers")
elif np.isin(row_indices, intersection) and np.logical_not(col_indices, influential_features):
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹ä¸æ˜¯ugly outliersï¼Œæ˜¯bad outliers")
else:
    print("æ‰€é€‰å…ƒç»„tåœ¨å±æ€§Aä¸‹æ—¢ä¸æ˜¯ugly outliersï¼Œä¹Ÿä¸æ˜¯bad outliers")

if np.isin(col_indices, influential_features):
    print("Aç‰¹å¾æ˜¯æœ‰å½±å“åŠ›çš„ç‰¹å¾")
else:
    print("Aç‰¹å¾ä¸æ˜¯æœ‰å½±å“åŠ›çš„ç‰¹å¾")

# section ç¡®å®šæ‰€æœ‰æœ‰å½±å“åŠ›çš„å±æ€§ä¸‹çš„æ‰€æœ‰ugly outliers
ugly_outlier_list = {}
for column in influential_features:
    select_feature = feature_names[column]
    # è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
    select_column_data = data_copy[select_feature].values
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°æ‰€é€‰å±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„æœ‰å½±å“åŠ›ç‰¹å¾ä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.intersect1d(outliers_index_numpy, train_indices[bad_samples])
    ugly_outlier_list[column] = intersection

for idx, key in enumerate(ugly_outlier_list):
    value = ugly_outlier_list[key]
    print(f"ç¬¬ {key} åˆ—çš„ugly outliersä¸º: {value}")