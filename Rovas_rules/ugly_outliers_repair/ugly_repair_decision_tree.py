"""
æ–¹æ¡ˆä¸€æ•ˆæœå¥½ï¼Œæ–¹æ¡ˆäºŒæ•ˆæœæœ‰é™
é‡‡ç”¨å†³ç­–æ ‘ä¿®æ”¹å¼‚å¸¸å€¼çš„æ ‡ç­¾
é‡‡ç”¨ç»Ÿè®¡æ–¹æ³•ä¿®å¤å¼‚å¸¸å€¼çš„ç‰¹å¾
ä½¿ç”¨decision_treeåˆ†ç±»å™¨è¿›è¡Œå®éªŒ
"""
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import log_loss
from sklearn.tree import DecisionTreeClassifier

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†
file_path = "../../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:, :-1]
y = data.values[:, -1]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.2, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)

# section å‘æ•°æ®é›†ä¸­åŠ å™ª
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=all_columns)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)
# print("è®­ç»ƒé›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", train_noise)
# print("æµ‹è¯•é›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", test_noise)

# section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)

i = 16
np.random.seed(1)
categorical_features = [0, 6]
categorical_names = {}

# è®¾ç½®å†³ç­–æ ‘å‚æ•°
params = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5, scoring='accuracy')
grid_search.fit(X_train_copy, y_train)

# è·å–æœ€ä½³æ¨¡å‹
decision_tree_classifier = grid_search.best_estimator_

for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data_copy.iloc[:, feature])
    data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
    categorical_names[feature] = le.classes_

explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)

predict_fn = lambda x: decision_tree_classifier.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
important_features = []
for feature_set in top_features:
    feature_long = feature_set[0]
    for feature in feature_names:
        if set(feature).issubset(set(feature_long)):
            important_features.append(feature)
            break

top_k_indices = [feature_names.index(feature_name) for feature_name in important_features]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„

# choice è®¡ç®—åˆ†ç±»é”™è¯¯çš„æ ·æœ¬

# é¢„æµ‹æ¯ä¸ªæ ·æœ¬è¢«åˆ’åˆ†åˆ°çš„ç±»åˆ«
predicted_labels = decision_tree_classifier.predict(X_copy)
ugly_outlier_candidates = np.where(y != predicted_labels)[0]

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„

outlier_feature_indices = {}
threshold = 0.01
for column_indice in top_k_indices:
    select_feature = feature_names[column_indice]
    select_column_data = data_copy[select_feature].values
    max_value = np.max(select_column_data)
    min_value = np.min(select_column_data)
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
    # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    outlier_feature_indices[column_indice] = intersection
# print(outlier_feature_indices)

# SECTION decision_treeæ¨¡å‹çš„å®ç°

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„decision_treeæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
# ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# è·å–æœ€ä½³æ¨¡å‹
decision_tree_clf = grid_search.best_estimator_
train_label_pred = decision_tree_clf.predict(X_train)
test_label_pred = decision_tree_clf.predict(X_test)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„decision_treeæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
train_label_pred_noise = decision_tree_classifier.predict(X_train_copy)
test_label_pred_noise = decision_tree_classifier.predict(X_test_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆdecision_treeæ–¹æ³•ï¼‰
#  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’Œdecision_treeåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

# subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾

# ç¡®å®šæ•°æ®é›†Dä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„å’Œæ­£å¸¸å…ƒç»„
outlier_tuple_set = set()
for value in outlier_feature_indices.values():
    outlier_tuple_set.update(value)
X_copy_repair_indices = list(outlier_tuple_set)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

decision_tree = KNeighborsClassifier(n_neighbors=3)
decision_tree.fit(X_copy_inners, y_inners)

# é¢„æµ‹å¼‚å¸¸å€¼
y_pred = decision_tree.predict(X_copy_repair)

# æ›¿æ¢å¼‚å¸¸å€¼
y[X_copy_repair_indices] = y_pred
y_train = y[train_indices]
y_test = y[test_indices]

# subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒdecision_treeæ¨¡å‹

# ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5, scoring='accuracy')
grid_search.fit(X_train_copy, y_train)

# è·å–æœ€ä½³æ¨¡å‹
decision_tree_repair = grid_search.best_estimator_
y_train_pred = decision_tree_repair.predict(X_train_copy)
y_test_pred = decision_tree_repair.predict(X_test_copy)

print("*" * 100)
# è®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
      len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆäºŒï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œç‰¹å¾ä¿®å¤ï¼ˆç»Ÿè®¡æ–¹æ³•ä¿®å¤ï¼‰
# #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’Œdecision_treeåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰
#
# # subsection æŒ‰ç…§ç‰¹å¾ä¸­çš„å¼‚å¸¸å€¼è¿›è¡Œä¿®å¤
#
# for key, value in outlier_feature_indices.items():
#     column_data = X_copy[:, key]
#     mean = np.mean(column_data)
#     X_copy[value, key] = mean
#
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒdecision_treeæ¨¡å‹
#
# # ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
# grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5, scoring='accuracy')
# grid_search.fit(X_train_copy, y_train)
#
# # è·å–æœ€ä½³æ¨¡å‹
# decision_tree_repair = grid_search.best_estimator_
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«decision_treeæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))