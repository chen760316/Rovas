"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´,M) â†’ ugly(ğ‘¡)
Rovaså¯¹ugly outliersçš„æ£€æµ‹èƒ½åŠ›
"""

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†

file_path = "../datasets/real_outlier/Cardiotocography.csv"
# file_path = "../datasets/real_outlier/annthyroid.csv"
# file_path = "../datasets/real_outlier/optdigits.csv"
# file_path = "../datasets/real_outlier/PageBlocks.csv"
# file_path = "../datasets/real_outlier/pendigits.csv"
# file_path = "../datasets/real_outlier/satellite.csv"
# file_path = "../datasets/real_outlier/shuttle.csv"
# file_path = "../datasets/real_outlier/yeast.csv"

data = pd.read_csv(file_path)

# å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
if len(data) > 20000:
    data = data.sample(n=20000, random_state=42)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)

# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

# section æ•°æ®ç‰¹å¾ç¼©æ”¾ä»¥åŠæ·»åŠ å™ªå£°

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=all_columns)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

# section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
import re

i = len(feature_names)
np.random.seed(1)
categorical_names = {}
softmax_model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, class_weight='balanced')
softmax_model.fit(X_train_copy, y_train)

for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data_copy.iloc[:, feature])
    data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
    categorical_names[feature] = le.classes_

explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)

predict_fn = lambda x: softmax_model.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names)//2)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
top_k_indices = [feature_names.index(name) for name in top_feature_names]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„

# choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax

# è·å–å†³ç­–å€¼
decision_values = softmax_model.decision_function(X_copy)
# å°†å†³ç­–å€¼è½¬æ¢ä¸ºé€‚ç”¨äº Softmax çš„äºŒç»´æ•°ç»„
decision_values_reshaped = decision_values.reshape(-1, 1)  # å˜æˆ (n_samples, 1)
# åº”ç”¨ Softmax å‡½æ•°ï¼ˆå¯ä»¥æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ scipyï¼‰
y_pred = softmax(np.hstack((decision_values_reshaped, -decision_values_reshaped)), axis=1)
# åˆ›å»º OneHotEncoder å®ä¾‹
encoder = OneHotEncoder(sparse=False)
# é¢„æµ‹y_testçš„å€¼ï¼Œå¹¶ä¸y_trainç»„åˆæˆä¸ºy_ground
y_test_pred = softmax_model.predict(X_test_copy)
y_ground = np.hstack((y_train, y_test_pred))
# å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
y_true = encoder.fit_transform(y_ground.reshape(-1, 1))
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
# è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
bad_samples = np.where(loss_per_sample < average_loss)[0]
good_samples = np.where(loss_per_sample >= average_loss)[0]
ugly_outlier_candidates = bad_samples

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„

outlier_feature_indices = {}
threshold = 0.01
for column_indice in top_k_indices:
    select_feature = feature_names[column_indice]
    select_column_data = data_copy[select_feature].values
    max_value = np.max(select_column_data)
    min_value = np.min(select_column_data)
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
    # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    outlier_feature_indices[column_indice] = intersection
# print(outlier_feature_indices)

# section ç¡®å®šæ•°æ®ä¸­çš„ugly outliers

outlier_tuple_set = set()
for value in outlier_feature_indices.values():
    outlier_tuple_set.update(value)
outlier_tuple_set.update(bad_samples)
X_copy_repair_indices = list(outlier_tuple_set)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

# section è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡çš„softmaxæ¨¡å‹

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„softmaxæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
softmax_model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, class_weight='balanced')
softmax_model.fit(X_train, y_train)
train_label_pred = softmax_model.predict(X_train)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != softmax_model.predict(X_train))[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != softmax_model.predict(X_test))[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„softmaxæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
softmax_model_noise = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, class_weight='balanced')
softmax_model_noise.fit(X_train_copy, y_train)
train_label_pred_noise = softmax_model_noise.predict(X_train_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != softmax_model_noise.predict(X_train_copy))[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != softmax_model_noise.predict(X_test_copy))[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section å…¨éƒ¨åŠ å™ªæ•°æ®ä¸­è¢«softmaxåˆ†ç±»å™¨è¯¯åˆ†ç±»çš„æ•°é‡
label_pred = softmax_model_noise.predict(X_copy)
wrong_classify_indices = []
for i in range(len(X_copy)):
    if y[i] != label_pred[i]:
        wrong_classify_indices.append(i)
print("è¢«è¯¯åˆ†ç±»çš„æ ·æœ¬æ•°é‡ï¼š", len(wrong_classify_indices))

# section æ£€æµ‹ugly outliersçš„å¬å›ç‡
# ugly_found_by_detector = list(set(X_copy_repair_indices) & set(wrong_classify_indices))
ugly_found_by_detector = list(set(X_copy_repair_indices) & set(wrong_classify_indices))
print("å¬å›çš„ugly outliersçš„æ•°é‡ï¼š", len(ugly_found_by_detector))
print("ugly outliersçš„å¬å›ç‡ä¸ºï¼š", len(ugly_found_by_detector)/len(wrong_classify_indices))