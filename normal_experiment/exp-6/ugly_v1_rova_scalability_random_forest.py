"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´,M) â†’ ugly(ğ‘¡)
æµ‹è¯•ugly v1 outliersæ£€æµ‹è§„åˆ™çš„å¯æ‰©å±•æ€§ï¼ˆé‡ç‚¹å…³æ³¨è°“è¯çš„å¯æ‰©å±•æ€§ï¼‰
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD

from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax
import warnings
warnings.filterwarnings("ignore")
import time
from memory_profiler import memory_usage
import tracemalloc
from contextlib import contextmanager

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

@contextmanager
def memory_timer():
    tracemalloc.start()  # å¼€å§‹è·Ÿè¸ªå†…å­˜
    yield  # å…è®¸æ‰§è¡Œä»£ç å—
    current, peak = tracemalloc.get_traced_memory()  # è·å–å½“å‰å’Œå³°å€¼å†…å­˜
    tracemalloc.stop()  # åœæ­¢è·Ÿè¸ªå†…å­˜
    print(f"Current Memory Usage: {current / 10**6:.2f} MiB")
    print(f"Peak Memory Usage: {peak / 10**6:.2f} MiB")

with memory_timer():
    # section æ ‡å‡†æ•°æ®é›†å¤„ç†

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    # subsection å«æœ‰ä¸åŒå¼‚å¸¸æ¯”ä¾‹çš„çœŸå®æ•°æ®é›†

    # choice Annthyroidæ•°æ®é›†(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_02_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_05_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Annthyroid/Annthyroid_07.csv"

    # choice Cardiotocographyæ•°æ®é›†(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_02_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_05_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_10_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_20_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Cardiotocography/Cardiotocography_22.csv"

    # choice PageBlocksæ•°æ®é›†(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/real_outlier_varying_ratios/PageBlocks/PageBlocks_02_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/PageBlocks/PageBlocks_05_v01.csv"

    # choice Wiltæ•°æ®é›†(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/real_outlier_varying_ratios/Wilt/Wilt_02_v01.csv"
    # file_path = "../datasets/real_outlier_varying_ratios/Wilt/Wilt_05.csv"

    # subsection å«æœ‰ä¸åŒå¼‚å¸¸ç±»å‹å’Œå¼‚å¸¸æ¯”ä¾‹çš„åˆæˆæ•°æ®é›†ï¼ˆä»çœŸå®æ•°æ®ä¸­åŠ å…¥å™ªå£°åˆæˆï¼‰

    # choice Annthyroidæ•°æ®é›†+clusterå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.1.csv"
    # file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.2.csv"
    # file_path = "../datasets/synthetic_outlier/annthyroid_cluster_0.3.csv"

    # choice Cardiotocographyæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
    # file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.1.csv"
    # file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.2.csv"
    # file_path = "../datasets/synthetic_outlier/Cardiotocography_local_0.3.csv"

    # choice PageBlocksæ•°æ®é›†+globalå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(æ•ˆæœç¨³å®š)
    # file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.1.csv"
    # file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.2.csv"
    # file_path = "../datasets/synthetic_outlier/PageBlocks_global_0.3.csv"

    # choice satelliteæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
    # file_path = "../datasets/synthetic_outlier/satellite_0.1.csv"
    # file_path = "../datasets/synthetic_outlier/satellite_0.2.csv"
    # file_path = "../datasets/synthetic_outlier/satellite_0.3.csv"

    # choice annthyroidæ•°æ®é›†+localå™ªå£°+ä¸åŒå™ªå£°æ¯”ä¾‹(å¥½ç”¨)
    # file_path = "../datasets/synthetic_outlier/annthyroid_0.1.csv"
    # file_path = "../datasets/synthetic_outlier/annthyroid_0.2.csv"
    file_path = "../datasets/synthetic_outlier/annthyroid_0.3.csv"

    data = pd.read_csv(file_path)
    # å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
    if len(data) > 20000:
        data = data.sample(n=20000, random_state=42)

    enc = LabelEncoder()
    label_name = data.columns[-1]

    # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
    data[label_name] = enc.fit_transform(data[label_name])

    # æ£€æµ‹éæ•°å€¼åˆ—
    non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

    # ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
    encoders = {}
    for column in non_numeric_columns:
        encoder = LabelEncoder()
        data[column] = encoder.fit_transform(data[column])
        encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

    X = data.values[:, :-1]
    y = data.values[:, -1]

    # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
    categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
    # è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
    categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

    all_columns = data.columns.values.tolist()
    feature_names = all_columns[:-1]
    class_name = all_columns[-1]

    # ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
    unique_values, counts = np.unique(y, return_counts=True)

    # è¾“å‡ºç»“æœ
    for value, count in zip(unique_values, counts):
        print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

    # æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
    min_count = counts.min()
    total_count = counts.sum()

    # è®¡ç®—æ¯”ä¾‹
    proportion = min_count / total_count
    print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
    min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
    min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æ•°æ®é›†æ ‡å‡†å¤„ç†è€—æ—¶ï¼š", end_time - start_time)

    # section æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ª

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
    X = StandardScaler().fit_transform(X)
    # è®°å½•åŸå§‹ç´¢å¼•
    original_indices = np.arange(len(X))
    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
    # åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
    noise_level = 0.2
    # è®¡ç®—å™ªå£°æ•°é‡
    n_samples = X.shape[0]
    n_noise = int(noise_level * n_samples)
    # éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
    noise_indices = np.random.choice(n_samples, n_noise, replace=False)
    # æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
    X_copy = np.copy(X)
    # X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
    # ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
    X_train_copy = X_copy[train_indices]
    X_test_copy = X_copy[test_indices]
    feature_names = data.columns.values.tolist()
    combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
    # æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
    data_copy = pd.DataFrame(combined_array, columns=feature_names)
    # è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    train_noise = np.intersect1d(train_indices, noise_indices)
    # æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    test_noise = np.intersect1d(test_indices, noise_indices)
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ªè€—æ—¶ï¼š", end_time - start_time)

    # section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
    import re

    i = len(feature_names)
    np.random.seed(1)
    categorical_names = {}
    rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    rf_model.fit(X_train_copy, y_train)

    for feature in categorical_features:
        le = LabelEncoder()
        le.fit(data_copy.iloc[:, feature])
        data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
        categorical_names[feature] = le.classes_

    explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                     categorical_features=categorical_features,
                                     categorical_names=categorical_names, kernel_width=3)

    predict_fn = lambda x: rf_model.predict_proba(x)
    exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names) // 2)
    # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
    top_features = exp.as_list()
    top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
    top_k_indices = [feature_names.index(name) for name in top_feature_names]
    print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

    # # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
    # top_features = exp.as_list()
    # important_features = []
    # for feature_set in top_features:
    #     feature_long = feature_set[0]
    #     for feature in feature_names:
    #         if set(feature).issubset(set(feature_long)):
    #             important_features.append(feature)
    #             break
    #
    # top_k_indices = [feature_names.index(feature_name) for feature_name in important_features]
    # print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æœ‰å½±å“åŠ›çš„ç‰¹å¾è®¡ç®—è€—æ—¶ï¼š", end_time - start_time)

    # SECTION random forestæ¨¡å‹çš„å®ç°

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    # subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„random forestæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

    print("*" * 100)
    rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    rf_clf.fit(X_train, y_train)
    train_label_pred = rf_clf.predict(X_train)
    test_label_pred = rf_clf.predict(X_test)

    # ä½¿ç”¨ np.unique ç»Ÿè®¡ä¸åŒæ ‡ç­¾åŠå…¶å‡ºç°æ¬¡æ•°
    unique_labels, counts = np.unique(train_label_pred, return_counts=True)

    # æ‰“å°ç»“æœ
    for label, count in zip(unique_labels, counts):
        print(f"å¹²å‡€è®­ç»ƒé›†Label: {label}, é¢„æµ‹Count: {count}")

    unique_labels, counts = np.unique(test_label_pred, return_counts=True)

    # æ‰“å°ç»“æœ
    for label, count in zip(unique_labels, counts):
        print(f"å¹²å‡€æµ‹è¯•é›†Label: {label}, é¢„æµ‹Count: {count}")

    print("*" * 100)

    # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
    print("è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices) / len(y_train))

    # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
    print("æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices) / len(y_test))

    # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
          (len(wrong_classified_train_indices) + len(wrong_classified_test_indices)) / (len(y_train) + len(y_test)))

    # subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„random forestæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

    print("*" * 100)
    train_label_pred_noise = rf_model.predict(X_train_copy)
    test_label_pred_noise = rf_model.predict(X_test_copy)

    # ä½¿ç”¨ np.unique ç»Ÿè®¡ä¸åŒæ ‡ç­¾åŠå…¶å‡ºç°æ¬¡æ•°
    unique_labels, counts = np.unique(train_label_pred_noise, return_counts=True)

    # æ‰“å°ç»“æœ
    for label, count in zip(unique_labels, counts):
        print(f"åŠ å™ªè®­ç»ƒé›†Label: {label}, é¢„æµ‹Count: {count}")

    unique_labels, counts = np.unique(test_label_pred_noise, return_counts=True)

    # æ‰“å°ç»“æœ
    for label, count in zip(unique_labels, counts):
        print(f"åŠ å™ªæµ‹è¯•é›†Label: {label}, é¢„æµ‹Count: {count}")

    print("*" * 100)

    # åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
    train_wrong_ratio = len(wrong_classified_train_indices_noise) / len(y_train)
    print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise) / len(y_train))

    # åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
    print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise) / len(y_test))

    # æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
          (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise)) / (len(y_train) + len(y_test)))

    # subsection ç”¨å¤šç§æŒ‡æ ‡è¯„ä»·åŠ å™ªæ•°æ®é›†ä¸­random forestçš„é¢„æµ‹æ•ˆæœ

    """Precision/Recall/F1æŒ‡æ ‡"""
    print("*" * 100)

    # average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
    # average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
    # average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
    # average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚
    y_test_pred = test_label_pred_noise
    print("random forestæ¨¡å‹åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(precision_score(y_test, y_test_pred, average='weighted')))
    print("random forestæ¨¡å‹åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(recall_score(y_test, y_test_pred, average='weighted')))
    print("random forestæ¨¡å‹åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, y_test_pred, average='weighted')))

    """ROC-AUCæŒ‡æ ‡"""
    # y_test_prob = rf_model.predict_proba(X_test_copy)
    # # å¯¹äºäºŒåˆ†ç±»ä»»åŠ¡
    # roc_auc_test = roc_auc_score(y_test, y_test_prob[:, 1])  # ä½¿ç”¨ç¬¬äºŒç±»çš„æ¦‚ç‡
    # print("random forestæ¨¡å‹åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

    """PR AUCæŒ‡æ ‡(ä¸æ”¯æŒå¤šåˆ†ç±»)"""
    # # è®¡ç®—é¢„æµ‹æ¦‚ç‡
    # y_scores = rf_model_noise.predict_proba(X_test)
    # # éå†æ¯ä¸ªç±»åˆ«
    # pr_scores = []
    # for i in range(y_scores.shape[1]):
    #     precision, recall, _ = precision_recall_curve(y_test, y_scores[:, i])
    #     pr_auc = auc(recall, precision)
    #     pr_scores.append(pr_auc)
    #     print(f"random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„PR AUC åˆ†æ•°ï¼ˆç±» {i}ï¼‰: {pr_auc}")
    # # å¦‚æœéœ€è¦è®¡ç®—æ‰€æœ‰ç±»çš„å®å¹³å‡ PR åˆ†æ•°
    # macro_pr_score = sum(pr_scores) / len(pr_scores)
    # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„å®å¹³å‡APåˆ†æ•°:", macro_pr_score)

    """APæŒ‡æ ‡(ä¸æ”¯æŒå¤šåˆ†ç±»)"""
    # # è®¡ç®—é¢„æµ‹æ¦‚ç‡
    # y_scores = rf_model_noise.predict_proba(X_test)
    # # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ Average Precision
    # ap_scores = []
    # for i in range(y_scores.shape[1]):
    #     ap_score = average_precision_score(y_test, y_scores[:, i])
    #     ap_scores.append(ap_score)
    #     print(f"random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„APåˆ†æ•°ï¼ˆç±» {i}ï¼‰: {ap_score}")
    #
    # # å¦‚æœéœ€è¦è®¡ç®—æ‰€æœ‰ç±»çš„å®å¹³å‡ AP åˆ†æ•°
    # macro_ap_score = sum(ap_scores) / len(ap_scores)
    # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„å®å¹³å‡APåˆ†æ•°:", macro_ap_score)

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("random forestæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹è€—æ—¶ï¼š", end_time - start_time)

    # section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    # choice ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
    # decision_values = rf_model.decision_function(X_copy)
    # predicted_labels = np.argmax(decision_values, axis=1)
    # # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
    # num_samples = X_copy.shape[0]
    # num_classes = rf_model.classes_.shape[0]
    # hinge_losses = np.zeros((num_samples, num_classes))
    # hinge_loss = np.zeros(num_samples)
    # for i in range(num_samples):
    #     correct_class = int(y[i])
    #     for j in range(num_classes):
    #         if j != correct_class:
    #             loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
    #             hinge_losses[i, j] = loss_j
    #     hinge_loss[i] = np.max(hinge_losses[i])
    #
    # # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
    # ugly_outlier_candidates = np.where(hinge_loss > 1)[0]
    # # print("Dä¸­æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", ugly_outlier_candidates)

    # choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
    # # è·å–å†³ç­–å€¼
    # decision_values = rf_model.decision_function(X_copy)
    # # å°†å†³ç­–å€¼è½¬æ¢ä¸ºé€‚ç”¨äº Softmax çš„äºŒç»´æ•°ç»„
    # decision_values_reshaped = decision_values.reshape(-1, 1)  # å˜æˆ (n_samples, 1)
    # # åº”ç”¨ Softmax å‡½æ•°ï¼ˆå¯ä»¥æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ scipyï¼‰
    # y_pred = softmax(np.hstack((decision_values_reshaped, -decision_values_reshaped)), axis=1)
    # # åˆ›å»º OneHotEncoder å®ä¾‹
    # encoder = OneHotEncoder(sparse=False)
    # # é¢„æµ‹y_testçš„å€¼ï¼Œå¹¶ä¸y_trainç»„åˆæˆä¸ºy_ground
    # y_test_pred = rf_model.predict(X_test_copy)
    # y_ground = np.hstack((y_train, y_test_pred))
    # # å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
    # y_true = encoder.fit_transform(y_ground.reshape(-1, 1))
    # # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
    # loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
    # # è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
    # average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
    # bad_samples = np.where(loss_per_sample > average_loss)[0]
    # good_samples = np.where(loss_per_sample <= average_loss)[0]
    # # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
    # # ugly_outlier_candidates = np.where(bad_samples > 1)[0]
    # ugly_outlier_candidates = bad_samples
    #
    # # bad_num_threshold = int(train_wrong_ratio * X_copy.shape[0])
    # # # è®¡ç®—æ¯ä¸ªæŸå¤±å€¼ä¸0çš„ç»å¯¹å·®å€¼
    # # abs_loss = np.abs(loss_per_sample)
    # # # æ‰¾åˆ°æœ€æ¥è¿‘0çš„æ ·æœ¬çš„ç´¢å¼•
    # # # closest_indices = np.argsort(abs_loss)[:bad_num_threshold]
    # # closest_indices = np.argsort(abs_loss)[:bad_num_threshold][::-1]

    # choice åˆ¤å®šåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
    y_pred = rf_model.predict(X_copy)
    ugly_outlier_candidates = np.where(y_pred != y)[0]
    # æå–å¯¹åº”ç´¢å¼•çš„æ ‡ç­¾
    selected_labels = y[ugly_outlier_candidates]
    print("ugly_outlier_candidatesçš„æ•°é‡ï¼š", len(ugly_outlier_candidates))
    print("ugly_outlier_candidatesä¸­æ ‡ç­¾ä¸º1çš„æ ·æœ¬æ•°é‡ï¼š", np.sum(selected_labels == 1))
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æŸå¤±å‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    outlier_feature_indices = {}
    threshold = 0.01
    for column_indice in top_k_indices:
        select_feature = feature_names[column_indice]
        select_column_data = data_copy[select_feature].values
        max_value = np.max(select_column_data)
        min_value = np.min(select_column_data)
        sorted_indices = np.argsort(select_column_data)
        sorted_data = select_column_data[sorted_indices]
        # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
        outliers = []
        outliers_index = []
        # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
        if len(sorted_data) > 1:
            if (sorted_data[1] - sorted_data[0] >= threshold):
                outliers.append(sorted_data[0])
                outliers_index.append(sorted_indices[0])
            if (sorted_data[-1] - sorted_data[-2] >= threshold):
                outliers.append(sorted_data[-1])
                outliers_index.append(sorted_indices[-1])
        # æ£€æŸ¥ä¸­é—´å…ƒç´ 
        for i in range(1, len(sorted_data) - 1):
            current_value = sorted_data[i]
            left_value = sorted_data[i - 1]
            right_value = sorted_data[i + 1]
            if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
                outliers.append(current_value)
                outliers_index.append(sorted_indices[i])
        outliers_index_numpy = np.array(outliers_index)
        intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
        # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
        outlier_feature_indices[column_indice] = intersection
    # print(outlier_feature_indices)

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )èšåˆå‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # section ç¡®å®šæ•°æ®ä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    outlier_tuple_set = set()
    for value in outlier_feature_indices.values():
        outlier_tuple_set.update(value)
    X_copy_repair_indices = list(outlier_tuple_set)
    X_copy_repair_indices.extend(ugly_outlier_candidates)
    X_copy_repair = X_copy[X_copy_repair_indices]
    print("æ€»çš„æ ·æœ¬æ•°é‡ä¸ºï¼š", len(X_copy))
    print("éœ€è¦ä¿®å¤çš„æ ·æœ¬æ•°é‡ä¸ºï¼š", len(X_copy_repair_indices))
    y_repair = y[X_copy_repair_indices]

    # ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
    rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

    # ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
    # æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
    X_copy_inners = X_copy[rows_to_keep]
    y_inners = y[rows_to_keep]

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("ç¡®å®šugly outliersè€—æ—¶ï¼š", end_time - start_time)

    # # section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆknnæ–¹æ³•ï¼‰
    # #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’Œrandom foreståˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰
    #
    # # subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾
    #
    # knn = KNeighborsClassifier(n_neighbors=3)
    # knn.fit(X_copy_inners, y_inners)
    #
    # # ç»Ÿè®¡y_innersä¸åŒå€¼åŠå…¶æ•°é‡
    # unique_values, counts = np.unique(y_inners, return_counts=True)
    #
    # # è¾“å‡ºç»“æœ
    # for value, count in zip(unique_values, counts):
    #     print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")
    #
    # # é¢„æµ‹å¼‚å¸¸å€¼
    # y_pred = knn.predict(X_copy_repair)
    #
    # # æ›¿æ¢å¼‚å¸¸å€¼
    # y[X_copy_repair_indices] = y_pred
    # y_train = y[train_indices]
    # y_test = y[test_indices]
    #
    # # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # print("*"*100)
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy, y_train)
    # y_train_pred = rf_repair.predict(X_train_copy)
    # y_test_pred = rf_repair.predict(X_test_copy)
    #
    # # ä½¿ç”¨ np.unique ç»Ÿè®¡ä¸åŒæ ‡ç­¾åŠå…¶å‡ºç°æ¬¡æ•°
    # unique_labels, counts = np.unique(y_train_pred, return_counts=True)
    #
    # # æ‰“å°ç»“æœ
    # for label, count in zip(unique_labels, counts):
    #     print(f"ä¿®å¤è®­ç»ƒé›†Label: {label}, é¢„æµ‹Count: {count}")
    #
    # unique_labels, counts = np.unique(y_test_pred, return_counts=True)
    #
    # # æ‰“å°ç»“æœ
    # for label, count in zip(unique_labels, counts):
    #     print(f"ä¿®å¤æµ‹è¯•é›†Label: {label}, é¢„æµ‹Count: {count}")
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))
    #
    # # subsection ç”¨å¤šç§æŒ‡æ ‡è¯„ä»·random foreståœ¨ä¿®å¤åçš„æ•°æ®ä¸Šçš„é¢„æµ‹æ•ˆæœ
    #
    # """Precision/Recall/F1æŒ‡æ ‡"""
    # print("*" * 100)
    #
    # # average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
    # # average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
    # # average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
    # # average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚
    #
    # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(precision_score(y_test, y_test_pred, average='weighted')))
    # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(recall_score(y_test, y_test_pred, average='weighted')))
    # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, y_test_pred, average='weighted')))
    #
    # """ROC-AUCæŒ‡æ ‡"""
    # # y_test_prob = rf_repair.predict_proba(X_test_copy)
    # # roc_auc_test = roc_auc_score(y_test, y_test_prob[:, 1])  # ä¸€å¯¹å¤šæ–¹å¼
    # # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))
    #
    # """PR AUCæŒ‡æ ‡(ä¸æ”¯æŒå¤šåˆ†ç±»)"""
    # # # è®¡ç®—é¢„æµ‹æ¦‚ç‡
    # # y_scores = rf_repair.predict_proba(X_test)
    # # # è®¡ç®— Precision å’Œ Recall
    # # precision, recall, _ = precision_recall_curve(y_test, y_scores)
    # # # è®¡ç®— PR AUC
    # # pr_auc = auc(recall, precision)
    # # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„PR AUC åˆ†æ•°:", pr_auc)
    # #
    # """APæŒ‡æ ‡(ä¸æ”¯æŒå¤šåˆ†ç±»)"""
    # # # è®¡ç®—é¢„æµ‹æ¦‚ç‡
    # # y_scores = rf_repair.predict_proba(X_test)
    # # # è®¡ç®— Average Precision
    # # ap_score = average_precision_score(y_test, y_scores)
    # # print("random forestæ¨¡å‹åœ¨ä¿®å¤æµ‹è¯•é›†ä¸­çš„APåˆ†æ•°:", ap_score)

    # # section æ–¹æ¡ˆäºŒï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œç‰¹å¾ä¿®å¤ï¼ˆç»Ÿè®¡æ–¹æ³•ä¿®å¤ï¼‰
    # #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’Œrandom foreståˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰(ä¿®å¤æ•ˆæœç”±äºç›‘ç£/æ— ç›‘ç£åŸºå‡†)
    #
    # # subsection ç¡®å®šæœ‰å½±å“åŠ›ç‰¹å¾ä¸­çš„ç¦»ç¾¤å€¼å¹¶é‡‡ç”¨å‡å€¼ä¿®å¤
    # for i in range(X_copy.shape[1]):
    #     if i in top_k_indices:
    #         column_data = X_copy[:, i]
    #         mean = np.mean(column_data)
    #         # å°†æ‰€æœ‰éœ€è¦ä¿®å¤çš„è¡Œå¯¹åº”çš„åˆ—ä½ç½®çš„å…ƒç´ æ›¿æ¢ä¸ºå‡å€¼
    #         intersection = X_copy_repair_indices
    #         X_copy[intersection, i] = mean
    #
    # X_train_copy = X_copy[train_indices]
    # X_test_copy = X_copy[test_indices]
    #
    # # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy, y_train)
    # y_train_pred = rf_repair.predict(X_train_copy)
    # y_test_pred = rf_repair.predict(X_test_copy)
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

    # # section æ–¹æ¡ˆä¸‰ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„å€ŸåŠ©knnè¿›è¡Œä¿®å¤ï¼Œchoice1 å°†å¼‚å¸¸å…ƒç»„ä¸­çš„å…ƒç´ ç›´æ¥è®¾ç½®ä¸ºnan(ä¿®å¤è¯¯å·®å¤ªå¤§ï¼Œä¿®å¤åå‡†ç¡®æ€§ä¸‹é™)
    # #  choice2 ä»…å°†æœ‰å½±å“åŠ›ç‰¹å¾ä¸Šçš„å…ƒç´ è®¾ç½®ä¸ºnp.nan
    #
    # # # choice å°†å¼‚å¸¸å…ƒç»„ä¸­çš„æ‰€æœ‰å…ƒç´ è®¾ç½®ä¸ºnan
    # # for i in range(X_copy.shape[1]):
    # #     X_copy[X_copy_repair_indices, i] = np.nan
    #
    # # choice ä»…å°†å¼‚å¸¸å…ƒç»„ä¸­çš„æœ‰å½±å“åŠ›çš„å…ƒç´ è®¾ç½®ä¸ºnan
    # for i in top_k_indices:
    #     X_copy[X_copy_repair_indices, i] = np.nan
    #
    # # choice ä½¿ç”¨knnä¿®å¤æ‰€æœ‰è¢«æ ‡è®°ä¸ºnançš„å¼‚å¸¸ç‰¹å¾
    # # åˆ›å»º KNN Imputer å¯¹è±¡
    # knn_imputer = KNNImputer(n_neighbors=5)
    #
    # # ä½¿ç”¨ KNN ç®—æ³•å¡«è¡¥å¼‚å¸¸ç‰¹å¾
    # X_copy = knn_imputer.fit_transform(X_copy)
    # X_train_copy = X_copy[train_indices]
    # X_test_copy = X_copy[test_indices]
    #
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy, y_train)
    # y_train_pred = rf_repair.predict(X_train_copy)
    # y_test_pred = rf_repair.predict(X_test_copy)
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    # print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
    #       len(wrong_classified_train_indices)/len(y_train))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    # print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
    #       len(wrong_classified_test_indices)/len(y_test))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))
    #       /(len(y_train) + len(y_test)))

    # # section æ–¹æ¡ˆå››ï¼šå°†X_copyä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†éœ€è¦ä¿®å¤çš„å…ƒç»„ç›´æ¥åˆ é™¤ï¼Œåœ¨å»é™¤åçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # set_X_copy_repair = set(X_copy_repair_indices)
    #
    # # è®¡ç®—å·®é›†ï¼Œå»é™¤è®­ç»ƒé›†ä¸­éœ€è¦ä¿®å¤çš„çš„å…ƒç´ 
    # set_train_indices = set(train_indices)
    # remaining_train_indices = list(set_train_indices - set_X_copy_repair)
    # X_train_copy_repair = X_copy[remaining_train_indices]
    # y_train_copy_repair = y[remaining_train_indices]
    #
    # # # choice è®¡ç®—å·®é›†ï¼Œå»é™¤æµ‹è¯•é›†ä¸­éœ€è¦ä¿®å¤çš„çš„å…ƒç´ 
    # # set_test_indices = set(test_indices)
    # # remaining_test_indices = list(set_test_indices - set_X_copy_repair)
    # # X_test_copy_repair = X_copy[remaining_test_indices]
    # # y_test_copy_repair = y[remaining_test_indices]
    #
    # # choice ä¸åˆ é™¤æµ‹è¯•é›†ä¸­çš„ç¦»ç¾¤æ ·æœ¬
    # X_test_copy_repair = X_copy[test_indices]
    # y_test_copy_repair = y[test_indices]
    #
    # # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy_repair, y_train_copy_repair)
    # y_train_pred = rf_repair.predict(X_train_copy_repair)
    # y_test_pred = rf_repair.predict(X_test_copy_repair)
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train_copy_repair != y_train_pred)[0]
    # print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
    #       len(wrong_classified_train_indices)/len(y_train_copy_repair))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test_copy_repair != y_test_pred)[0]
    # print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
    #       len(wrong_classified_test_indices)/len(y_test_copy_repair))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))
    #       /(len(y_train_copy_repair) + len(y_test_copy_repair)))

    # # section æ–¹æ¡ˆäº”ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆéšæœºæ£®æ—æ¨¡å‹ï¼‰ï¼Œä¿®å¤æ ‡ç­¾å€¼
    #
    # from sklearn.ensemble import RandomForestRegressor
    # from sklearn.ensemble import RandomForestClassifier
    # from sklearn.metrics import mean_absolute_error
    #
    # # subsection ä¿®å¤æ ‡ç­¾å€¼
    # # è®­ç»ƒæ¨¡å‹
    # model = RandomForestClassifier(n_estimators=100, random_state=42)
    # model.fit(X_copy_inners, y_inners)  # ä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒæ¨¡å‹
    #
    # # é¢„æµ‹ç¦»ç¾¤æ ·æœ¬çš„æ ‡ç­¾
    # y_repair_pred = model.predict(X_copy_repair)
    #
    # # è®¡ç®—é¢„æµ‹çš„å‡†ç¡®æ€§ï¼ˆå¯é€‰ï¼‰
    # mae = mean_absolute_error(y_repair, y_repair_pred)
    # print(f'Mean Absolute Error: {mae}')
    #
    # # subsection ä¿®å¤ç‰¹å¾å€¼
    #
    # X_copy[X_copy_repair_indices] = X_copy_repair
    # y[X_copy_repair_indices] = y_repair_pred
    # X_train_copy = X_copy[train_indices]
    # X_test_copy = X_copy[test_indices]
    # y_train = y[train_indices]
    # y_test = y[test_indices]
    #
    # # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy, y_train)
    # y_train_pred = rf_repair.predict(X_train_copy)
    # y_test_pred = rf_repair.predict(X_test_copy)
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

    # # section æ–¹æ¡ˆå…­ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹(éšæœºæ£®æ—æ¨¡å‹)ï¼Œä¿®å¤ç‰¹å¾å€¼ï¼ˆä¿®å¤æ—¶é—´å¾ˆä¹…ï¼Œæ…ç”¨ï¼‰
    # #  ä¾æ¬¡å°†æœ‰å½±å“åŠ›çš„ç‰¹å¾ä½œä¸ºè¦ä¿®å¤çš„æ ‡ç­¾ï¼ˆè¿ç»­ç‰¹å¾å¯¹åº”å›å½’æ¨¡å‹ï¼Œåˆ†ç±»ç‰¹å¾å¯¹åº”åˆ†ç±»æ¨¡å‹ï¼‰ï¼Œä½¿ç”¨å…¶ä»–ç‰¹å¾å‚ä¸è®­ç»ƒ
    #
    # from sklearn.ensemble import RandomForestRegressor
    # from sklearn.ensemble import RandomForestClassifier
    # from sklearn.metrics import mean_absolute_error
    #
    # # subsection ä¿®å¤ç‰¹å¾å€¼
    #
    # for i in top_k_indices:
    #     y_train_inf = X_copy_inners[:, i]
    #     columns_to_keep = np.delete(range(X_copy_inners.shape[1]), i)
    #     X_train_remain = X_copy_inners[:, columns_to_keep]
    #     if i in categorical_features:
    #         model = RandomForestClassifier(n_estimators=100, random_state=42)
    #     else:
    #         model = RandomForestRegressor(n_estimators=100, random_state=42)
    #     model.fit(X_train_remain, y_train_inf)  # ä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒæ¨¡å‹
    #     X_test_repair = X_copy_repair[:, columns_to_keep]
    #     y_test_pred = model.predict(X_test_repair)
    #     X_copy_repair[:, i] = y_test_pred
    #
    # X_copy[X_copy_repair_indices] = X_copy_repair
    # X_train_copy = X_copy[train_indices]
    # X_test_copy = X_copy[test_indices]
    # y_train = y[train_indices]
    # y_test = y[test_indices]
    #
    # # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒrandom forestæ¨¡å‹
    #
    # rf_repair = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
    # rf_repair.fit(X_train_copy, y_train)
    # y_train_pred = rf_repair.predict(X_train_copy)
    # y_test_pred = rf_repair.predict(X_test_copy)
    #
    # print("*" * 100)
    # # è®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
    #
    # # æµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
    #
    # # æ•´ä½“æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    # print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«random forestæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
    #       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))