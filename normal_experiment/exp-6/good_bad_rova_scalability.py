"""
ğ‘…(ğ‘¡) âˆ§outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§loss(M, D, ğ‘¡) > ğœ† â†’ good(ğ‘¡)
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§ loss(M, D, ğ‘¡) â‰¤ ğœ† â†’ bad(ğ‘¡)
æµ‹è¯•good/bad outliersæ£€æµ‹è§„åˆ™çš„å¯æ‰©å±•æ€§ï¼ˆé‡ç‚¹å…³æ³¨è°“è¯çš„å¯æ‰©å±•æ€§ï¼‰
"""

from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
from sklearn.preprocessing import OneHotEncoder
from scipy.special import softmax
import re
import time
from memory_profiler import memory_usage
import tracemalloc
from contextlib import contextmanager

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

def calculate_made(data):
    median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

@contextmanager
def memory_timer():
    tracemalloc.start()  # å¼€å§‹è·Ÿè¸ªå†…å­˜
    yield  # å…è®¸æ‰§è¡Œä»£ç å—
    current, peak = tracemalloc.get_traced_memory()  # è·å–å½“å‰å’Œå³°å€¼å†…å­˜
    tracemalloc.stop()  # åœæ­¢è·Ÿè¸ªå†…å­˜
    print(f"Current Memory Usage: {current / 10**6:.2f} MiB")
    print(f"Peak Memory Usage: {peak / 10**6:.2f} MiB")

with memory_timer():
    t0 = time.time()  # å¼€å§‹æ—¶é—´
    epochs = 1
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    n_trans = 64
    random_state = 42

    # section æ ‡å‡†æ•°æ®é›†å¤„ç†

    start_time = time.time()  # å¼€å§‹æ—¶é—´

    # choice drybeanæ•°æ®é›†(æ•ˆæœå¥½)
    # file_path = "../datasets/multi_class_to_outlier/drybean_outlier.csv"
    # data = pd.read_csv(file_path)

    # choice obesityæ•°æ®é›†(æ•ˆæœå¥½)
    # file_path = "../datasets/multi_class_to_outlier/obesity_outlier.csv"
    # data = pd.read_csv(file_path)

    # choice balitaæ•°æ®é›†(SVMæ‹Ÿåˆæ•ˆæœå·®ï¼Œä½†ä¿®å¤åæ•ˆæœæå‡æ˜¾è‘—)
    # file_path = "../datasets/multi_class_to_outlier/balita_outlier.csv"
    # data = pd.read_csv(file_path)

    # choice appleæ•°æ®é›†(æ•ˆæœæå‡å°)
    # file_path = "../datasets/multi_class_to_outlier/apple_outlier.csv"
    # data = pd.read_csv(file_path)

    # choice adultæ•°æ®é›†(æ•ˆæœæå‡æ˜æ˜¾)
    # file_path = "../datasets/multi_class_to_outlier/adult_outlier.csv"
    # data = pd.read_csv(file_path)

    # choice çœŸå®å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ï¼ˆæœ¬èº«ä¸åŒ…å«é”™è¯¯æ•°æ®ï¼Œä¸é€‚åˆç”¨äºä¿®å¤ä»»åŠ¡ï¼Œä¸”éœ€è¦æ­é…éçº¿æ€§SVMï¼‰
    # file_path = "../datasets/real_outlier/Cardiotocography.csv"
    # file_path = "../datasets/real_outlier/annthyroid.csv"
    file_path = "../datasets/real_outlier/optdigits.csv"
    # file_path = "../datasets/real_outlier/PageBlocks.csv"
    # file_path = "../datasets/real_outlier/pendigits.csv"
    # file_path = "../datasets/real_outlier/satellite.csv"
    # file_path = "../datasets/real_outlier/shuttle.csv"
    # file_path = "../datasets/real_outlier/yeast.csv"
    data = pd.read_csv(file_path)

    # subsection è¿›è¡Œè¡Œé‡‡æ ·å’Œåˆ—é‡‡æ ·
    print("åŸå§‹æ•°æ®é›†è¡Œæ•°ï¼š", data.shape[0])
    print("åŸå§‹æ•°æ®é›†åˆ—æ•°ï¼š", data.shape[1])
    # éšæœºé‡‡æ ·å›ºå®šæ¯”ä¾‹çš„è¡Œ
    sample_size = 0.5  # è¡Œé‡‡æ ·æ¯”ä¾‹
    data = data.sample(frac=sample_size, random_state=1)

    # éšæœºé‡‡æ ·å›ºå®šæ¯”ä¾‹çš„åˆ—
    sample_ratio = 0.5  # åˆ—é‡‡æ ·æ¯”ä¾‹

    # è®¡ç®—é‡‡æ ·çš„åˆ—æ•°ï¼ˆä¸åŒ…æ‹¬æ ‡ç­¾åˆ—ï¼‰
    num_features = data.shape[1] - 1  # ä¸åŒ…æ‹¬æ ‡ç­¾åˆ—
    num_sampled_features = int(num_features * sample_ratio)

    # éšæœºé€‰æ‹©ç‰¹å¾åˆ—
    sampled_columns = data.columns[:-1].to_series().sample(n=num_sampled_features, random_state=42)

    # æå–é‡‡æ ·çš„ç‰¹å¾åˆ—å’Œæ ‡ç­¾åˆ—
    label_name = data.columns[-1]
    data = data[sampled_columns.tolist() + [label_name]]

    print("é‡‡æ ·åçš„æ•°æ®é›†è¡Œæ•°ï¼š", data.shape[0])
    print("é‡‡æ ·åçš„æ•°æ®é›†åˆ—æ•°ï¼š", data.shape[1])

    # # å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
    # if len(data) > 20000:
    #     data = data.sample(n=20000, random_state=42)

    enc = LabelEncoder()

    # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
    data[label_name] = enc.fit_transform(data[label_name])

    # æ£€æµ‹éæ•°å€¼åˆ—
    non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

    # ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
    encoders = {}
    for column in non_numeric_columns:
        encoder = LabelEncoder()
        data[column] = encoder.fit_transform(data[column])
        encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

    X = data.values[:, :-1]
    y = data.values[:, -1]

    # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
    categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
    # è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
    categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

    all_columns = data.columns.values.tolist()
    feature_names = all_columns[:-1]
    class_name = all_columns[-1]

    # ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
    unique_values, counts = np.unique(y, return_counts=True)

    # è¾“å‡ºç»“æœ
    for value, count in zip(unique_values, counts):
        print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

    # æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
    min_count = counts.min()
    total_count = counts.sum()

    # è®¡ç®—æ¯”ä¾‹
    proportion = min_count / total_count
    print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
    min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
    min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æ•°æ®é›†æ ‡å‡†å¤„ç†è€—æ—¶ï¼š", end_time - start_time)

    # section æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ª

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
    X = StandardScaler().fit_transform(X)
    # è®°å½•åŸå§‹ç´¢å¼•
    original_indices = np.arange(len(X))
    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
    # åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
    noise_level = 0.2
    # è®¡ç®—å™ªå£°æ•°é‡
    n_samples = X.shape[0]
    n_noise = int(noise_level * n_samples)
    # éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
    noise_indices = np.random.choice(n_samples, n_noise, replace=False)
    # æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
    X_copy = np.copy(X)
    # X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
    # ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
    X_train_copy = X_copy[train_indices]
    X_test_copy = X_copy[test_indices]
    feature_names = data.columns.values.tolist()
    combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
    # æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
    data_copy = pd.DataFrame(combined_array, columns=feature_names)
    # è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    train_noise = np.intersect1d(train_indices, noise_indices)
    # æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    test_noise = np.intersect1d(test_indices, noise_indices)
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ªè€—æ—¶ï¼š", end_time - start_time)

    # SECTION Mğ‘œ (ğ‘¡, D),é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    clf = GOAD(epochs=epochs, device=device, n_trans=n_trans)
    clf.fit(X_train, y=None)
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒè€—æ—¶ï¼š", end_time - start_time)

    # SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹

    # train_scores = clf.decision_function(X_train)
    # train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
    # print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
    # train_outliers_index = []
    # print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
    # for i in range(len(X_train)):
    #     if train_pred_labels[i] == 1:
    #         train_outliers_index.append(i)
    # # åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å¼‚å¸¸å€¼ç´¢å¼•ä¸‹æ ‡
    # print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
    # print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))

    # SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    test_scores = clf.decision_function(X_test)
    test_pred_labels, test_confidence = clf.predict(X_test, return_confidence=True)
    print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
    test_outliers_index = []
    print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_test))
    for i in range(len(X_test)):
        if test_pred_labels[i] == 1:
            test_outliers_index.append(i)
    # åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å¼‚å¸¸å€¼ç´¢å¼•ä¸‹æ ‡
    print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index)
    print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index))
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æ£€æµ‹å™¨é¢„æµ‹è€—æ—¶ï¼š", end_time - start_time)

    # SECTION Mğ‘ (ğ‘…, ğ´,M)ï¼Œåœ¨è®­ç»ƒé›†ä¸­å¼•å…¥æœ‰å½±å“åŠ›çš„ç‰¹å¾

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # # SUBSECTION å€ŸåŠ©æ–¹å·®åˆ¤åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾
    # top_k_var = 6
    # variances = np.var(X_train, axis=0)
    # top_k_indices_var = np.argsort(variances)[-top_k_var:][::-1]
    # print("æ–¹å·®æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_var, top_k_indices_var))
    #
    # # SUBSECTION å€ŸåŠ©pearsonç›¸å…³ç³»æ•°ç­›é€‰é‡è¦ç‰¹å¾(å°†ç‰¹å¾åˆ—å’Œæ ‡ç­¾æ±‚pearsonç›¸å…³ç³»æ•°ä¸å¤ªç§‘å­¦)
    # top_k_pearson = 6
    # y_trans = y_train.reshape(-1)
    # pearson_matrix = np.corrcoef(X_train.T, y_trans)
    # correlations = np.abs(pearson_matrix[0, 1:])
    # top_k_indices_pearson = np.argsort(correlations)[::-1][:top_k_pearson]
    # print("ä¸æ ‡ç­¾yçš„Pearsonç›¸å…³ç³»æ•°ç»å¯¹å€¼æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_pearson, top_k_indices_pearson))
    #
    # # SUBSECTION å€ŸåŠ©äº’ä¿¡æ¯ç­›é€‰é‡è¦ç‰¹å¾(å•ä¸ªç‰¹å¾å’Œæ ‡ç­¾ä¹‹é—´çš„äº’ä¿¡æ¯)
    # top_k_mi = 6
    # y_trans_mi = y_train.reshape(-1)
    # mi = mutual_info_regression(X_train, y_trans_mi)
    # top_k_indices = np.argsort(mi)[::-1][:top_k_mi]
    # print("äº’ä¿¡æ¯æœ€å¤šçš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_mi, top_k_indices))
    #
    # # SUBSECTION å€ŸåŠ©lassoç­›é€‰é‡è¦ç‰¹å¾(ç‰¹å¾çš„è”åˆåˆ†å¸ƒå’Œæ ‡ç­¾ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§)
    # alpha = 0.0001
    # top_k_lasso = 6
    # lasso = Lasso(alpha, max_iter=10000, tol=0.01)
    # lasso.fit(X_train, y_train)
    # coef = lasso.coef_
    # coef_abs = abs(coef)
    # top_k_indices = np.argsort(coef_abs)[::-1][:top_k_lasso]
    # print("lassoç»å¯¹å€¼æœ€å¤§çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_lasso, top_k_indices))
    #
    # SUBSECTION sklearnåº“çš„SelectKBesté€‰æ‹©å™¨ï¼Œå€ŸåŠ©Fisheræ£€éªŒç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
    # top_k_fisher = 6
    # selector = SelectKBest(score_func=f_classif, k=top_k_fisher)
    # y_trans_fisher = y_train.reshape(-1)
    # X_new = selector.fit_transform(X_train, y_trans_fisher)
    # # è·å–è¢«é€‰ä¸­çš„ç‰¹å¾çš„ç´¢å¼•
    # selected_feature_indices = selector.get_support(indices=True)
    # print("SelectKBesté€‰æ‹©å™¨å€ŸåŠ©Fisheræ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_fisher, selected_feature_indices))

    # # SUBSECTION å€ŸåŠ©CARTå†³ç­–æ ‘ç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
    # top_k_cart = 6
    # classifier = DecisionTreeClassifier()
    # classifier.fit(X_train, y_train)
    # # è·å–ç‰¹å¾é‡è¦æ€§å¾—åˆ†
    # feature_importance = classifier.feature_importances_
    # # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
    # sorted_indices = np.argsort(feature_importance)[::-1]
    # # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
    # top_k_features = sorted_indices[:top_k_cart]
    # print("CARTå†³ç­–æ ‘æ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_cart, top_k_features))
    #
    # # SUBSECTION sklearnåº“SelectFromModelé€‰æ‹©å™¨,å®ƒå¯ä»¥ä¸ä»»ä½•å…·æœ‰coef_ æˆ– feature_importances_ å±æ€§ï¼ˆå¦‚éšæœºæ£®æ—å’Œå†³ç­–æ ‘æ¨¡å‹ï¼‰çš„è¯„ä¼°å™¨ä¸€èµ·ä½¿ç”¨æ¥é€‰æ‹©ç‰¹å¾
    # classifier = RandomForestClassifier()
    # classifier.fit(X_train, y_train)
    # # ä½¿ç”¨SelectFromModelæ¥é€‰æ‹©é‡è¦ç‰¹å¾
    # sfm = SelectFromModel(classifier, threshold='mean', prefit=True)
    # X_selected = sfm.transform(X_train)
    # # è·å–é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    # selected_idx = sfm.get_support(indices=True)
    # # æ‰“å°é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    # print("SelectFromModelé€‰æ‹©å™¨é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:", selected_idx)
    #
    # # SUBSECTION å€ŸåŠ©wrapper(åŒ…è£…)æ–¹æ³•ç”Ÿæˆç‰¹å¾å­é›†
    # model = LinearRegression()
    # # åˆå§‹åŒ– RFE ç‰¹å¾é€‰æ‹©å™¨ï¼Œé€‰æ‹©è¦ä¿ç•™çš„ç‰¹å¾æ•°é‡
    # rfe = RFE(model, n_features_to_select=6)
    # # æ‹Ÿåˆ RFE ç‰¹å¾é€‰æ‹©å™¨
    # rfe.fit(X_train, y_train)
    # # è¾“å‡ºé€‰æ‹©çš„ç‰¹å¾
    # indices = np.where(rfe.support_)[0]
    # print("wrapper(åŒ…è£…)æ–¹æ³•é€‰æ‹©çš„ç‰¹å¾:", indices)
    # # è¾“å‡ºç‰¹å¾æ’å
    # print("wrapper(åŒ…è£…)æ–¹æ³•ä¸‹çš„ç‰¹å¾æ’å:", rfe.ranking_)
    #

    # SUBSECTION åŸºäºXGBoostæ¨¡å‹ä»¥åŠXGBçš„ç‰¹å¾é‡è¦æ€§(ä¸€èˆ¬æƒ…å†µä¸‹XGBoostçš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°æ–¹æ³•æ›´å…·å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§)
    # top_k_xgboost = 6
    # gbtree = XGBClassifier(n_estimators=2000, max_depth=4, learning_rate=0.05, n_jobs=8)
    # gbtree.set_params(eval_metric='auc', early_stopping_rounds=100)
    # X_train_df = pd.DataFrame(X_train, columns=feature_names[:16])
    # X_test_df = pd.DataFrame(X_test, columns=feature_names[:16])
    # gbtree.fit(X_train, y_train,eval_set=[(X_test, y_test)], verbose=100)
    # feature_importances = gbtree.feature_importances_
    # top_k_indices = np.argpartition(-feature_importances, top_k_xgboost)[:top_k_xgboost]
    # print("XGBoostæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_xgboost, top_k_indices))

    # choice æ— æ¨¡å‹(éå‚æ•°)æ–¹æ³•ä¸­çš„Permutation Feature Importance-slearnï¼Œéœ€è¦å€ŸåŠ©XGBoost

    # # åŠŸèƒ½ï¼špermutation_importance é€šè¿‡åœ¨æ•°æ®é›†ä¸­å¯¹ç‰¹å¾è¿›è¡Œéšæœºæ‰“ä¹±ï¼ˆpermutationï¼‰å¹¶è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ï¼Œæ¥è¡¡é‡ç‰¹å¾çš„é‡è¦æ€§ã€‚ç‰¹å¾çš„é‡è¦æ€§æ˜¯é€šè¿‡æŸ¥çœ‹ç‰¹å¾æ‰“ä¹±åæ¨¡å‹æ€§èƒ½çš„ä¸‹é™ç¨‹åº¦æ¥ç¡®å®šçš„ã€‚
    # # é€‚ç”¨æ€§ï¼šè¿™ä¸ªæ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šçš„æ¨¡å‹ï¼Œå› æ­¤å¯ä»¥ä¸ä»»ä½• sklearn å…¼å®¹çš„æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºçº¿æ€§æ¨¡å‹ã€æ ‘æ¨¡å‹ã€é›†æˆæ¨¡å‹ç­‰ã€‚
    # result = permutation_importance(gbtree, X_train, y_train, n_repeats=10,random_state=42)
    # permutation_importance = result.importances_mean
    # top_k_permutation = np.argpartition(-permutation_importance, top_k_xgboost)[:top_k_xgboost]
    # print("Permutation Feature Importance-slearnæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_xgboost, top_k_permutation))

    # choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
    import re

    svm_model = svm.SVC(class_weight='balanced', probability=True)
    svm_model.fit(X_train, y_train)
    # ç‰¹å¾æ•°å–4æˆ–6
    i = len(feature_names)
    np.random.seed(1)
    categorical_names = {}
    for feature in categorical_features:
        le = LabelEncoder()
        le.fit(data.iloc[:, feature])
        data.iloc[:, feature] = le.transform(data.iloc[:, feature])
        categorical_names[feature] = le.classes_
    explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                     categorical_features=categorical_features,
                                     categorical_names=categorical_names, kernel_width=3)
    # predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
    predict_fn = lambda x: svm_model.predict_proba(x)
    exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names) // 2)
    # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
    top_features = exp.as_list()
    top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
    top_k_indices = [feature_names.index(name) for name in top_feature_names]
    print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))
    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("æœ‰å½±å“åŠ›çš„ç‰¹å¾è®¡ç®—è€—æ—¶ï¼š", end_time - start_time)

    # section outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)èšåˆå‡½æ•°ï¼Œå¦‚æœå¯¹äºDä¸­æ‰€æœ‰å…ƒç»„sï¼Œt.Aä¸s.Aè‡³å°‘ç›¸å·®ä¸€ä¸ªå› å­ğœƒï¼Œåˆ™è°“è¯è¿”å›trueï¼Œå¦åˆ™è¿”å›false

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # # subsection ä»å­—é¢æ„æ€å®ç°outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)
    # threshold = 0.1
    # col_indices = 3
    # row_indices = 10
    # select_feature = feature_names[col_indices]
    # # è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
    # select_column_data = data_copy[select_feature].values
    # # æ‰¾åˆ°æ‰€é€‰åˆ—çš„æœ€å¤§å€¼å’Œæœ€å°å€¼
    # max_value = np.max(select_column_data)
    # min_value = np.min(select_column_data)
    # # æ‰¾åˆ°t.Aå¯¹åº”çš„å€¼
    # t_value = data_copy.iloc[row_indices, col_indices]
    # # å¯¹æ•°æ®è¿›è¡Œæ’åº
    # # sorted_data = np.sort(select_column_data)
    # sorted_indices = np.argsort(select_column_data)
    # sorted_data = select_column_data[sorted_indices]
    # # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ¯” t_value å¤§çš„å€¼å’Œæ¯” t_value å°çš„å€¼
    # greater_than_t_value = sorted_data[sorted_data > t_value]
    # less_than_t_value = sorted_data[sorted_data < t_value]
    # # æ‰¾åˆ°ä¸t_valueæœ€æ¥è¿‘çš„æ›´å¤§çš„å€¼å’Œæ›´å°çš„å€¼
    # if greater_than_t_value.size > 0:
    #     closest_greater = greater_than_t_value[0]  # æœ€è¿‘çš„å¤§äº t_value çš„å€¼
    # else:
    #     closest_greater = t_value
    # if less_than_t_value.size > 0:
    #     closest_less = less_than_t_value[-1]  # æœ€è¿‘çš„å°äº t_value çš„å€¼
    # else:
    #     closest_less = t_value
    # # åˆ¤æ–­t.Aæ˜¯å¦æ˜¯å¼‚å¸¸å€¼
    # if max_value == t_value:
    #     print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold)
    # elif min_value == t_value:
    #     print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", closest_greater - t_value > threshold)
    # else:
    #     print("å…ƒç»„tåœ¨å±æ€§Aä¸Šçš„æŠ•å½±æ˜¯å¼‚å¸¸å€¼å—:", t_value - closest_less > threshold and t_value - closest_less > threshold)
    # # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    # outliers = []
    # outliers_index = []
    # # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    # if len(sorted_data) > 1:
    #     if (sorted_data[1] - sorted_data[0] >= threshold):
    #         outliers.append(sorted_data[0])
    #         outliers_index.append(sorted_indices[0])
    #     if (sorted_data[-1] - sorted_data[-2] >= threshold):
    #         outliers.append(sorted_data[-1])
    #         outliers_index.append(sorted_indices[-1])
    # # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    # for i in range(1, len(sorted_data) - 1):
    #     current_value = sorted_data[i]
    #     left_value = sorted_data[i - 1]
    #     right_value = sorted_data[i + 1]
    #     if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
    #         outliers.append(current_value)
    #         outliers_index.append(sorted_indices[i])
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼çš„ç´¢å¼•ä¸ºï¼š", outliers_index)
    # print("Aå±æ€§ä¸‹æ‰€æœ‰å¼‚å¸¸å€¼ä¸ºï¼š", outliers)

    # # subsection é‡‡ç”¨é—´éš”æ–¹æ³•ï¼Œä½¿ç”¨Modified Z-scoreæ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
    # def modified_z_score(points, thresh=3.5):
    #     if len(points.shape) == 1:
    #         points = points[:, None]
    #     median = np.median(points, axis=0)
    #     diff = np.sum((points - median)**2, axis=-1)
    #     diff = np.sqrt(diff)
    #     med_abs_deviation = np.median(diff)
    #     modified_z_score = 0.6745 * diff / med_abs_deviation
    #     return modified_z_score > thresh
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # value_labels = modified_z_score(feature_values)
    # true_indices = np.where(value_labels)[0]
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("modified_z_scoreæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", true_indices)
    # print("modified_z_scoreæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(true_indices))

    # # subsection é‡‡ç”¨é—´éš”æ–¹æ³•ï¼Œä½¿ç”¨2MADeæ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
    # def calculate_made(data):
    #     median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    #     abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    #     mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    #     made = 1.843 * mad
    #     return median, made
    #
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # median, made = calculate_made(feature_values)
    # lower_threshold = median - 2 * made
    # upper_threshold = median + 2 * made
    # made_indices = np.where((feature_values > upper_threshold) | (feature_values < lower_threshold))[0]
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("2MADeæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", made_indices)
    # print("2MADeæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(made_indices))

    # # subsection é‡‡ç”¨1.5IQRä¸‹çš„ç®±çº¿å›¾æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
    # def calculate_iqr(data):
    #     sorted_data = np.sort(data)  # å°†æ•°æ®é›†æŒ‰å‡åºæ’åˆ—
    #     q1 = np.percentile(sorted_data, 25)  # è®¡ç®—ä¸‹å››åˆ†ä½æ•°
    #     q3 = np.percentile(sorted_data, 75)  # è®¡ç®—ä¸Šå››åˆ†ä½æ•°
    #     iqr = q3 - q1
    #     return q1, q3, iqr
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # q1, q3, iqr = calculate_iqr(feature_values)
    # lower_bound = q1 - 1.5 * iqr
    # upper_bound = q3 + 1.5 * iqr
    # box_plot_indices = np.where((feature_values > upper_bound) | (feature_values < lower_bound))[0]
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("ç®±çº¿å›¾æ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", box_plot_indices)
    # print("ç®±çº¿å›¾æ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(box_plot_indices))

    # # subsection é‡‡ç”¨æ ‡å‡†å·®æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # mean = feature_values.mean()
    # std = feature_values.std()
    # upper_bound = mean + 3 * std
    # lower_bound = mean - 3 * std
    # std_indices = np.where((feature_values > upper_bound) | (feature_values < lower_bound))[0]
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("æ ‡å‡†å·®æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", std_indices)
    # print("æ ‡å‡†å·®æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(std_indices))

    # # subsection é‡‡ç”¨distæ‹Ÿåˆå•åˆ—æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•ï¼Œæ•°æ®ä¸­å¯èƒ½å­˜åœ¨å¤šä¸ªåˆ†å¸ƒï¼Œå¯ä»¥è€ƒè™‘ç”¨åˆ†æ®µå‡½æ•°å»ºæ¨¡ï¼ˆç›¸å¯¹äºfilteræ–¹æ³•è¯¯å·®èŒƒå›´å¾ˆå¤§ï¼‰
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # dist = distfit(todf=True)
    # dist.fit_transform(feature_values)
    # # è·å–æœ€ä½³åˆ†å¸ƒ
    # best_distribution_name = dist.model['name']
    # best_distribution_params = dist.model['params']
    # # æ ¹æ®æœ€ä½³åˆ†å¸ƒåç§°å’Œå‚æ•°æ„å»ºå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡
    # best_distribution = getattr(stats, best_distribution_name)(*best_distribution_params)
    # # è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡å¯†åº¦
    # densities = best_distribution.pdf(feature_values)
    # # å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œä¾‹å¦‚ä½äºè¿™ä¸ªé˜ˆå€¼çš„ç‚¹è¢«è§†ä¸ºå¼‚å¸¸ç‚¹
    # threshold = 0.01
    # # æ‰¾åˆ°å¼‚å¸¸ç‚¹
    # outliers_indices = np.where(densities < threshold)[0]
    # dist.plot()
    # plt.show()
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("ä½äºdistæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹ç´¢å¼•:", outliers_indices)
    # print("ä½äºdistæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹æ•°é‡:", len(outliers_indices))

    # # subsection é‡‡ç”¨filter fittingæ‹Ÿåˆå•åˆ—æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
    # col_indices = 5
    # select_feature = feature_names[col_indices]
    # feature_values = data_copy[select_feature].values
    # f = Fitter(feature_values, distributions=['norm', 't', 'laplace'])
    # f.fit()
    # # è®¡ç®—æœ€ä½³åˆ†å¸ƒå’Œæœ€ä½³å‚æ•°
    # pattern = r'\[(.*?)\]'
    # best_dist_name_key = f.get_best(method='sumsquare_error').keys()
    # best_dist_name = key_string = ', '.join(str(key) for key in best_dist_name_key)
    # best_params = None
    # for dist_name, params in f.fitted_param.items():
    #     if dist_name == best_dist_name:
    #         best_params = params
    #         break
    # if best_params is None:
    #     raise ValueError(f"No parameters found for the best distribution '{best_dist_name}'")
    # # æ„å»ºå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡
    # best_dist = getattr(stats, best_dist_name)(*best_params)
    # # è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡å¯†åº¦
    # densities = best_dist.pdf(feature_values)
    # # è®¾å®šé˜ˆå€¼æ‰¾å‡ºæ¦‚ç‡å¯†åº¦ä½äºé˜ˆå€¼çš„æ ·æœ¬ç‚¹ä½œä¸ºå¼‚å¸¸ç‚¹
    # threshold = 0.01  # ä¸¾ä¾‹è®¾å®šé˜ˆå€¼
    # outliers_indices = np.where(densities < threshold)[0]
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("ä½äºfilter fittingæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹ç´¢å¼•:", outliers_indices)
    # print("ä½äºfilter fittingæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹æ•°é‡:", len(outliers_indices))

    # subsection åŸºäºå®šä¹‰ï¼Œé‡‡ç”¨åˆ†ç®±æ–¹æ³•å®ç°outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)èšåˆå‡½æ•°

    # outlier_feature_indices = {}
    # threshold = 0.01
    # for column_indice in top_k_indices:
    #     select_feature = feature_names[column_indice]
    #     select_column_data = data_copy[select_feature].values
    #     max_value = np.max(select_column_data)
    #     min_value = np.min(select_column_data)
    #     sorted_indices = np.argsort(select_column_data)
    #     sorted_data = select_column_data[sorted_indices]
    #     # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    #     outliers = []
    #     outliers_index = []
    #     # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    #     if len(sorted_data) > 1:
    #         if (sorted_data[1] - sorted_data[0] >= threshold):
    #             outliers.append(sorted_data[0])
    #             outliers_index.append(sorted_indices[0])
    #         if (sorted_data[-1] - sorted_data[-2] >= threshold):
    #             outliers.append(sorted_data[-1])
    #             outliers_index.append(sorted_indices[-1])
    #     # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    #     for i in range(1, len(sorted_data) - 1):
    #         current_value = sorted_data[i]
    #         left_value = sorted_data[i - 1]
    #         right_value = sorted_data[i + 1]
    #         if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
    #             outliers.append(current_value)
    #             outliers_index.append(sorted_indices[i])
    #     outliers_index_numpy = np.array(outliers_index)
    #     intersection = np.array(outliers_index)
    #     # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    #     outlier_feature_indices[column_indice] = intersection
    # # print(outlier_feature_indices)

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)èšåˆå‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # section imbalance d(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œå¦‚æœDä¸­æŒ‰t.Aåˆ†ç»„çš„å…ƒç»„æ•°é‡æ¯”å…¶ä»–ç»„çš„è®¡æ•°å°Aå€¼(è‡³å°‘å°ä¸€ä¸ªå› å­ğ›¿)ï¼Œåˆ™è¿”å›trueï¼Œå¦åˆ™è¿”å›false

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # subsection ä»å­—é¢æ„æ€çš„å…·ä½“å€¼å‡ºç°é¢‘ç‡åˆ¤æ–­æ˜¯å¦ä¸å¹³è¡¡,å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼ŒåŸºç¡€ç‰ˆæœ¬(å­˜åœ¨åˆ†ç»„æ•°é‡ä¸t.Aç›¸å·®ğ›¿)
    # import balanace.imbalanced as imbalance
    # col_indices = 16
    # row_indices = 10
    # delta = 2
    # feature = feature_names[col_indices]
    # data_copy = pd.read_excel(file_path).astype(str)
    # imbalanced = imbalance.Imbalanced(data_copy, feature)
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å…ƒç»„ä¸‹æ ‡
    # ta = data_copy.iloc[row_indices, col_indices]
    # print("æ‰€é€‰åˆ—æ˜¯å¦ä¸å¹³è¡¡ï¼š", imbalanced.enum_check(ta, delta))

    # subsection ä»å­—é¢æ„æ€å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼ŒåŸºç¡€ç‰ˆæœ¬(æ‰€æœ‰åˆ†ç»„æ•°é‡ä¸t.Aè‡³å°‘ç›¸å·®ğ›¿)

    # def check_delta(value_counts, ta_count, delta):
    #     for value, count in value_counts.items():
    #         if value != ta and abs(count - ta_count) < delta:
    #             return False
    #     return True
    # col_indices = 16
    # row_indices = 10
    # delta = 2
    # feature = feature_names[col_indices]
    # data_copy = pd.read_excel(file_path).astype(str)
    # # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å…ƒç»„ä¸‹æ ‡
    # ta = data_copy.iloc[row_indices, col_indices]
    # # è·å¾—æ‰€é€‰åˆ—çš„æ•°æ®
    # select_column_data = data_copy[feature].values
    # equal_size = len(data_copy[feature]) / len(set(data_copy[feature]))
    # delta_threshold = delta * equal_size
    # # è·å–æ‰€æœ‰å€¼çš„è®¡æ•°
    # value_counts = pd.Series(select_column_data).value_counts()
    # ta_count = value_counts.get(ta, 0)
    # # å°†taåˆ†ç»„è®¡æ•°ä¸ç»„å†…å…¶ä»–å€¼è®¡æ•°è¿›è¡Œæ¯”è¾ƒ
    # print("æ‰€é€‰åˆ—æ˜¯å¦ä¸å¹³è¡¡ï¼š", check_delta(value_counts, ta_count, delta_threshold))

    # subsection ä»ç»Ÿè®¡è§†è§’å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—è¿›è¡Œæ ‡å‡†åŒ–å’Œåˆ†ç®±ï¼Œåˆ¤æ–­åˆ†ç®±ä¸­çš„å…ƒç´ æ•°æ˜¯å¦è¾¾åˆ°ä¸å¹³è¡¡ï¼ˆå­˜åœ¨ä¸¤åˆ†ç®±å¯¹åº”è®¡æ•°çš„å·®å€¼è‡³å°‘ä¸ºğ›¿ï¼‰

    # from sklearn.preprocessing import MinMaxScaler
    # # è®¾ç½®åˆ†ç®±ä¸­å…ƒç»„æ•°ç›¸å·®é˜ˆå€¼
    # delta = 0.05
    # # è®¾ç½®åˆ†ç»„çš„é—´éš”
    # interval = 0.01
    # # åˆå§‹åŒ–MinMaxScaler
    # scaler = MinMaxScaler()
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # data_copy = pd.read_excel(file_path)
    # data_copy[data.columns] = scaler.fit_transform(data[data.columns])
    # # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    # bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    # digitized = np.digitize(data_copy[select_feature], bins)
    # unique_bins, counts = np.unique(digitized, return_counts=True)
    # print(f"åˆ— '{select_feature}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
    # # ç»Ÿè®¡åŒ…å«æœ€å¤§å…ƒç´ æ•°å’Œæœ€å°å…ƒç´ æ•°çš„å·®å€¼
    # max_elements = np.max(counts)
    # min_elements = np.min(counts)
    # difference = max_elements - min_elements
    # print(f"åˆ— '{select_feature}' binsä¸­åŒ…å«æœ€å¤šçš„å…ƒç»„æ•°å’Œæœ€å°‘çš„å…ƒç»„æ•°ç›¸å·®äº† {difference}")
    # print("æ‰€é€‰åˆ—æ˜¯å¦ä¸å¹³è¡¡ï¼š", difference/data_copy.shape[0] >= delta)

    # subsection ä»ç»Ÿè®¡è§†è§’å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—è¿›è¡Œæ ‡å‡†åŒ–å’Œåˆ†ç®±ï¼Œåˆ¤æ–­åˆ†ç®±ä¸­çš„å…ƒç´ æ•°æ˜¯å¦è¾¾åˆ°ä¸å¹³è¡¡ï¼ˆt.Aä¸å…¶ä»–æ‰€æœ‰åˆ†ç®±å¯¹åº”è®¡æ•°çš„å·®å€¼è‡³å°‘ä¸ºğ›¿ï¼‰

    # from sklearn.preprocessing import MinMaxScaler
    # # è®¾ç½®åˆ†ç®±ä¸­å…ƒç»„æ•°ç›¸å·®é˜ˆå€¼
    # delta = 0.05
    # # è®¾ç½®åˆ†ç»„çš„é—´éš”
    # interval = 0.01
    # # åˆå§‹åŒ–MinMaxScaler
    # scaler = MinMaxScaler()
    # col_indices = 16
    # row_indices = 100
    # select_feature = feature_names[col_indices]
    # data_minmax = pd.read_excel(file_path)
    # data_minmax[data.columns] = scaler.fit_transform(data[data.columns])
    # ta = data_minmax.iloc[row_indices, col_indices]
    # # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    # bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    # digitized = np.digitize(data_minmax[select_feature], bins)
    # # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
    # unique_bins, counts = np.unique(digitized, return_counts=True)
    # # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
    # ta_bin = np.digitize([ta], bins)[0]
    # # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
    # ta_count = counts[unique_bins == ta_bin][0]
    # # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
    # min_diff = delta * data_minmax.shape[0]
    # # åˆ¤æ–­ ta æ‰€åœ¨é—´éš”çš„æ”¯æŒæ•°æ˜¯å¦ä¸å…¶ä»–æ‰€æœ‰é—´éš”çš„æ”¯æŒæ•°ç›¸å·®è‡³å°‘ min_diff
    # def check_min_diff(counts, ta_count, min_diff):
    #     for count in counts:
    #         if abs(count - ta_count) < min_diff:
    #             return False
    #     return True
    # # è¿›è¡Œæ£€æŸ¥
    # result = check_min_diff(counts, ta_count, min_diff)
    # print(f"Value of ta: {ta}")
    # print(f"Count in ta's bin: {ta_count}")
    # print("Is ta's bin count different from other bins by at least delta * data_copy.shape[0]?", result)

    # subsection ä»ç»Ÿè®¡è§†è§’å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œä¸è®¾ç½®ğ›¿çš„æ— é˜ˆå€¼æ–¹æ³•

    from sklearn.preprocessing import MinMaxScaler

    # è®¾ç½®åˆ†ç»„çš„é—´éš”
    interval = 0.01
    # åˆå§‹åŒ–MinMaxScaler
    scaler = MinMaxScaler()
    col_indices = 1
    row_indices = 100
    select_feature = feature_names[col_indices]
    data[data.columns] = scaler.fit_transform(data[data.columns])
    # åœ¨æ‰€æœ‰æ•°æ®Dä¸‹çš„å…ƒç»„ä¸‹æ ‡
    ta = data.iloc[row_indices, col_indices]
    # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    digitized = np.digitize(data[select_feature], bins)
    # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
    unique_bins, counts = np.unique(digitized, return_counts=True)
    # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
    ta_bin = np.digitize([ta], bins)[0]
    # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
    ta_count = counts[unique_bins == ta_bin][0]
    # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
    median, made = calculate_made(counts)
    lower_threshold = median - 2 * made
    upper_threshold = median + 2 * made
    if ta_count < lower_threshold or ta_count > upper_threshold:
        print("æ‰€é€‰åˆ—Aåœ¨æ‰€é€‰å…ƒç»„tå¤„æ˜¯ä¸å¹³è¡¡çš„")
    else:
        print("æ‰€é€‰åˆ—Aåœ¨æ‰€é€‰å…ƒç»„tå¤„æ˜¯å¹³è¡¡çš„")

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("imbalance d(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)èšåˆå‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # SECTION SDomain(ğ·, ğ‘…, ğ´, ğœ)ï¼Œå¦‚æœDçš„Aå±æ€§çš„ä¸åŒå€¼æ•°é‡å°äºç•Œé™ğœï¼Œåˆ™è¿”å›true

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    # subsection ä»å­—é¢æ„æ€Aåˆ—çš„ä¸åŒå€¼æ•°é‡æ˜¯å¦æ˜æ˜¾å°äºç»™å®šçš„é˜ˆå€¼ğœ
    # import balanace.sdomain as sd
    # col_indices = 16
    # # è®¾ç½®æ¯åˆ—ä¸åŒå…ƒç´ æ•°é‡è¦è¾¾åˆ°çš„æœ€å°é˜ˆå€¼
    # sigma = 2
    # feature = feature_names[col_indices]
    # data_copy = pd.read_excel(file_path)
    # imbalanced = sd.SDomian(data_copy, feature)
    # print("æ‰€é€‰åˆ—çš„æ´»åŠ¨åŸŸæ˜¯å¦å°äºè®¾ç½®é˜ˆå€¼ï¼š", imbalanced.enum_check(sigma))

    # subsection æ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—çš„å€¼è¿›è¡Œæ ‡å‡†åŒ–ååˆ†ç®±åˆ¤æ–­æŸåˆ—å¯¹åº”åˆ†ç®±çš„æ•°é‡æ˜¯å¦å°äºæ‰€æœ‰åˆ—åˆ†ç®±æ•°çš„å‡å€¼ï¼ˆä¸åˆç†ï¼‰

    # from sklearn.preprocessing import MinMaxScaler
    # # è®¾ç½®åˆ†ç»„çš„é—´éš”
    # interval = 0.01
    # # åˆå§‹åŒ–MinMaxScaler
    # scaler = MinMaxScaler()
    # col_indices = 3
    # select_feature = feature_names[col_indices]
    # data_copy = pd.read_excel(file_path)
    # data_copy[data.columns] = scaler.fit_transform(data[data.columns])
    # # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    # bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    # # ç»Ÿè®¡æ¯åˆ—æ•°æ®å æ®äº†å¤šå°‘ä¸ªé—´éš”
    # total_bins = 0
    # selected_bins = 0
    # for column in data_copy.columns:
    #     digitized = np.digitize(data_copy[column], bins)
    #     unique_bins, counts = np.unique(digitized, return_counts=True)
    #     print(f"åˆ— '{column}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
    #     total_bins += len(unique_bins)
    #     if column == select_feature:
    #         selected_bins = len(unique_bins)
    # mean_bins = total_bins / len(data_copy.columns)
    # print("æ‰€é€‰ç‰¹å¾æ˜¯å¦æ´»åŠ¨åŸŸå¾ˆå°ï¼š", selected_bins < mean_bins)

    # subsection æ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—çš„å€¼è¿›è¡Œæ ‡å‡†åŒ–ååˆ†ç®±åˆ¤æ–­æŸåˆ—å¯¹åº”åˆ†ç®±çš„æ•°é‡æ˜¯å¦è¿‡å°ï¼ˆä¸è®¾ç½®é˜ˆå€¼ğ›¿ï¼Œé‡‡ç”¨2MADeç»Ÿè®¡æ–¹æ³•ï¼‰

    # from sklearn.preprocessing import MinMaxScaler
    # # è®¾ç½®åˆ†ç»„çš„é—´éš”
    # interval = 0.01
    # col_indices = 3
    # selected_bins = 0
    # columns_bins = {}
    # columns_bins_count = []
    # # åˆå§‹åŒ–MinMaxScaler
    # scaler = MinMaxScaler()
    # select_feature = feature_names[col_indices]
    # data_minmax = pd.read_csv(file_path)
    # data_minmax[data.columns] = scaler.fit_transform(data[data.columns])
    # # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    # bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    # # ç»Ÿè®¡æ¯åˆ—æ•°æ®å æ®äº†å¤šå°‘ä¸ªé—´éš”
    # for column in data_minmax.columns:
    #     digitized = np.digitize(data_minmax[column], bins)
    #     unique_bins, counts = np.unique(digitized, return_counts=True)
    #     print(f"åˆ— '{column}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
    #     columns_bins[column] = len(unique_bins)
    #     columns_bins_count.append(len(unique_bins))
    #     if column == select_feature:
    #         selected_bins = len(unique_bins)
    # median, made = calculate_made(np.array(columns_bins_count))
    # lower_threshold = median - 2 * made
    # upper_threshold = median + 2 * made
    # if selected_bins < lower_threshold:
    #     print("æ‰€é€‰åˆ—çš„æ´»åŠ¨åŸŸè¿‡å°")
    # else:
    #     print("æ‰€é€‰åˆ—çš„æ´»åŠ¨åŸŸæ­£å¸¸")

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("SDomain(ğ·, ğ‘…, ğ´, ğœ)èšåˆå‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # section è°“è¯loss(M, D, ğ‘¡)çš„å®ç°ï¼ˆäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼‰ï¼Œæ£€æµ‹good/bad outliers

    start_time = time.time()  # å¼€å§‹æ—¶é—´
    print("*" * 100)
    # è·å–å†³ç­–å€¼
    decision_values = svm_model.decision_function(X_copy)
    # å°†å†³ç­–å€¼è½¬æ¢ä¸ºé€‚ç”¨äº Softmax çš„äºŒç»´æ•°ç»„
    decision_values_reshaped = decision_values.reshape(-1, 1)  # å˜æˆ (n_samples, 1)
    # åº”ç”¨ Softmax å‡½æ•°ï¼ˆå¯ä»¥æ‰‹åŠ¨å®ç°æˆ–ä½¿ç”¨ scipyï¼‰
    y_pred = softmax(np.hstack((decision_values_reshaped, -decision_values_reshaped)), axis=1)
    # åˆ›å»º OneHotEncoder å®ä¾‹
    encoder = OneHotEncoder(sparse=False)
    # é¢„æµ‹y_testçš„å€¼ï¼Œå¹¶ä¸y_trainç»„åˆæˆä¸ºy_ground
    y_test_pred = svm_model.predict(X_test_copy)
    y_ground = np.hstack((y_train, y_test_pred))
    # å¯¹y_groundè¿›è¡Œç‹¬çƒ­ç¼–ç 
    y_true = encoder.fit_transform(y_ground.reshape(-1, 1))
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
    loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
    # è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
    average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
    bad_samples = np.where(loss_per_sample > average_loss)[0]
    good_samples = np.where(loss_per_sample <= average_loss)[0]
    # æµ‹è¯•æ ·æœ¬ä¸­çš„bad outliersç´¢å¼•
    bad_outliers_index = np.intersect1d(test_outliers_index, bad_samples)
    print("æ£€æµ‹å‡ºçš„outliersä¸­bad outliersçš„æ•°é‡ï¼š", len(bad_outliers_index))
    # æµ‹è¯•æ ·æœ¬ä¸­çš„good outliersç´¢å¼•
    good_outliers_index = np.intersect1d(test_outliers_index, good_samples)
    print("æ£€æµ‹å‡ºçš„outliersä¸­good outliersçš„æ•°é‡ï¼š", len(good_outliers_index))
    # good outliersä¸­åˆ†é”™çš„æ¯”ä¾‹
    good_wrong_indies = []
    for i in good_outliers_index:
        true_label = y_test[i]
        if true_label != test_pred_labels[i]:
            good_wrong_indies.append(i)
    print("good outliersä¸­æ ·æœ¬åˆ†é”™çš„æ¯”ä¾‹ï¼š", len(good_wrong_indies) / len(good_outliers_index))
    # bad outliersä¸­åˆ†é”™çš„æ¯”ä¾‹
    bad_wrong_indies = []
    for i in bad_outliers_index:
        true_label = y_test[i]
        if true_label != test_pred_labels[i]:
            bad_wrong_indies.append(i)
    print("bad outliersä¸­æ ·æœ¬åˆ†é”™çš„æ¯”ä¾‹ï¼š", len(bad_wrong_indies) / len(bad_outliers_index))

    end_time = time.time()  # ç»“æŸæ—¶é—´
    print("loss(M, D, ğ‘¡)æŸå¤±å‡½æ•°å®ç°è€—æ—¶ï¼š", end_time - start_time)

    # å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¤„ç†åé‡æ–°è®­ç»ƒSVMæ¨¡å‹

    # SECTION åŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
    # print("*" * 100)
    # print("åŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
    # print("åŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
    # print("*" * 100)

    # SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­åˆ†ç±»é”™è¯¯ï¼ˆhingeæŸå¤±å‡½æ•°é«˜äº1ï¼‰ä¸”è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
    # # ç”Ÿæˆå¸ƒå°”ç´¢å¼•ï¼Œä¸ºè¦åˆ é™¤çš„è¡Œåˆ›å»ºå¸ƒå°”å€¼æ•°ç»„
    # mask = np.ones(len(X_train), dtype=bool)
    # mask[inter_anomalies] = False
    # # ä½¿ç”¨å¸ƒå°”ç´¢å¼•åˆ é™¤é‚£äº›æ—¢è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œåˆä½¿å¾—hingeæŸå¤±é«˜äº1çš„æ ·æœ¬
    # X_train_split = X_train[mask]
    # y_train_split = y_train[mask]
    # # é‡æ–°è®­ç»ƒSVMæ¨¡å‹
    # svm_model_split = svm.SVC(class_weight='balanced', probability=True)
    # svm_model_split.fit(X_train_split, y_train_split)
    # print("*" * 100)
    # print("å»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_split, svm_model_split.predict(X_train_split))))
    # print("å»é™¤å¼‚å¸¸å€¼ä¸­åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_split.predict(X_test))))
    # print("*" * 100)

    # SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­åˆ†ç±»é”™è¯¯ï¼ˆhingeæŸå¤±å‡½æ•°é«˜äº1ï¼‰çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
    # mask_h = np.ones(len(X_train), dtype=bool)
    # mask_h[anomalies] = False
    # X_train_h = X_train[mask_h]
    # y_train_h = y_train[mask_h]
    # svm_model_h = svm.SVC(class_weight='balanced', probability=True)
    # svm_model_h.fit(X_train_h, y_train_h)
    # print("*" * 100)
    # print("å»é™¤æŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_h, svm_model_h.predict(X_train_h))))
    # print("å»é™¤æŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_h.predict(X_test))))
    # print("*" * 100)

    # SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
    # mask_o = np.ones(len(X_train), dtype=bool)
    # mask_o[train_outliers_index] = False
    # X_train_o = X_train[mask_o]
    # y_train_o = y_train[mask_o]
    # svm_model_o = svm.SVC(class_weight='balanced', probability=True)
    # svm_model_o.fit(X_train_o, y_train_o)
    # print("*" * 100)
    # print("å»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_o, svm_model_o.predict(X_train_o))))
    # print("å»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_o.predict(X_test))))
    # print("*" * 100)

    t1 = time.time()  # å¼€å§‹æ—¶é—´
    print("è§„åˆ™æ‰§è¡Œæ€»è€—æ—¶ï¼š", t1-t0)