"""
ğ‘…(ğ‘¡) âˆ§ Mğ‘œ (ğ‘¡, D) âˆ§ ğ‘‹1 â†’ ugly(ğ‘¡)
é‡‡ç”¨ä¸åŒä¿®å¤ç­–ç•¥å¯¹ä¸Šè§„åˆ™å½¢å¼å‘ç°çš„ugly outliersè¿›è¡Œä¿®å¤
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from deepod.models.tabular import PReNet
from sklearn.preprocessing import MinMaxScaler

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†ï¼Œè¾“å…¥åŸå§‹å¤šåˆ†ç±»æ•°æ®é›†ï¼Œåœ¨ä¸­é—´å¤„ç†è¿‡ç¨‹è½¬åŒ–ä¸ºå¼‚å¸¸æ£€æµ‹æ•°æ®é›†

# choice drybeanæ•°æ®é›†

# file_path = "../datasets/multi_class_to_outlier/drybean_outlier.csv"
# data = pd.read_csv(file_path)
file_path = "../datasets/multi_class/drybean.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
label_name = data.columns[-1]
# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])
X = data.values[:, :-1]
y = data.values[:, -1]

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)

# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

# section æ•°æ®ç‰¹å¾ç¼©æ”¾å’Œæ•°æ®åŠ å™ª

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
feature_names = data.columns.values.tolist()
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

# SECTION Mğ‘œ (ğ‘¡, D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨PReNet
# subsection è®¾ç½®è®­ç»ƒæµ‹è¯•å¼±ç›‘ç£æ ·æœ¬
# è®¾ç½®å¼±ç›‘ç£è®­ç»ƒæ ·æœ¬
# æ‰¾åˆ°æ‰€æœ‰æ ‡ç­¾ä¸º 1 çš„æ ·æœ¬ç´¢å¼•
semi_label_ratio = 0.1  # è®¾ç½®å·²çŸ¥çš„å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹
positive_indices = np.where(y_train == min_label)[0]
# éšæœºé€‰æ‹© 10% çš„æ­£æ ·æœ¬
n_positive_to_keep = int(len(positive_indices) * semi_label_ratio)
selected_positive_indices = np.random.choice(positive_indices, n_positive_to_keep, replace=False)
# åˆ›å»ºç”¨äºå¼‚å¸¸æ£€æµ‹å™¨çš„è®­ç»ƒæ ‡ç­¾
y_semi = np.zeros_like(y_train)  # é»˜è®¤å…¨ä¸º 0
y_semi[selected_positive_indices] = 1  # è®¾ç½®é€‰ä¸­çš„æ­£æ ·æœ¬ä¸º 1
# åˆ›å»ºç”¨äºå¼‚å¸¸æ£€æµ‹å™¨çš„æµ‹è¯•æ ‡ç­¾
y_semi_test = np.zeros_like(y_test)
test_positive_indices = np.where(y_test == min_label)[0]
y_semi_test[test_positive_indices] = 1

# subsection å¼‚å¸¸æ£€æµ‹å™¨è®­ç»ƒ
epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42

out_clf = PReNet(epochs=epochs, device=device, random_state=random_state)
out_clf.fit(X_train, y=y_semi)

out_clf_noise = PReNet(epochs=epochs, device=device, random_state=random_state)
out_clf_noise.fit(X_train_copy, y_semi)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
# subsection ä»åŸå§‹è®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

print("*"*100)
train_scores = out_clf.decision_function(X_train)
train_pred_labels, train_confidence = out_clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
train_correct_detect_samples = []
for i in range(len(X_train)):
    if train_pred_labels[i] == y_semi[i]:
        train_correct_detect_samples.append(i)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸æ£€æµ‹å™¨çš„æ£€æµ‹å‡†ç¡®åº¦ï¼š", len(train_correct_detect_samples)/len(X_train))
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index)/len(X_train))

# subsection ä»åŸå§‹æµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

print("*"*100)
test_scores = out_clf.decision_function(X_test)
test_pred_labels, test_confidence = out_clf.predict(X_test, return_confidence=True)
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
test_outliers_index = []
print("æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test))
for i in range(len(X_test)):
    if test_pred_labels[i] == 1:
        test_outliers_index.append(i)
test_correct_detect_samples = []
for i in range(len(X_test)):
    if test_pred_labels[i] == y_semi_test[i]:
        test_correct_detect_samples.append(i)
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸æ£€æµ‹å™¨çš„æ£€æµ‹å‡†ç¡®åº¦ï¼š", len(test_correct_detect_samples)/len(X_test))
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index)
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index))
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index)/len(X_test))

# section ä»åŠ å™ªæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºçš„å¼‚å¸¸å€¼
# subsection ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

print("*"*100)
train_scores_noise = out_clf_noise.decision_function(X_train_copy)
train_pred_labels_noise, train_confidence_noise = out_clf_noise.predict(X_train_copy, return_confidence=True)
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
train_outliers_index_noise = []
print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
for i in range(len(X_train_copy)):
    if train_pred_labels_noise[i] == 1:
        train_outliers_index_noise.append(i)
train_correct_detect_samples_noise = []
for i in range(len(X_train_copy)):
    if train_pred_labels_noise[i] == y_semi[i]:
        train_correct_detect_samples_noise.append(i)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸æ£€æµ‹å™¨çš„æ£€æµ‹å‡†ç¡®åº¦ï¼š", len(train_correct_detect_samples_noise)/len(X_train_copy))
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index_noise))
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index_noise)/len(X_train_copy))

# subsection ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

print("*"*100)
test_scores_noise = out_clf_noise.decision_function(X_test_copy)
test_pred_labels_noise, test_confidence_noise = out_clf_noise.predict(X_test_copy, return_confidence=True)
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
test_outliers_index_noise = []
print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
for i in range(len(X_test_copy)):
    if test_pred_labels_noise[i] == 1:
        test_outliers_index_noise.append(i)
test_correct_detect_samples_noise = []
print(len(test_pred_labels_noise), len(y_test))
for i in range(len(X_test_copy)):
    if test_pred_labels_noise[i] == y_semi_test[i]:
        test_correct_detect_samples_noise.append(i)
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸æ£€æµ‹å™¨çš„æ£€æµ‹å‡†ç¡®åº¦ï¼š", len(test_correct_detect_samples_noise)/len(X_test_copy))
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index_noise))
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index_noise)/len(X_test_copy))

# SECTION SVMæ¨¡å‹çš„å®ç°å’Œå‡†ç¡®åº¦æµ‹è¯•

# subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
svm_model = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)

# è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != svm_model.predict(X_train))[0]
print("è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != svm_model.predict(X_test))[0]
print("æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

print("*" * 100)
svm_model_noise = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_model_noise.fit(X_train_copy, y_train)
train_label_pred_noise = svm_model_noise.predict(X_train_copy)

# åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices_noise = np.where(y_train != svm_model_noise.predict(X_train_copy))[0]
print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices_noise)/len(y_train))

# åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices_noise = np.where(y_test != svm_model_noise.predict(X_test_copy))[0]
print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices_noise)/len(y_test))

# æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š", (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise))/(len(y_train) + len(y_test)))

# section ç¡®å®šæœ‰å½±å“åŠ›çš„ç‰¹å¾
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
import re

# ç‰¹å¾æ•°å–4æˆ–6
i = len(feature_names)
np.random.seed(1)
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_
explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)
# predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
predict_fn = lambda x: svm_model.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names)//2)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
top_k_indices = [feature_names.index(name) for name in top_feature_names]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
# top_features = exp.as_list()
# important_features = []
# for feature_set in top_features:
#     feature_long = feature_set[0]
#     for feature in feature_names:
#         if set(feature).issubset(set(feature_long)):
#             important_features.append(feature)
#             break
# top_k_indices = [feature_names.index(feature_name) for feature_name in important_features]
# print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# section è¯†åˆ«X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„

# å¼‚å¸¸æ£€æµ‹å™¨æ£€æµ‹å‡ºçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
train_outliers_noise = train_indices[train_outliers_index_noise]
test_outliers_noise = test_indices[test_outliers_index_noise]
outliers_noise = np.union1d(train_outliers_noise, test_outliers_noise)

# åœ¨åŠ å™ªæ•°æ®é›†D'ä¸Šè®­ç»ƒçš„SVMæ¨¡å‹ï¼Œå…¶åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
train_wrong_clf_noise = train_indices[wrong_classified_train_indices_noise]
test_wrong_clf_noise = test_indices[wrong_classified_test_indices_noise]
wrong_clf_noise = np.union1d(train_wrong_clf_noise, test_wrong_clf_noise)

# outlierså’Œåˆ†é”™æ ·æœ¬çš„å¹¶é›†
train_union = np.union1d(train_outliers_noise, train_wrong_clf_noise)
test_union = np.union1d(test_outliers_noise, test_wrong_clf_noise)

# åŠ å™ªæ•°æ®é›†D'ä¸Šéœ€è¦ä¿®å¤çš„å€¼
# éœ€è¦ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_repair_indices = np.union1d(outliers_noise, wrong_clf_noise)

# section é€‰å–æ´»åŠ¨åŸŸè¿‡å°çš„ç‰¹å¾

def calculate_made(data):
    median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
    abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
    mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
    made = 1.843 * mad
    return median, made

# åˆå§‹åŒ–MinMaxScaler
scaler = MinMaxScaler()
data_minmax = pd.read_excel(file_path)
data_minmax[data.columns] = scaler.fit_transform(data[data.columns])
# è®¾ç½®åˆ†ç»„çš„é—´éš”
interval = 0.01
# å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
columns_bins = {}
columns_bins_count = []
small_domain_features = []

for column in data_minmax.columns:
    digitized = np.digitize(data_minmax[column], bins)
    unique_bins, counts = np.unique(digitized, return_counts=True)
    columns_bins[column] = len(unique_bins)
    columns_bins_count.append(len(unique_bins))

for i in top_k_indices:
    select_feature = feature_names[i]
    selected_bins = columns_bins[select_feature]
    median, made = calculate_made(np.array(columns_bins_count))
    lower_threshold = median - 2 * made
    upper_threshold = median + 2 * made
    if selected_bins < lower_threshold:
        small_domain_features.append(i)
filtered_important_feature_indices = [item for item in top_k_indices if item not in small_domain_features]

imbalanced_tuple_indices = set()

# åˆå§‹åŒ–MinMaxScaler
scaler_new = MinMaxScaler()
data_imbalance = pd.read_excel(file_path)
data_imbalance[data.columns] = scaler_new.fit_transform(data[data.columns])

for feature in filtered_important_feature_indices:
    select_feature = feature_names[feature]
    # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
    bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
    digitized = np.digitize(data_imbalance[select_feature], bins)
    # ç»Ÿè®¡æ¯ä¸ªåŒºé—´çš„è®¡æ•°
    unique_bins, counts = np.unique(digitized, return_counts=True)
    # è®¾ç½®æœ€å°æ”¯æŒæ•°å·®å€¼
    median_imbalance, made_imbalance = calculate_made(counts)

    for t in X_copy_repair_indices:
        train_row_number = X_train.shape[0]
        ta = data_imbalance.iloc[t, feature]
        # æ‰¾åˆ° ta æ‰€åœ¨çš„é—´éš”
        ta_bin = np.digitize([ta], bins)[0]
        # æ‰¾åˆ° ta æ‰€åœ¨é—´éš”çš„è®¡æ•°
        ta_count = counts[unique_bins == ta_bin][0]
        lower_threshold = median_imbalance - 2 * made_imbalance
        upper_threshold = median_imbalance + 2 * made_imbalance
        if ta_count < lower_threshold or ta_count > upper_threshold:
            imbalanced_tuple_indices.add(t)

X_copy_repair_indices = list(imbalanced_tuple_indices)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

# section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆknnæ–¹æ³•ï¼‰
#  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

# subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_copy_inners, y_inners)

# é¢„æµ‹å¼‚å¸¸å€¼
y_pred = knn.predict(X_copy_repair)

# æ›¿æ¢å¼‚å¸¸å€¼
y[X_copy_repair_indices] = y_pred
y_train = y[train_indices]
y_test = y[test_indices]

# subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹

svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
svm_repair.fit(X_train_copy, y_train)
y_train_pred = svm_repair.predict(X_train_copy)
y_test_pred = svm_repair.predict(X_test_copy)

print("*" * 100)
# è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))

# æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))

# æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
      (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆäºŒï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œç‰¹å¾ä¿®å¤ï¼ˆç»Ÿè®¡æ–¹æ³•ä¿®å¤ï¼‰
# #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’ŒSVMåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰(ä¿®å¤æ•ˆæœç”±äºç›‘ç£/æ— ç›‘ç£åŸºå‡†)
#
# # subsection ç¡®å®šæœ‰å½±å“åŠ›ç‰¹å¾ä¸­çš„ç¦»ç¾¤å€¼å¹¶é‡‡ç”¨å‡å€¼ä¿®å¤
# for i in range(X_copy.shape[1]):
#     if i in top_k_indices:
#         column_data = X_copy[:, i]
#         mean = np.mean(column_data)
#         # å°†æ‰€æœ‰éœ€è¦ä¿®å¤çš„è¡Œå¯¹åº”çš„åˆ—ä½ç½®çš„å…ƒç´ æ›¿æ¢ä¸ºå‡å€¼
#         intersection = X_copy_repair_indices
#         X_copy[intersection, i] = mean
#
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹
#
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy, y_train)
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆä¸‰ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„å€ŸåŠ©knnè¿›è¡Œä¿®å¤ï¼Œchoice1 å°†å¼‚å¸¸å…ƒç»„ä¸­çš„å…ƒç´ ç›´æ¥è®¾ç½®ä¸ºnan(ä¿®å¤è¯¯å·®å¤ªå¤§ï¼Œä¿®å¤åå‡†ç¡®æ€§ä¸‹é™)
# #  choice2 ä»…å°†æœ‰å½±å“åŠ›ç‰¹å¾ä¸Šçš„å…ƒç´ è®¾ç½®ä¸ºnp.nan
#
# # # choice å°†å¼‚å¸¸å…ƒç»„ä¸­çš„æ‰€æœ‰å…ƒç´ è®¾ç½®ä¸ºnan
# # for i in range(X_copy.shape[1]):
# #     X_copy[X_copy_repair_indices, i] = np.nan
#
# # choice ä»…å°†å¼‚å¸¸å…ƒç»„ä¸­çš„æœ‰å½±å“åŠ›çš„å…ƒç´ è®¾ç½®ä¸ºnan
# for i in top_k_indices:
#     X_copy[X_copy_repair_indices, i] = np.nan
#
# # choice ä½¿ç”¨knnä¿®å¤æ‰€æœ‰è¢«æ ‡è®°ä¸ºnançš„å¼‚å¸¸ç‰¹å¾
# # åˆ›å»º KNN Imputer å¯¹è±¡
# knn_imputer = KNNImputer(n_neighbors=5)
#
# # ä½¿ç”¨ KNN ç®—æ³•å¡«è¡¥å¼‚å¸¸ç‰¹å¾
# X_copy = knn_imputer.fit_transform(X_copy)
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
#
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy, y_train)
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
#       len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
#       len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("å€ŸåŠ©knnä¿®å¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))
#       /(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆå››ï¼šå°†X_copyä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†éœ€è¦ä¿®å¤çš„å…ƒç»„ç›´æ¥åˆ é™¤ï¼Œåœ¨å»é™¤åçš„è®­ç»ƒé›†ä¸Šè®­ç»ƒsvmæ¨¡å‹
#
# set_X_copy_repair = set(X_copy_repair_indices)
#
# # è®¡ç®—å·®é›†ï¼Œå»é™¤è®­ç»ƒé›†ä¸­éœ€è¦ä¿®å¤çš„çš„å…ƒç´ 
# set_train_indices = set(train_indices)
# remaining_train_indices = list(set_train_indices - set_X_copy_repair)
# X_train_copy_repair = X_copy[remaining_train_indices]
# y_train_copy_repair = y[remaining_train_indices]
#
# # # choice è®¡ç®—å·®é›†ï¼Œå»é™¤æµ‹è¯•é›†ä¸­éœ€è¦ä¿®å¤çš„çš„å…ƒç´ 
# # set_test_indices = set(test_indices)
# # remaining_test_indices = list(set_test_indices - set_X_copy_repair)
# # X_test_copy_repair = X_copy[remaining_test_indices]
# # y_test_copy_repair = y[remaining_test_indices]
#
# # choice ä¸åˆ é™¤æµ‹è¯•é›†ä¸­çš„ç¦»ç¾¤æ ·æœ¬
# X_test_copy_repair = X_copy[test_indices]
# y_test_copy_repair = y[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹
#
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy_repair, y_train_copy_repair)
# y_train_pred = svm_repair.predict(X_train_copy_repair)
# y_test_pred = svm_repair.predict(X_test_copy_repair)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train_copy_repair != y_train_pred)[0]
# print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
#       len(wrong_classified_train_indices)/len(y_train_copy_repair))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test_copy_repair != y_test_pred)[0]
# print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
#       len(wrong_classified_test_indices)/len(y_test_copy_repair))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åˆ é™¤éœ€è¦ä¿®å¤çš„æ ·æœ¬åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))
#       /(len(y_train_copy_repair) + len(y_test_copy_repair)))

# # section æ–¹æ¡ˆäº”ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆéšæœºæ£®æ—æ¨¡å‹ï¼‰ï¼Œä¿®å¤æ ‡ç­¾å€¼
#
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import mean_absolute_error
#
# # subsection ä¿®å¤æ ‡ç­¾å€¼
# # è®­ç»ƒæ¨¡å‹
# model = RandomForestClassifier(n_estimators=100, random_state=42)
# model.fit(X_copy_inners, y_inners)  # ä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒæ¨¡å‹
#
# # é¢„æµ‹ç¦»ç¾¤æ ·æœ¬çš„æ ‡ç­¾
# y_repair_pred = model.predict(X_copy_repair)
#
# # è®¡ç®—é¢„æµ‹çš„å‡†ç¡®æ€§ï¼ˆå¯é€‰ï¼‰
# mae = mean_absolute_error(y_repair, y_repair_pred)
# print(f'Mean Absolute Error: {mae}')
#
# # subsection ä¿®å¤ç‰¹å¾å€¼
#
#
# X_copy[X_copy_repair_indices] = X_copy_repair
# y[X_copy_repair_indices] = y_repair_pred
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹
#
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy, y_train)
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))

# # section æ–¹æ¡ˆå…­ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹(éšæœºæ£®æ—æ¨¡å‹)ï¼Œä¿®å¤ç‰¹å¾å€¼ï¼ˆä¿®å¤æ—¶é—´å¾ˆä¹…ï¼Œæ…ç”¨ï¼‰
# #  ä¾æ¬¡å°†æœ‰å½±å“åŠ›çš„ç‰¹å¾ä½œä¸ºè¦ä¿®å¤çš„æ ‡ç­¾ï¼ˆè¿ç»­ç‰¹å¾å¯¹åº”å›å½’æ¨¡å‹ï¼Œåˆ†ç±»ç‰¹å¾å¯¹åº”åˆ†ç±»æ¨¡å‹ï¼‰ï¼Œä½¿ç”¨å…¶ä»–ç‰¹å¾å‚ä¸è®­ç»ƒ
#
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import mean_absolute_error
#
# # subsection ä¿®å¤ç‰¹å¾å€¼
#
# for i in top_k_indices:
#     y_train_inf = X_copy_inners[:, i]
#     columns_to_keep = np.delete(range(X_copy_inners.shape[1]), i)
#     X_train_remain = X_copy_inners[:, columns_to_keep]
#     if i in categorical_features:
#         model = RandomForestClassifier(n_estimators=100, random_state=42)
#     else:
#         model = RandomForestRegressor(n_estimators=100, random_state=42)
#     model.fit(X_train_remain, y_train_inf)  # ä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒæ¨¡å‹
#     X_test_repair = X_copy_repair[:, columns_to_keep]
#     y_test_pred = model.predict(X_test_repair)
#     X_copy_repair[:, i] = y_test_pred
#
# X_copy[X_copy_repair_indices] = X_copy_repair
# X_train_copy = X_copy[train_indices]
# X_test_copy = X_copy[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
#
# # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒSVMæ¨¡å‹
#
# svm_repair = svm.SVC(kernel='linear', C=1.0, probability=True)
# svm_repair.fit(X_train_copy, y_train)
# y_train_pred = svm_repair.predict(X_train_copy)
# y_test_pred = svm_repair.predict(X_test_copy)
#
# print("*" * 100)
# # è®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_train_indices)/len(y_train))
#
# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices)/len(y_test))
#
# # æ•´ä½“æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
#       (len(wrong_classified_train_indices) + len(wrong_classified_test_indices))/(len(y_train) + len(y_test)))