"""
(åŠ) ç›‘ç£ç¦»ç¾¤å€¼æ£€æµ‹ç®—æ³•åœ¨ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„æ•ˆæœ
"""
from collections import Counter

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from deepod.models.tabular import DeepSVDD
from deepod.models.tabular import RCA
from deepod.models import REPEN, SLAD, ICL, NeuTraL
from deepod.models.tabular import DevNet
from deepod.models import DeepSAD, RoSAS, PReNet
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†

# choice é€‰å–æ•°æ®é›†
file_path = "../datasets/real_outlier/Cardiotocography.csv"
# file_path = "../datasets/real_outlier/annthyroid.csv"
# file_path = "../datasets/real_outlier/optdigits.csv"
# file_path = "../datasets/real_outlier/PageBlocks.csv"
# file_path = "../datasets/real_outlier/pendigits.csv"
# file_path = "../datasets/real_outlier/satellite.csv"
# file_path = "../datasets/real_outlier/shuttle.csv"
# file_path = "../datasets/real_outlier/yeast.csv"

data = pd.read_csv(file_path)

# å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
if len(data) > 20000:
    data = data.sample(n=20000, random_state=42)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)

# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

# section æ•°æ®ç‰¹å¾ç¼©æ”¾ä»¥åŠæ·»åŠ å™ªå£°

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
feature_names = data.columns.values.tolist()
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

# SECTION Mğ‘œ (ğ‘¡, D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„(å¼±)ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨

# subsection ç¡®å®šå‚æ•°ä»¥åŠå°‘æ•°æ ‡ç­¾çš„ç´¢å¼•

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
hidden_dims = 20
epoch_steps = 20
batch_size = 256
lr = 1e-5

# section é€‰æ‹©ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨

# è®¾ç½®å¼±ç›‘ç£è®­ç»ƒæ ·æœ¬
# æ‰¾åˆ°æ‰€æœ‰æ ‡ç­¾ä¸º 1 çš„æ ·æœ¬ç´¢å¼•
semi_label_ratio = 0.1  # è®¾ç½®å·²çŸ¥çš„å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹
positive_indices = np.where(y_train == min_label)[0]
# éšæœºé€‰æ‹© 10% çš„æ­£æ ·æœ¬
n_positive_to_keep = int(len(positive_indices) * semi_label_ratio)
selected_positive_indices = np.random.choice(positive_indices, n_positive_to_keep, replace=False)
# åˆ›å»ºç”¨äºå¼‚å¸¸æ£€æµ‹å™¨çš„è®­ç»ƒæ ‡ç­¾
y_semi = np.zeros_like(y_train)  # é»˜è®¤å…¨ä¸º 0
y_semi[selected_positive_indices] = 1  # è®¾ç½®é€‰ä¸­çš„æ­£æ ·æœ¬ä¸º 1
# åˆ›å»ºç”¨äºå¼‚å¸¸æ£€æµ‹å™¨çš„æµ‹è¯•æ ‡ç­¾
y_semi_test = np.zeros_like(y_test)
test_positive_indices = np.where(y_test == min_label)[0]
y_semi_test[test_positive_indices] = 1

# choice DevNetå¼‚å¸¸æ£€æµ‹å™¨
# out_clf = DevNet(epochs=epochs, hidden_dims=hidden_dims, device=device,
#                           random_state=random_state)
# out_clf.fit(X_train, y_semi)
# out_clf_noise = DevNet(epochs=epochs, hidden_dims=hidden_dims, device=device,
#                           random_state=random_state)
# out_clf_noise.fit(X_train_copy, y_semi)

# choice DeepSADå¼‚å¸¸æ£€æµ‹å™¨
# out_clf = DeepSAD(epochs=epochs, hidden_dims=hidden_dims,
#                    device=device,
#                    random_state=random_state)
# out_clf.fit(X_train, y_semi)
# out_clf_noise = DeepSAD(epochs=epochs, hidden_dims=hidden_dims,
#                    device=device,
#                    random_state=random_state)
# out_clf_noise.fit(X_train_copy, y_semi)

# choice RoSASå¼‚å¸¸æ£€æµ‹å™¨
# out_clf = RoSAS(epochs=epochs, hidden_dims=hidden_dims, device=device, random_state=random_state)
# out_clf.fit(X_train, y_semi)
# out_clf_noise = RoSAS(epochs=epochs, hidden_dims=hidden_dims, device=device, random_state=random_state)
# out_clf_noise.fit(X_train_copy, y_semi)

# choice PReNeTå¼‚å¸¸æ£€æµ‹å™¨
out_clf = PReNet(epochs=epochs, device=device, random_state=random_state)
out_clf.fit(X_train, y_semi)
out_clf_noise = PReNet(epochs=epochs, device=device, random_state=random_state)
out_clf_noise.fit(X_train_copy, y_semi)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹ã€‚
#  ç»è¿‡æ£€éªŒï¼ŒåŠ å…¥é«˜æ–¯å™ªå£°ä¼šå½±å“å¼‚å¸¸å€¼åˆ¤åˆ«

# subsection ä»åŸå§‹è®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

print("*"*100)
train_scores = out_clf.decision_function(X_train)
train_pred_labels, train_confidence = out_clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index)/len(X_train))

# subsection ä»åŸå§‹æµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

test_scores = out_clf.decision_function(X_test)
test_pred_labels, test_confidence = out_clf.predict(X_test, return_confidence=True)
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
test_outliers_index = []
print("æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test))
for i in range(len(X_test)):
    if test_pred_labels[i] == 1:
        test_outliers_index.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index)
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index))
print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index)/len(X_test))

"""AccuracyæŒ‡æ ‡"""
print("*" * 100)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, test_pred_labels)))

# # æµ‹è¯•æ ·æœ¬ä¸­è¢«SVMæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
# wrong_classified_test_indices = np.where(y_test != test_pred_labels)[0]
# print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦v2ï¼š", 1-len(wrong_classified_test_indices)/len(y_test))

"""Precision/Recall/F1æŒ‡æ ‡"""
print("*" * 100)

# average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
# average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
# average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚

print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(precision_score(y_test, test_pred_labels, average='weighted')))
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(recall_score(y_test, test_pred_labels, average='weighted')))
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, test_pred_labels, average='weighted')))

"""ROC-AUCæŒ‡æ ‡"""
print("*" * 100)
y_test_prob = 1 / (1 + np.exp(-test_scores))
roc_auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

"""PR AUCæŒ‡æ ‡"""
print("*" * 100)
# è®¡ç®—é¢„æµ‹æ¦‚ç‡
y_scores = 1 / (1 + np.exp(-test_scores))
# è®¡ç®— Precision å’Œ Recall
precision, recall, _ = precision_recall_curve(y_test, y_scores)
# è®¡ç®— PR AUC
pr_auc = auc(recall, precision)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„PR AUC åˆ†æ•°:", pr_auc)

"""APæŒ‡æ ‡"""
print("*" * 100)
# è®¡ç®—é¢„æµ‹æ¦‚ç‡
y_scores = 1 / (1 + np.exp(-test_scores))
# è®¡ç®— Average Precision
ap_score = average_precision_score(y_test, y_scores)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„APåˆ†æ•°:", ap_score)

# section ä»åŠ å™ªæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºçš„å¼‚å¸¸å€¼

# subsection ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

train_scores_noise = out_clf_noise.decision_function(X_train_copy)
train_pred_labels_noise, train_confidence_noise = out_clf_noise.predict(X_train_copy, return_confidence=True)
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
train_outliers_index_noise = []
print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
for i in range(len(X_train_copy)):
    if train_pred_labels_noise[i] == 1:
        train_outliers_index_noise.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index_noise))
print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index_noise)/len(X_train_copy))

# subsection ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

test_scores_noise = out_clf_noise.decision_function(X_test_copy)
test_pred_labels_noise, test_confidence_noise = out_clf_noise.predict(X_test_copy, return_confidence=True)
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
test_outliers_index_noise = []
print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
for i in range(len(X_test_copy)):
    if test_pred_labels_noise[i] == 1:
        test_outliers_index_noise.append(i)
# è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index_noise))
print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index_noise)/len(X_test_copy))

"""AccuracyæŒ‡æ ‡"""
print("*" * 100)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, test_pred_labels_noise)))

"""Precision/Recall/F1æŒ‡æ ‡"""
print("*" * 100)

# average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
# average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
# average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚

print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(precision_score(y_test, test_pred_labels_noise, average='weighted')))
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(recall_score(y_test, test_pred_labels_noise, average='weighted')))
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, test_pred_labels_noise, average='weighted')))

"""ROC-AUCæŒ‡æ ‡"""
print("*" * 100)
y_test_prob = 1 / (1 + np.exp(-test_scores_noise))
roc_auc_test = roc_auc_score(y_test, y_test_prob, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

"""PR AUCæŒ‡æ ‡"""
print("*" * 100)
# è®¡ç®—é¢„æµ‹æ¦‚ç‡
y_scores = 1 / (1 + np.exp(-test_scores_noise))
# è®¡ç®— Precision å’Œ Recall
precision, recall, _ = precision_recall_curve(y_test, y_scores)
# è®¡ç®— PR AUC
pr_auc = auc(recall, precision)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„PR AUC åˆ†æ•°:", pr_auc)

"""APæŒ‡æ ‡"""
print("*" * 100)
# è®¡ç®—é¢„æµ‹æ¦‚ç‡
y_scores = 1 / (1 + np.exp(-test_scores_noise))
# è®¡ç®— Average Precision
ap_score = average_precision_score(y_test, y_scores)
print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„APåˆ†æ•°:", ap_score)

print("*"*100)
print("Recallï¼š", recall_score(y_test, test_pred_labels_noise, average='weighted'))
print("Accuracyï¼š", accuracy_score(y_test, test_pred_labels_noise))
print("ROC-AUCï¼š", roc_auc_test)
print("*"*100)