"""
ğ‘…(ğ‘¡) âˆ§ outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ) âˆ§ loss(M, D, ğ‘¡) > ğœ† âˆ§ Mğ‘ (ğ‘…, ğ´,M) â†’ ugly(ğ‘¡)
Rovasåœ¨ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹é¢†åŸŸæ£€æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸å€¼çš„æ•ˆæœ
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from deepod.models.tabular import GOAD
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import average_precision_score

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)

# section æ ‡å‡†æ•°æ®é›†å¤„ç†

# choice é€‰å–æ•°æ®é›†
# file_path = "../datasets/real_outlier/Cardiotocography.csv"
# file_path = "../datasets/real_outlier/annthyroid.csv"
file_path = "../datasets/real_outlier/optdigits.csv"
# file_path = "../datasets/real_outlier/PageBlocks.csv"
# file_path = "../datasets/real_outlier/pendigits.csv"
# file_path = "../datasets/real_outlier/satellite.csv"
# file_path = "../datasets/real_outlier/shuttle.csv"
# file_path = "../datasets/real_outlier/yeast.csv"

data = pd.read_csv(file_path)

# å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
if len(data) > 20000:
    data = data.sample(n=20000, random_state=42)

enc = LabelEncoder()
label_name = data.columns[-1]

# åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
data[label_name] = enc.fit_transform(data[label_name])

# æ£€æµ‹éæ•°å€¼åˆ—
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

# ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
encoders = {}
for column in non_numeric_columns:
    encoder = LabelEncoder()
    data[column] = encoder.fit_transform(data[column])
    encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 

X = data.values[:, :-1]
y = data.values[:, -1]

all_columns = data.columns.values.tolist()
feature_names = all_columns[:-1]
class_name = all_columns[-1]

# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y, return_counts=True)
# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")
# æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()

# è®¡ç®—æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

# æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
# è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

# section æ•°æ®ç‰¹å¾ç¼©æ”¾ä»¥åŠæ·»åŠ å™ªå£°

# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
# è®°å½•åŸå§‹ç´¢å¼•
original_indices = np.arange(len(X))
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
# åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
noise_level = 0.2
# è®¡ç®—å™ªå£°æ•°é‡
n_samples = X.shape[0]
n_noise = int(noise_level * n_samples)
# éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
noise_indices = np.random.choice(n_samples, n_noise, replace=False)
# æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
X_copy = np.copy(X)
X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
# ä»åŠ å™ªæ•°æ®ä¸­ç”ŸæˆåŠ å™ªè®­ç»ƒæ•°æ®å’ŒåŠ å™ªæµ‹è¯•æ•°æ®
X_train_copy = X_copy[train_indices]
X_test_copy = X_copy[test_indices]
feature_names = data.columns.values.tolist()
combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
# æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
data_copy = pd.DataFrame(combined_array, columns=feature_names)
# è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
train_noise = np.intersect1d(train_indices, noise_indices)
# æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
test_noise = np.intersect1d(test_indices, noise_indices)

# section æ‰¾åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾ Mğ‘ (ğ‘…, ğ´, M)
# choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)
import re

i = len(feature_names)
np.random.seed(1)
categorical_names = {}
rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=10, min_samples_leaf=4, random_state=42, class_weight='balanced')
rf_model.fit(X_train_copy, y_train)

for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data_copy.iloc[:, feature])
    data_copy.iloc[:, feature] = le.transform(data_copy.iloc[:, feature])
    categorical_names[feature] = le.classes_

explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_name,
                                                   categorical_features=categorical_features,
                                                   categorical_names=categorical_names, kernel_width=3)

predict_fn = lambda x: rf_model.predict_proba(x)
exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names)//2)
# è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
top_features = exp.as_list()
top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
top_k_indices = [feature_names.index(name) for name in top_feature_names]
print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

# section æ‰¾åˆ°loss(M, D, ğ‘¡) > ğœ†çš„å…ƒç»„

# # choice ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
# decision_values = rf_model.decision_function(X_copy)
# predicted_labels = np.argmax(decision_values, axis=1)
# # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
# num_samples = X_copy.shape[0]
# num_classes = rf_model.classes_.shape[0]
# hinge_losses = np.zeros((num_samples, num_classes))
# hinge_loss = np.zeros(num_samples)
# for i in range(num_samples):
#     correct_class = int(y[i])
#     for j in range(num_classes):
#         if j != correct_class:
#             loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
#             hinge_losses[i, j] = loss_j
#     hinge_loss[i] = np.max(hinge_losses[i])
#
# # åœ¨æ‰€æœ‰åŠ å™ªæ•°æ®Dä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•
# ugly_outlier_candidates = np.where(hinge_loss > 1)[0]
# # print("Dä¸­æŸå¤±å‡½æ•°é«˜äºæŸå¤±é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•ä¸ºï¼š", ugly_outlier_candidates)

# choice ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
from sklearn.preprocessing import OneHotEncoder

# è·å–æ¦‚ç‡å€¼
y_pred = rf_model.predict_proba(X_copy)[:, [1, 0]]
# åˆ›å»º OneHotEncoder å®ä¾‹
encoder = OneHotEncoder(sparse=False)
# æ‹Ÿåˆå¹¶è½¬æ¢ y_test
y_true = encoder.fit_transform(y.reshape(-1, 1))
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
loss_per_sample = -np.sum(y_true * np.log(y_pred + 1e-12), axis=1)
# è®¡ç®—æµ‹è¯•é›†å¹³å‡å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
average_loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-12), axis=1))
bad_samples = np.where(loss_per_sample < average_loss)[0]
good_samples = np.where(loss_per_sample >= average_loss)[0]
ugly_outlier_candidates = bad_samples

# section è°“è¯outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )çš„å®ç°ï¼Œæ‰¾åˆ°æ‰€æœ‰æœ‰å½±å“åŠ›çš„ç‰¹å¾ä¸‹çš„å¼‚å¸¸å…ƒç»„

outlier_feature_indices = {}
threshold = 0.01
for column_indice in top_k_indices:
    select_feature = feature_names[column_indice]
    select_column_data = data_copy[select_feature].values
    max_value = np.max(select_column_data)
    min_value = np.min(select_column_data)
    sorted_indices = np.argsort(select_column_data)
    sorted_data = select_column_data[sorted_indices]
    # æ‰¾åˆ°Aå±æ€§ä¸‹çš„æ‰€æœ‰å¼‚å¸¸å€¼
    outliers = []
    outliers_index = []
    # æ£€æŸ¥åˆ—è¡¨é¦–å°¾å…ƒç´ 
    if len(sorted_data) > 1:
        if (sorted_data[1] - sorted_data[0] >= threshold):
            outliers.append(sorted_data[0])
            outliers_index.append(sorted_indices[0])
        if (sorted_data[-1] - sorted_data[-2] >= threshold):
            outliers.append(sorted_data[-1])
            outliers_index.append(sorted_indices[-1])
    # æ£€æŸ¥ä¸­é—´å…ƒç´ 
    for i in range(1, len(sorted_data) - 1):
        current_value = sorted_data[i]
        left_value = sorted_data[i - 1]
        right_value = sorted_data[i + 1]
        if (current_value - left_value >= threshold) and (right_value - current_value >= threshold):
            outliers.append(current_value)
            outliers_index.append(sorted_indices[i])
    outliers_index_numpy = np.array(outliers_index)
    intersection = np.intersect1d(np.array(outliers_index), ugly_outlier_candidates)
    # print("æœ‰å½±å“åŠ›çš„ç‰¹å¾Aä¸‹åŒæ—¶æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡ .ğ´, ğœƒ )å’Œloss(M, D, ğ‘¡) > ğœ†çš„æ‰€æœ‰å¼‚å¸¸å€¼ç´¢å¼•ä¸ºï¼š", intersection)
    outlier_feature_indices[column_indice] = intersection
# print(outlier_feature_indices)

# section ç¡®å®šæ•°æ®ä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„

outlier_tuple_set = set()
for value in outlier_feature_indices.values():
    outlier_tuple_set.update(value)
X_copy_repair_indices = list(outlier_tuple_set)
X_copy_repair = X_copy[X_copy_repair_indices]
y_repair = y[X_copy_repair_indices]

# ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

# ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
# æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
X_copy_inners = X_copy[rows_to_keep]
y_inners = y[rows_to_keep]

# section æ±‡æ€»åŠ å™ªæ ·æœ¬ä¸­æ£€æµ‹åˆ°çš„outliers

y_pred = np.zeros_like(y)
y_pred[X_copy_repair_indices] = 1
y_train_pred = y_pred[train_indices]
y_test_pred = y_pred[test_indices]
# ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
unique_values, counts = np.unique(y_test_pred, return_counts=True)
# è¾“å‡ºç»“æœ
for value, count in zip(unique_values, counts):
    print(f"é¢„æµ‹çš„æµ‹è¯•é›†æ ‡ç­¾: {value}, é¢„æµ‹çš„æ ‡ç­¾æ•°é‡: {count}")
# æ‰¾åˆ°é¢„æµ‹çš„æœ€å°æ ‡ç­¾çš„æ•°é‡
min_count = counts.min()
total_count = counts.sum()
# è®¡ç®—é¢„æµ‹çš„æœ€å°‘æ ‡ç­¾çš„æ¯”ä¾‹
proportion = min_count / total_count
print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")

# section ä½¿ç”¨å„ç§è¯„ä»·æŒ‡æ ‡è¯„ä»·Rovasæ£€æµ‹åˆ°çš„outliers

"""AccuracyæŒ‡æ ‡"""
print("*" * 100)
print("Rovasåœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, y_test_pred)))

"""Precision/Recall/F1æŒ‡æ ‡"""
print("*" * 100)

# average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
# average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
# average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
# average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚

print("Rovasåœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(precision_score(y_test, y_test_pred, average='weighted')))
print("Rovasåœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(recall_score(y_test, y_test_pred, average='weighted')))
print("Rovasåœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, y_test_pred, average='weighted')))

"""ROC-AUCæŒ‡æ ‡"""
print("*" * 100)
roc_auc_test = roc_auc_score(y_test, y_test_pred, multi_class='ovr')  # ä¸€å¯¹å¤šæ–¹å¼
print("Rovasåœ¨åŠ å™ªæµ‹è¯•é›†ä¸­çš„ROC-AUCåˆ†æ•°ï¼š" + str(roc_auc_test))

# """PR AUCæŒ‡æ ‡"""
# print("*" * 100)
# # è®¡ç®—é¢„æµ‹æ¦‚ç‡
# y_scores = 1 / (1 + np.exp(-test_scores))
# # è®¡ç®— Precision å’Œ Recall
# precision, recall, _ = precision_recall_curve(y_test, y_scores)
# # è®¡ç®— PR AUC
# pr_auc = auc(recall, precision)
# print("åŠç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„PR AUC åˆ†æ•°:", pr_auc)
#
# """APæŒ‡æ ‡"""
# print("*" * 100)
# # è®¡ç®—é¢„æµ‹æ¦‚ç‡
# y_scores = 1 / (1 + np.exp(-test_scores))
# # è®¡ç®— Average Precision
# ap_score = average_precision_score(y_test, y_scores)
# print("æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸­çš„APåˆ†æ•°:", ap_score)