"""
ä½¿ç”¨SVMè®­ç»ƒé›†æ±‚è§£hingeæŸå¤±å‡½æ•°ï¼Œå› ä¸ºæµ‹è¯•é›†æ ‡ç­¾ä¸å¯è§
SVMå’Œå¼‚å¸¸æ£€æµ‹å™¨ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
å¼‚å¸¸æ£€æµ‹å™¨ç›´æ¥è¾“å‡ºåœ¨è®­ç»ƒé›†ä¸Šçš„å¼‚å¸¸å€¼
"""
# unsupervised methods
from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)  # æ˜¾ç¤ºæ‰€æœ‰åˆ—
pd.set_option('display.max_rows', None)     # æ˜¾ç¤ºæ‰€æœ‰è¡Œ
pd.set_option('display.width', None)        # ä¸é™åˆ¶æ˜¾ç¤ºå®½åº¦
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
# æŒ‡å®šè¦æ£€æµ‹çš„æ ‡ç­¾åˆ—ç±»åˆ«ä¸ºtarget_classæ—¶ï¼Œæ ·æœ¬ä¸­å‡ºç°çš„å¼‚å¸¸å€¼
target_class = 0

# SECTION kaggle datasetsä¸Šçš„æ•°æ®é¢„å¤„ç†
# SUBSECTION dry_beanæ•°æ®é›†
file_path = "../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:,0:16]
y = data.values[:,16]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# SUBSECTION obesityæ•°æ®é›†
# file_path = "../kaggle_datasets/Obesity_prediction/obesity_data.csv"
# label_col_name = "ObesityCategory"
# data = pd.read_csv(file_path)
# enc = LabelEncoder()
# data[label_col_name] = enc.fit_transform(data[label_col_name])
# categorical_features = [1]
# categorical_names = {}
# feature_names = data.columns[:-1].tolist()
# # å¯¹å­—ç¬¦ä¸²åˆ—è¿›è¡Œæ•°å€¼æ˜ å°„
# for feature in categorical_features:
#     le = LabelEncoder()
#     le.fit(data.iloc[:, feature])
#     data.iloc[:, feature] = le.transform(data.iloc[:, feature])
#     categorical_names[feature] = le.classes_
# data[feature_names] = data[feature_names].astype(float)
# num_col = data.shape[1]
# X = data.values[:, 0:num_col-1]
# y = data.values[:, num_col-1]
# # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
# X = StandardScaler().fit_transform(X)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# SUBSECTION wine-qualityæ•°æ®é›†
# file_path = "../UCI_datasets/wine+quality/winequality-white.csv"
# label_col_name = "quality"
# data = pd.read_csv(file_path, sep=';')
# enc = LabelEncoder()
# data[label_col_name] = enc.fit_transform(data[label_col_name])
# feature_names = data.columns[:-1].tolist()
# data[feature_names] = data[feature_names].astype(float)
# data[label_col_name] = data[label_col_name].astype(int)
# num_col = data.shape[1]
# X = data.values[:, 0:num_col-1]
# y = data.values[:, num_col-1]
# # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
# X = StandardScaler().fit_transform(X)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# SECTION Mğ‘œ (ğ‘¡, D),é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„å¼‚å¸¸æ£€æµ‹å™¨
# SUBSECTION  GOADå¼‚å¸¸æ£€æµ‹å™¨
clf = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf.fit(X_train, y=None)

# SUBSECTION DeepSADå¼‚å¸¸æ£€æµ‹å™¨
# clf = DeepSAD(epochs=1, hidden_dims=20,
#                    device=device,
#                    random_state=42)
# anom_id = np.where(y_train == 1)[0]
# known_anom_id = np.random.choice(anom_id, 10, replace=False)
# y_semi = np.zeros_like(y_train, dtype=int)
# y_semi[known_anom_id] = 1
# clf.fit(X_train, y_semi)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
train_scores = clf.decision_function(X_train)
train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))

# SECTION Mğ‘ (ğ‘…, ğ´,M)ï¼Œåœ¨è®­ç»ƒé›†ä¸­å¼•å…¥æœ‰å½±å“åŠ›çš„ç‰¹å¾
# # SUBSECTION å€ŸåŠ©æ–¹å·®åˆ¤åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾
# top_k_var = 6
# variances = np.var(X_train, axis=0)
# top_k_indices_var = np.argsort(variances)[-top_k_var:][::-1]
# print("æ–¹å·®æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_var, top_k_indices_var))
#
# # SUBSECTION å€ŸåŠ©pearsonç›¸å…³ç³»æ•°ç­›é€‰é‡è¦ç‰¹å¾(å’Œæ ‡ç­¾yæ±‚pearsonç›¸å…³ç³»æ•°æ„Ÿè§‰ä¸å¤ªè¡Œ)
# top_k_pearson = 6
# y_trans = y_train.reshape(-1)
# pearson_matrix = np.corrcoef(X_train.T, y_trans)
# correlations = np.abs(pearson_matrix[0, 1:])
# top_k_indices_pearson = np.argsort(correlations)[::-1][:top_k_pearson]
# print("ä¸æ ‡ç­¾yçš„Pearsonç›¸å…³ç³»æ•°ç»å¯¹å€¼æœ€å¤§çš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_pearson, top_k_indices_pearson))
#
# # SUBSECTION å€ŸåŠ©äº’ä¿¡æ¯ç­›é€‰é‡è¦ç‰¹å¾(å•ä¸ªç‰¹å¾å’Œæ ‡ç­¾ä¹‹é—´çš„äº’ä¿¡æ¯)
# top_k_mi = 6
# y_trans_mi = y_train.reshape(-1)
# mi = mutual_info_regression(X_train, y_trans_mi)
# top_k_indices = np.argsort(mi)[::-1][:top_k_mi]
# print("äº’ä¿¡æ¯æœ€å¤šçš„å‰{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_mi, top_k_indices))
#
# # SUBSECTION å€ŸåŠ©lassoç­›é€‰é‡è¦ç‰¹å¾(ç‰¹å¾çš„è”åˆåˆ†å¸ƒå’Œæ ‡ç­¾ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§)
# alpha = 0.0001
# top_k_lasso = 6
# lasso = Lasso(alpha, max_iter=10000, tol=0.01)
# lasso.fit(X_train, y_train)
# coef = lasso.coef_
# coef_abs = abs(coef)
# top_k_indices = np.argsort(coef_abs)[::-1][:top_k_lasso]
# print("lassoç»å¯¹å€¼æœ€å¤§çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_lasso, top_k_indices))
#
# # SUBSECTION sklearnåº“çš„SelectKBesté€‰æ‹©å™¨ï¼Œå€ŸåŠ©Fisheræ£€éªŒç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
# top_k_fisher = 6
# selector = SelectKBest(score_func=f_classif, k=top_k_fisher)
# y_trans_fisher = y_train.reshape(-1)
# X_new = selector.fit_transform(X_train, y_trans_fisher)
# # è·å–è¢«é€‰ä¸­çš„ç‰¹å¾çš„ç´¢å¼•
# selected_feature_indices = selector.get_support(indices=True)
# print("SelectKBesté€‰æ‹©å™¨å€ŸåŠ©Fisheræ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„{}ä¸ªç‰¹å¾çš„ç´¢å¼•ï¼š{}".format(top_k_fisher, selected_feature_indices))
#
# # SUBSECTION å€ŸåŠ©CARTå†³ç­–æ ‘ç­›é€‰æœ€æœ‰å½±å“åŠ›çš„kä¸ªç‰¹å¾
# top_k_cart = 6
# clf = DecisionTreeClassifier()
# clf.fit(X_train, y_train)
# # è·å–ç‰¹å¾é‡è¦æ€§å¾—åˆ†
# feature_importance = clf.feature_importances_
# # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
# sorted_indices = np.argsort(feature_importance)[::-1]
# # æ ¹æ®é‡è¦æ€§å¾—åˆ†é™åºæ’åº
# top_k_features = sorted_indices[:top_k_cart]
# print("CARTå†³ç­–æ ‘æ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_cart, top_k_features))
#
# # SUBSECTION sklearnåº“SelectFromModelé€‰æ‹©å™¨,å®ƒå¯ä»¥ä¸ä»»ä½•å…·æœ‰coef_ æˆ– feature_importances_ å±æ€§ï¼ˆå¦‚éšæœºæ£®æ—å’Œå†³ç­–æ ‘æ¨¡å‹ï¼‰çš„è¯„ä¼°å™¨ä¸€èµ·ä½¿ç”¨æ¥é€‰æ‹©ç‰¹å¾
# clf = RandomForestClassifier()
# clf.fit(X_train, y_train)
# # ä½¿ç”¨SelectFromModelæ¥é€‰æ‹©é‡è¦ç‰¹å¾
# sfm = SelectFromModel(clf, threshold='mean', prefit=True)
# X_selected = sfm.transform(X_train)
# # è·å–é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
# selected_idx = sfm.get_support(indices=True)
# # æ‰“å°é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
# print("SelectFromModelé€‰æ‹©å™¨é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:", selected_idx)
#
# # SUBSECTION å€ŸåŠ©wrapper(åŒ…è£…)æ–¹æ³•ç”Ÿæˆç‰¹å¾å­é›†
# model = LinearRegression()
# # åˆå§‹åŒ– RFE ç‰¹å¾é€‰æ‹©å™¨ï¼Œé€‰æ‹©è¦ä¿ç•™çš„ç‰¹å¾æ•°é‡
# rfe = RFE(model, n_features_to_select=6)
# # æ‹Ÿåˆ RFE ç‰¹å¾é€‰æ‹©å™¨
# rfe.fit(X_train, y_train)
# # è¾“å‡ºé€‰æ‹©çš„ç‰¹å¾
# indices = np.where(rfe.support_)[0]
# print("wrapper(åŒ…è£…)æ–¹æ³•é€‰æ‹©çš„ç‰¹å¾:", indices)
# # è¾“å‡ºç‰¹å¾æ’å
# print("wrapper(åŒ…è£…)æ–¹æ³•ä¸‹çš„ç‰¹å¾æ’å:", rfe.ranking_)
#
# # SUBSECTION åŸºäºXGBoostæ¨¡å‹ä»¥åŠXGBçš„ç‰¹å¾é‡è¦æ€§
# top_k_xgboost = 6
# gbtree = XGBClassifier(n_estimators=2000, max_depth=4, learning_rate=0.05, n_jobs=8)
# gbtree.set_params(eval_metric='auc', early_stopping_rounds=100)
# X_train_df = pd.DataFrame(X_train, columns=feature_names[:16])
# X_test_df = pd.DataFrame(X_test, columns=feature_names[:16])
# gbtree.fit(X_train, y_train,eval_set=[(X_test, y_test)], verbose=100)
# feature_importances = gbtree.feature_importances_
# top_k_indices = np.argpartition(-feature_importances, top_k_xgboost)[:top_k_xgboost]
# print("XGBoostæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_xgboost, top_k_indices))
# # SUBSECTION æ— æ¨¡å‹æ–¹æ³•ä¸­çš„Permutation Feature Importance-slearn
# result = permutation_importance(gbtree, X_train, y_train, n_repeats=10,random_state=42)
# feature_importance = result.importances_mean
# top_k_permutation = np.argpartition(-feature_importances, top_k_xgboost)[:top_k_xgboost]
# print("Permutation Feature Importance-slearnæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_xgboost, top_k_indices))
#
# # SUBSECTION åŸºäºPartial Dependency Plotsé¢„æµ‹å’Œæ ‡ç­¾ç›¸å…³çš„ç‰¹å¾é‡è¦æ€§(é¢„æµ‹å•ä¸ªç‰¹å¾å’Œæ ‡ç­¾çš„å…³è”ï¼Œæš‚æ—¶æ— ç”¨)
# # SUBSECTION æ·±å…¥åˆ°å•ä¸ªæ ·æœ¬ï¼Œåˆ†æç‰¹å¾å˜åŒ–å¯¹å•ä¸ªæ ·æœ¬çš„å½±å“ã€‚æ±‚è§£è¿‡ç¨‹å’Œä¸Šè¿°PDPç±»ä¼¼ï¼Œä½†æ˜¯ICEä¼šç»™å‡ºæ¯ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼(æš‚æ—¶æ— ç”¨ï¼Œå¯èƒ½ä¼šæœ‰ç”¨)
#
# # SUBSECTION LIME(Local Interpretable Model-Agnostic Explanation), é€šè¿‡æ‰°åŠ¨è¾“å…¥æ ·æœ¬ï¼ˆperturb the inputï¼‰ï¼Œæ¥å¯¹æ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œè§£é‡Šã€‚
# # æ‰€é€‰çš„è¿›è¡ŒLIMEåˆ†æçš„æ ·æœ¬
# i = 16
# top_k_LIME = 6
# np.random.seed(1)
# explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names,
#                                                    categorical_features=categorical_features,
#                                                    categorical_names=categorical_names, kernel_width=3)
# predict_fn = lambda x: gbtree.predict_proba(x)
# exp = explainer.explain_instance(X_train[i], predict_fn, num_features=6)
# # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
# # select_feature_list = []
# top_features = exp.as_list()
# # print("æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡:")
# # for feature, weight in top_features:
# #     select_feature_list.append(feature)
# #     print(f"{feature}: {weight}")
# important_features = []
# for feature_set in top_features:
#     feature_long = feature_set[0]
#     for feature in feature_names:
#         if set(feature).issubset(set(feature_long)):
#             important_features.append(feature)
#             break
# important_feature_indices = [feature_names.index(feature_name) for feature_name in important_features]
# # print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_LIME, select_feature_list))
# print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_LIME, important_feature_indices))
#
# # SUBSECTION å€ŸåŠ©SHAP(Shapley Additive explanation)å€¼å¾—åˆ°æœ‰å½±å“åŠ›çš„ç‰¹å¾(æŠ¥é”™ï¼ŒXGBoostå’Œshapè®­ç»ƒæ•°æ®ç»´åº¦ä¸é€‚é…ï¼ŒåŸå› å‚è§https://github.com/shap/shap/issues/580)
# # æ‰€é€‰çš„è¿›è¡ŒSHAPåˆ†æçš„æ ·æœ¬
# # i = 16
# # top_k_shap = 6
# # explainer = shap.TreeExplainer(gbtree)
# # shap_values = explainer.shap_values(X_test, y=y_test)
# # # å¯¹ä¸€ä¸ªæ ·æœ¬æ±‚shapå€¼ï¼Œå„ä¸ªç‰¹å¾å¯¹outputæ‰€å¸¦æ¥çš„å˜åŒ–
# # shap.force_plot(explainer.expected_value, shap_values[i,:], X_test[16], matplotlib=True)
# # # è®¡ç®—ç»å¯¹å¹³å‡ SHAP å€¼
# # mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)
# # # å°†ç‰¹å¾æŒ‰ç…§ç»å¯¹å¹³å‡ SHAP å€¼çš„å¤§å°è¿›è¡Œæ’åº
# # sorted_indices = np.argsort(mean_abs_shap_values)[::-1]  # é€†åºæ’åº
# # # è·å–å½±å“æœ€å¤§çš„ç‰¹å¾åç§°
# # top_features = X_test.columns[sorted_indices]
# # # è¾“å‡ºå½±å“æœ€å¤§çš„ç‰¹å¾
# # print("å€ŸåŠ©å¹³å‡shapå€¼åˆ¤åˆ«çš„å½±å“åŠ›è¾ƒå¤§çš„ç‰¹å¾ï¼š", top_features)
# # print("shapæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„çš„å‰{}ä¸ªå±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_shap, sorted_indices[top_k_shap]))

# section outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)èšåˆå‡½æ•°ï¼Œå¦‚æœå¯¹äºDä¸­æ‰€æœ‰å…ƒç»„sï¼Œt.Aä¸s.Aè‡³å°‘ç›¸å·®ä¸€ä¸ªå› å­ğœƒï¼Œåˆ™è°“è¯è¿”å›trueï¼Œå¦åˆ™è¿”å›false
# subsection ä»å­—é¢æ„æ€å®ç°outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)
# threshold = 0.1
# col_indices = 3
# row_indices = 10
# select_feature = feature_names[col_indices]
# # å°†col_indiceså¯¹åº”ç‰¹å¾ä¸‹æ‰€æœ‰å€¼ä¸¤ä¸¤ç›¸å‡ä¿å­˜åœ¨diff_featureä¸­
# diff_feature = abs(data[select_feature].values.reshape(-1, 1) - data[select_feature].values)
# # å°†diff_featureä¸è‡ªèº«æ¯”è¾ƒçš„å…ƒç´ å€¼è®¾ä¸ºé˜ˆå€¼
# diff_feature[np.diag_indices(len(data))] = threshold
# # æ‰¾åˆ°ç¬¦åˆä¸æ‰€æœ‰å…¶ä»–å…ƒç»„å€¼ä¸¤ä¸¤æ¯”è¾ƒå‡å¤§äºç­‰äºé˜ˆå€¼çš„å…ƒç»„ç´¢å¼•
# satisfying_indices = np.where((diff_feature >= threshold).all(axis=1))[0]
# print("å­—é¢å®ç°æ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", satisfying_indices)

# subsection é‡‡ç”¨é—´éš”æ–¹æ³•ï¼Œä½¿ç”¨Modified Z-scoreæ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
# def modified_z_score(points, thresh=3.5):
#     if len(points.shape) == 1:
#         points = points[:,None]
#     median = np.median(points, axis=0)
#     diff = np.sum((points - median)**2, axis=-1)
#     diff = np.sqrt(diff)
#     med_abs_deviation = np.median(diff)
#     modified_z_score = 0.6745 * diff / med_abs_deviation
#     return modified_z_score > thresh
# col_indices = 3
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# value_labels = modified_z_score(feature_values)
# true_indices = np.where(value_labels)[0]
# print("modified_z_scoreæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", true_indices)
# print("modified_z_scoreæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(true_indices))

# subsection é‡‡ç”¨é—´éš”æ–¹æ³•ï¼Œä½¿ç”¨2MADeæ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
# def calculate_made(data):
#     median = np.median(data)  # è®¡ç®—ä¸­ä½æ•°
#     abs_deviation = np.abs(data - median)  # è®¡ç®—æ¯ä¸ªæ•°æ®ç‚¹ä¸ä¸­ä½æ•°çš„ç»å¯¹è¯¯å·®
#     mad = np.median(abs_deviation)  # è®¡ç®—ç»å¯¹è¯¯å·®å‡å€¼
#     made = 1.843 * mad
#     return median, made
#
# col_indices = 3
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# median, made = calculate_made(feature_values)
# lower_threshold = median - 2 * made
# upper_threshold = median + 2 * made
# made_indices = np.where((feature_values > upper_threshold) | (feature_values < lower_threshold))[0]
# print("2MADeæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", made_indices)
# print("2MADeæ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(made_indices))

# subsection é‡‡ç”¨1.5IQRä¸‹çš„ç®±çº¿å›¾æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
# def calculate_iqr(data):
#     sorted_data = np.sort(data)  # å°†æ•°æ®é›†æŒ‰å‡åºæ’åˆ—
#     q1 = np.percentile(sorted_data, 25)  # è®¡ç®—ä¸‹å››åˆ†ä½æ•°
#     q3 = np.percentile(sorted_data, 75)  # è®¡ç®—ä¸Šå››åˆ†ä½æ•°
#     iqr = q3 - q1
#     return q1, q3, iqr
# col_indices = 3
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# q1, q3, iqr = calculate_iqr(feature_values)
# lower_bound = q1 - 1.5 * iqr
# upper_bound = q3 + 1.5 * iqr
# box_plot_indices = np.where((feature_values > upper_bound) | (feature_values < lower_bound))[0]
# print("ç®±çº¿å›¾æ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", box_plot_indices)
# print("ç®±çº¿å›¾æ–¹æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(box_plot_indices))

# subsection é‡‡ç”¨æ ‡å‡†å·®æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
# col_indices = 3
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# mean = feature_values.mean()
# std = feature_values.std()
# upper_bound = mean + 3 * std
# lower_bound = mean - 3 * std
# std_indices = np.where((feature_values > upper_bound) | (feature_values < lower_bound))[0]
# print("æ ‡å‡†å·®æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„ç´¢å¼•ä¸ºï¼š", std_indices)
# print("æ ‡å‡†å·®æ³•æ‰¾åˆ°çš„ç¬¦åˆæ¡ä»¶çš„å…ƒç»„æ•°ï¼š", len(std_indices))

# subsection é‡‡ç”¨distæ‹Ÿåˆå•åˆ—æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•ï¼Œæ•°æ®ä¸­å¯èƒ½å­˜åœ¨å¤šä¸ªåˆ†å¸ƒï¼Œå¯ä»¥è€ƒè™‘ç”¨åˆ†æ®µå‡½æ•°å»ºæ¨¡
# col_indices = 3
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# dist = distfit(todf=True)
# dist.fit_transform(feature_values)
# # è·å–æœ€ä½³åˆ†å¸ƒ
# best_distribution_name = dist.model['name']
# best_distribution_params = dist.model['params']
# # æ ¹æ®æœ€ä½³åˆ†å¸ƒåç§°å’Œå‚æ•°æ„å»ºå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡
# best_distribution = getattr(stats, best_distribution_name)(*best_distribution_params)
# # è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡å¯†åº¦
# densities = best_distribution.pdf(feature_values)
# # å®šä¹‰ä¸€ä¸ªé˜ˆå€¼ï¼Œä¾‹å¦‚ä½äºè¿™ä¸ªé˜ˆå€¼çš„ç‚¹è¢«è§†ä¸ºå¼‚å¸¸ç‚¹
# threshold = 0.01
# # æ‰¾åˆ°å¼‚å¸¸ç‚¹
# outliers_indices = np.where(densities < threshold)[0]
# dist.plot()
# plt.show()
# print("ä½äºdistæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹ç´¢å¼•:", outliers_indices)
# print("ä½äºdistæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹æ•°é‡:", len(outliers_indices))

# subsection é‡‡ç”¨filter fittingæ‹Ÿåˆå•åˆ—æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•
# col_indices = 5
# select_feature = feature_names[col_indices]
# feature_values = data[select_feature].values
# f = Fitter(feature_values, distributions=['norm', 't', 'laplace'])
# f.fit()
# # è®¡ç®—æœ€ä½³åˆ†å¸ƒå’Œæœ€ä½³å‚æ•°
# pattern = r'\[(.*?)\]'
# best_dist_name_key = f.get_best(method='sumsquare_error').keys()
# best_dist_name = key_string = ', '.join(str(key) for key in best_dist_name_key)
# best_params = None
# for dist_name, params in f.fitted_param.items():
#     if dist_name == best_dist_name:
#         best_params = params
#         break
# if best_params is None:
#     raise ValueError(f"No parameters found for the best distribution '{best_dist_name}'")
# # æ„å»ºå¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡
# best_dist = getattr(stats, best_dist_name)(*best_params)
# # è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡å¯†åº¦
# densities = best_dist.pdf(feature_values)
# # è®¾å®šé˜ˆå€¼æ‰¾å‡ºæ¦‚ç‡å¯†åº¦ä½äºé˜ˆå€¼çš„æ ·æœ¬ç‚¹ä½œä¸ºå¼‚å¸¸ç‚¹
# threshold = 0.01  # ä¸¾ä¾‹è®¾å®šé˜ˆå€¼
# outliers_indices = np.where(densities < threshold)[0]
# print("ä½äºfilter fittingæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹ç´¢å¼•:", outliers_indices)

# print("ä½äºfilter fittingæ‹Ÿåˆçš„æ•°æ®åˆ†å¸ƒå¤–çš„å¼‚å¸¸ç‚¹æ•°é‡:", len(outliers_indices))

# subsection é‡‡ç”¨åˆ†æ®µæ‹Ÿåˆæ–¹æ³•æ‹Ÿåˆå•åˆ—æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•å¯»æ‰¾æ»¡è¶³outlier(ğ·, ğ‘…, ğ‘¡.ğ´, ğœƒ)çš„å…ƒç»„ç´¢å¼•(éœ€è¦æŒ‡å®šæ‹Ÿåˆå‡½æ•°çš„å½¢å¼)

# section imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œå¦‚æœDä¸­æŒ‰t.Aåˆ†ç»„çš„å…ƒç»„æ•°é‡æ¯”å…¶ä»–ç»„çš„è®¡æ•°å°Aå€¼(è‡³å°‘å°ä¸€ä¸ªå› å­ğ›¿)ï¼Œåˆ™è¿”å›trueï¼Œå¦åˆ™è¿”å›false
# subsection ä»å­—é¢æ„æ€çš„å…·ä½“å€¼å‡ºç°é¢‘ç‡åˆ¤æ–­æ˜¯å¦ä¸å¹³è¡¡,å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼ŒåŸºç¡€ç‰ˆæœ¬
# import balanace.imbalanced as im
# col_indices = 16
# feature = feature_names[col_indices]
# data_copy = pd.read_excel(file_path).astype(str)
# imbalanced = im.Imbalanced(data_copy, feature)
# ta = "SEKER"
# delta = 2
# print(imbalanced.enum_check(ta, delta))

# subsection ä»å­—é¢æ„æ€å®ç°imbalanced(ğ·, ğ‘…, ğ‘¡.ğ´, ğ›¿)ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—è¿›è¡Œæ ‡å‡†åŒ–å’Œåˆ†ç®±ï¼Œåˆ¤æ–­åˆ†ç®±ä¸­çš„å…ƒç´ æ•°æ˜¯å¦è¾¾åˆ°ä¸å¹³è¡¡
# from sklearn.preprocessing import MinMaxScaler
# # è®¾ç½®åˆ†ç®±ä¸­å…ƒç»„æ•°ç›¸å·®é˜ˆå€¼
# delta = 0.01
# # è®¾ç½®åˆ†ç»„çš„é—´éš”
# interval = 0.01
# # åˆå§‹åŒ–MinMaxScaler
# scaler = MinMaxScaler()
# col_indices = 3
# select_feature = feature_names[col_indices]
# data_copy = pd.read_excel(file_path)
# data_copy[data.columns] = scaler.fit_transform(data[data.columns])
# # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
# bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
# # ç»Ÿè®¡æ¯åˆ—æ•°æ®å æ®äº†å¤šå°‘ä¸ªé—´éš”
# # for column in data_copy.columns:
# #     digitized = np.digitize(data_copy[column], bins)
# #     unique_bins, counts = np.unique(digitized, return_counts=True)
# #     print(f"åˆ— '{column}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
# #     # ç»Ÿè®¡åŒ…å«æœ€å¤§å…ƒç´ æ•°å’Œæœ€å°å…ƒç´ æ•°çš„å·®å€¼
# #     max_elements = np.max(counts)
# #     min_elements = np.min(counts)
# #     difference = max_elements - min_elements
# #     print(f"åˆ— '{column}' binsä¸­åŒ…å«æœ€å¤šçš„å…ƒç»„æ•°å’Œæœ€å°‘çš„å…ƒç»„æ•°ç›¸å·®äº† {difference}")
# digitized = np.digitize(data_copy[select_feature], bins)
# unique_bins, counts = np.unique(digitized, return_counts=True)
# print(f"åˆ— '{select_feature}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
# # ç»Ÿè®¡åŒ…å«æœ€å¤§å…ƒç´ æ•°å’Œæœ€å°å…ƒç´ æ•°çš„å·®å€¼
# max_elements = np.max(counts)
# min_elements = np.min(counts)
# difference = max_elements - min_elements
# print(f"åˆ— '{select_feature}' binsä¸­åŒ…å«æœ€å¤šçš„å…ƒç»„æ•°å’Œæœ€å°‘çš„å…ƒç»„æ•°ç›¸å·®äº† {difference}")
# print("æ‰€é€‰åˆ—æ˜¯å¦ä¸å¹³è¡¡ï¼š", difference/data_copy.shape[0] >= delta)

# SECTION SDomain(ğ·, ğ‘…, ğ´, ğœ)ï¼Œå¦‚æœDçš„Aå±æ€§çš„ä¸åŒå€¼æ•°é‡å°äºç•Œé™ğœï¼Œåˆ™è¿”å›true
# subsection ä»å­—é¢æ„æ€Aåˆ—çš„ä¸åŒå€¼æ•°é‡æ˜¯å¦æ˜æ˜¾å°äºå…¶ä»–åˆ—
# import balanace.sdomain as sd
# col_indices = 16
# # è®¾ç½®æ¯åˆ—ä¸åŒå…ƒç´ æ•°é‡è¦è¾¾åˆ°çš„æœ€å°é˜ˆå€¼
# sigma = 2
# feature = feature_names[col_indices]
# data_copy = pd.read_excel(file_path)
# imbalanced = sd.SDomian(data_copy, feature)
# print("æ‰€é€‰åˆ—çš„æ´»åŠ¨åŸŸæ˜¯å¦å¾ˆå°ï¼š", imbalanced.enum_check(sigma))

# subsection ä»å­—é¢æ„æ€Aåˆ—çš„ä¸åŒå€¼æ•°é‡æ˜¯å¦æ˜æ˜¾å°äºå…¶ä»–åˆ—ï¼Œæ”¹è¿›ç‰ˆæœ¬ï¼Œå¯¹åˆ—çš„å€¼è¿›è¡Œæ ‡å‡†åŒ–ååˆ†ç®±åˆ¤æ–­åˆ†ç®±çš„æ•°é‡
# from sklearn.preprocessing import MinMaxScaler
# # è®¾ç½®åˆ†ç»„çš„é—´éš”
# interval = 0.01
# # åˆå§‹åŒ–MinMaxScaler
# scaler = MinMaxScaler()
# col_indices = 3
# select_feature = feature_names[col_indices]
# data_copy = pd.read_excel(file_path)
# data_copy[data.columns] = scaler.fit_transform(data[data.columns])
# # å¯¹æ¯åˆ—æ•°æ®è¿›è¡Œåˆ†ç»„
# bins = np.arange(0, 1.01, interval)  # ç”Ÿæˆ0-1ä¹‹é—´100ä¸ªé—´éš”çš„æ•°ç»„
# # ç»Ÿè®¡æ¯åˆ—æ•°æ®å æ®äº†å¤šå°‘ä¸ªé—´éš”
# total_bins = 0
# selected_bins = 0
# for column in data_copy.columns:
#     digitized = np.digitize(data_copy[column], bins)
#     unique_bins, counts = np.unique(digitized, return_counts=True)
#     print(f"åˆ— '{column}' å æ®äº† {len(unique_bins)} ä¸ªé—´éš”")
#     total_bins += len(unique_bins)
#     if column == select_feature:
#         selected_bins = len(unique_bins)
# mean_bins = total_bins / len(data_copy.columns)
# print("æ‰€é€‰ç‰¹å¾æ˜¯å¦æ´»åŠ¨åŸŸå¾ˆå°ï¼š", selected_bins < mean_bins)

# SECTION SVMæ¨¡å‹è®­ç»ƒå’Œåˆ†ç±»å‡†ç¡®åº¦
svm_model = svm.SVC()
# svm_model = svm.SVC(C=10)  # é»˜è®¤ä½¿ç”¨ RBF æ ¸å‡½æ•°ï¼ˆå¾„å‘åŸºå‡½æ•°ï¼‰ï¼Œå³é«˜æ–¯æ ¸å‡½æ•°
# svm_model = svm.SVC(C=2, gamma=0.1)
# svm_model = svm.SVC(kernel='linear')  # çº¿æ€§æ ¸å‡½æ•°
# svm_model = svm.SVC(kernel='poly', degree=3, coef0=1)  # å¤šé¡¹å¼æ ¸å‡½æ•°
# svm_model = svm.SVC(kernel='sigmoid', gamma=0.1, coef0=0.5)  # sigmoidæ ¸å‡½æ•°

svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]
X_train_outliers = X_train[train_outliers_index]
y_train_outliers_numpy = y_train[train_outliers_index]
y_train_outliers = pd.Series(y_train_outliers_numpy)
if not y_train_outliers.empty:
    print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼çš„SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_outliers, svm_model.predict(X_train_outliers))))

# SECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
print("åŸºäºhingeæŸå¤±çš„è®­ç»ƒé›†ä¸Šæ ‡ç­¾çš„SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, predicted_labels)))
# è®¡ç®—è®­ç»ƒæ ·æœ¬çš„å¹³å‡æŸå¤±
train_losses = hinge_loss(y_train, decision_values, labels=np.unique(y_train))
print("æ•´ä¸ªè®­ç»ƒé›†ä¸‹çš„å¹³å‡hingeæŸå¤±ï¼š", train_losses)
# è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šå‡è®¾é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
anomalies = np.where(hinge_loss > 1)[0]
soft_anomalies = np.where((hinge_loss > 0) & (hinge_loss <= 1))[0]
correct_class = np.where(hinge_loss == 0)[0]
# è¾“å‡ºè®­ç»ƒé›†ä¸­outliersä¸­å…·æœ‰è¾ƒé«˜hingeæŸå¤±çš„æ ·æœ¬ç´¢å¼•
# è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¯¼è‡´SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
inter_anomalies = list(set(train_outliers_index) & set(anomalies))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä½†æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸å‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®ä¸è¶³é˜ˆå€¼1
inter_soft_anomalies = list(set(train_outliers_index) & set(soft_anomalies))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä¸”æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸å‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®è¶…è¿‡é˜ˆå€¼1
inter_correct_class = list(set(train_outliers_index) & set(correct_class))

print("*" * 100)
print("è®­ç»ƒé›†ä¸­SVMå…·æœ‰è¾ƒé«˜hingeæŸå¤±å‡½æ•°çš„æ ·æœ¬æ•°é‡ï¼š", len(anomalies))
# print("è®­ç»ƒé›†ä¸­SVMçš„hingeæŸå¤±å‡½æ•°é«˜äº1çš„æ ·æœ¬ç´¢å¼•ï¼š", anomalies)
print("è®­ç»ƒé›†ä¸­SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ•°é‡ï¼š", len(wrong_classified_indices))
# print("è®­ç»ƒé›†ä¸­SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬ç´¢å¼•ï¼š", wrong_classified_indices)
intersection = np.intersect1d(anomalies, wrong_classified_indices)
diff_elements = np.setdiff1d(wrong_classified_indices, intersection)
print("åˆ†ç±»é”™è¯¯çš„æ ·æœ¬ä¸­æœªè¢«hingeé˜ˆå€¼å¤§äº1è¯†åˆ«çš„æ ·æœ¬ç´¢å¼•ï¼š", diff_elements)
print("hingeæŸå¤±å¤§äº1çš„æ ·æœ¬å’Œåˆ†ç±»é”™è¯¯çš„æ ·æœ¬çš„äº¤é›†æ•°é‡ï¼š", len(intersection))
print("è¯¥äº¤é›†æ‰€å åˆ†ç±»é”™è¯¯çš„æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(intersection)/len(wrong_classified_indices))
print("*" * 100)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°é«˜äºé˜ˆå€¼1çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_anomalies)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°åœ¨0å’Œé˜ˆå€¼1ä¹‹é—´çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_soft_anomalies)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°ä¸º0çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_correct_class)

# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¤„ç†åé‡æ–°è®­ç»ƒSVMæ¨¡å‹

# SECTION åŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
print("*" * 100)
print("åŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­hingeæŸå¤±å‡½æ•°é«˜äº1ï¼Œä¸”è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
# ç”Ÿæˆå¸ƒå°”ç´¢å¼•ï¼Œä¸ºè¦åˆ é™¤çš„è¡Œåˆ›å»ºå¸ƒå°”å€¼æ•°ç»„
mask = np.ones(len(X_train), dtype=bool)
mask[inter_anomalies] = False
# ä½¿ç”¨å¸ƒå°”ç´¢å¼•åˆ é™¤é‚£äº›æ—¢è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œåˆä½¿å¾—hingeæŸå¤±é«˜äº1çš„æ ·æœ¬
X_train_split = X_train[mask]
y_train_split = y_train[mask]
# é‡æ–°è®­ç»ƒSVMæ¨¡å‹
svm_model_split = svm.SVC()
svm_model_split.fit(X_train_split, y_train_split)
print("*" * 100)
print("å»é™¤åŒæ—¶æ»¡è¶³å¼‚å¸¸å’ŒæŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_split, svm_model_split.predict(X_train_split))))
print("å»é™¤åŒæ—¶æ»¡è¶³å¼‚å¸¸å’ŒæŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_split.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­hingeæŸå¤±å‡½æ•°é«˜äº1çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
mask_h = np.ones(len(X_train), dtype=bool)
mask_h[anomalies] = False
X_train_h = X_train[mask_h]
y_train_h = y_train[mask_h]
svm_model_h = svm.SVC()
svm_model_h.fit(X_train_h, y_train_h)
print("*" * 100)
print("å»é™¤æŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_h, svm_model_h.predict(X_train_h))))
print("å»é™¤æŸå¤±é«˜äºé˜ˆå€¼çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_h.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
mask_o = np.ones(len(X_train), dtype=bool)
mask_o[train_outliers_index] = False
X_train_o = X_train[mask_o]
y_train_o = y_train[mask_o]
svm_model_o = svm.SVC()
svm_model_o.fit(X_train_o, y_train_o)
print("*" * 100)
print("å»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_o, svm_model_o.predict(X_train_o))))
print("å»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_o.predict(X_test))))
print("*" * 100)