"""
ä½¿ç”¨SVMè®­ç»ƒé›†æ±‚è§£hingeæŸå¤±å‡½æ•°ï¼Œå› ä¸ºæµ‹è¯•é›†æ ‡ç­¾ä¸å¯è§
SVMå’Œå¼‚å¸¸æ£€æµ‹å™¨ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†
å¼‚å¸¸æ£€æµ‹å™¨ç›´æŽ¥è¾“å‡ºåœ¨è®­ç»ƒé›†ä¸Šçš„å¼‚å¸¸å€¼
"""
# unsupervised methods
from deepod.models import REPEN, SLAD, ICL, NeuTraL, DeepSAD
from deepod.models.tabular import GOAD
from deepod.models.tabular import RCA
from deepod.models.tabular import DeepSVDD
import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import matplotlib
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.metrics import hinge_loss
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier, plot_importance
from sklearn.inspection import permutation_importance
from lime.lime_tabular import LimeTabularExplainer
import shap
from distfit import distfit
from fitter import Fitter
import scipy.stats as stats
import re

pd.set_option('display.max_columns', None)  # æ˜¾ç¤ºæ‰€æœ‰åˆ—
pd.set_option('display.max_rows', None)     # æ˜¾ç¤ºæ‰€æœ‰è¡Œ
pd.set_option('display.width', None)        # ä¸é™åˆ¶æ˜¾ç¤ºå®½åº¦
np.set_printoptions(threshold=np.inf)

def one_hot_encode(y, num_classes):
    y = y.astype(int)
    return np.eye(num_classes)[y]

epochs = 1
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_trans = 64
random_state = 42
# æŒ‡å®šè¦æ£€æµ‹çš„æ ‡ç­¾åˆ—ç±»åˆ«ä¸ºtarget_classæ—¶ï¼Œæ ·æœ¬ä¸­å‡ºçŽ°çš„å¼‚å¸¸å€¼
target_class = 0

# SECTION kaggle datasetsä¸Šçš„æ•°æ®é¢„å¤„ç†
# SUBSECTION dry_beanæ•°æ®é›†
file_path = "../UCI_datasets/dry+bean+dataset/DryBeanDataset/Dry_Bean_Dataset.xlsx"
data = pd.read_excel(file_path)
enc = LabelEncoder()
data['Class'] = enc.fit_transform(data['Class'])
X = data.values[:,0:16]
y = data.values[:,16]
# å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
X = StandardScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)
class_names = enc.classes_
feature_names = data.columns.values.tolist()
categorical_features = [0, 6]
categorical_names = {}
for feature in categorical_features:
    le = LabelEncoder()
    le.fit(data.iloc[:, feature])
    data.iloc[:, feature] = le.transform(data.iloc[:, feature])
    categorical_names[feature] = le.classes_

# SECTION Mð‘œ (ð‘¡, D),é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„å¼‚å¸¸æ£€æµ‹å™¨
# SUBSECTION  GOADå¼‚å¸¸æ£€æµ‹å™¨
clf = GOAD(epochs=epochs, device=device, n_trans=n_trans)
clf.fit(X_train, y=None)

# SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
train_scores = clf.decision_function(X_train)
train_pred_labels, train_confidence = clf.predict(X_train, return_confidence=True)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", clf.threshold_)
train_outliers_index = []
print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
for i in range(len(X_train)):
    if train_pred_labels[i] == 1:
        train_outliers_index.append(i)
print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))

# SECTION SVMæ¨¡åž‹è®­ç»ƒå’Œåˆ†ç±»å‡†ç¡®åº¦
svm_model = svm.SVC()
svm_model.fit(X_train, y_train)
train_label_pred = svm_model.predict(X_train)
wrong_classified_indices = np.where(y_train != svm_model.predict(X_train))[0]
X_train_outliers = X_train[train_outliers_index]
y_train_outliers_numpy = y_train[train_outliers_index]
y_train_outliers = pd.Series(y_train_outliers_numpy)
print("*" * 100)
print("åŽŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŽŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
if not y_train_outliers.empty:
    print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼çš„SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_outliers, svm_model.predict(X_train_outliers))))
print("*" * 100)

# SECTION ä½¿ç”¨sklearnåº“ä¸­çš„hingeæŸå¤±å‡½æ•°
decision_values = svm_model.decision_function(X_train)
predicted_labels = np.argmax(decision_values, axis=1)
print("åŸºäºŽhingeæŸå¤±çš„è®­ç»ƒé›†ä¸Šæ ‡ç­¾çš„SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, predicted_labels)))
# è®¡ç®—è®­ç»ƒæ ·æœ¬çš„å¹³å‡æŸå¤±
train_losses = hinge_loss(y_train, decision_values, labels=np.unique(y_train))
print("æ•´ä¸ªè®­ç»ƒé›†ä¸‹çš„å¹³å‡hingeæŸå¤±ï¼š", train_losses)

# section è®¡ç®—è®­ç»ƒæ ·æœ¬çš„è¿­ä»£æŸå¤±
# è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æ¯è½®è¿­ä»£çš„æŸå¤±å€¼
losses_per_iteration = []
# å¼€å§‹è®­ç»ƒè¿­ä»£
interval = 30
# outlier_out_thresholdä¸ºè¢«å¼‚å¸¸æ£€æµ‹å™¨åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œä¸”å…¶hingeæŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬
outlier_out_threshold = [1794, 1669, 775, 3472, 1044, 535, 5400, 4637, 1821, 42, 6570, 4784, 2745, 4155, 4162, 5575, 6216, 5196, 6745, 3036, 6238, 1118, 5854, 999, 4968, 1773, 4847, 370]
for iteration in range(10):  # å‡è®¾è¿­ä»£10è½®
    svm_iteration = svm.SVC(max_iter=interval*(iteration+1))
    # åœ¨æ¯è½®è¿­ä»£ä¸­è®­ç»ƒ SVM åˆ†ç±»å™¨
    svm_iteration.fit(X_train, y_train)
    # èŽ·å–å†³ç­–å‡½æ•°å€¼
    decision_values = svm_iteration.decision_function(X_train)
    num_classes_iteration = svm_iteration.classes_.shape[0]
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ hinge æŸå¤±
    mean_hinge_losses = hinge_loss(y_train, decision_values)
    # å°†æ¯è½®è¿­ä»£çš„æŸå¤±å€¼è®°å½•ä¸‹æ¥
    losses_per_iteration.append(mean_hinge_losses)
    num_samples = X_train.shape[0]
    num_classes = svm_iteration.classes_.shape[0]
    hinge_losses = np.zeros((num_samples, num_classes))
    hinge_loss_per_sample = np.zeros(num_samples)
    print("-"*100)
    print(f"Iteration {(iteration + 1) * interval}")
    for i in outlier_out_threshold:
        correct_class = int(y_train[i])
        for j in range(num_classes_iteration):
            if j != correct_class:
                loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
                hinge_losses[i, j] = loss_j
        hinge_loss_per_sample[i] = np.max(hinge_losses[i])
        print(f"Sample {i} has hinge losses = {hinge_loss_per_sample[i]}")
    print("-" * 100)
# è¾“å‡ºæ¯è½®è¿­ä»£çš„æŸå¤±å€¼
print("*" * 100)
for i, losses in enumerate(losses_per_iteration):
    print(f"Iteration {(i + 1)*interval}: Mean hinge loss = {np.mean(losses)}")
print("*" * 100)

# section è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„hingeæŸå¤±
num_samples = X_train.shape[0]
num_classes = svm_model.classes_.shape[0]
hinge_losses = np.zeros((num_samples, num_classes))
hinge_loss_per_sample = np.zeros(num_samples)
for i in range(num_samples):
    correct_class = int(y_train[i])
    for j in range(num_classes):
        if j != correct_class:
            loss_j = max(0, 1 - decision_values[i, correct_class] + decision_values[i, j])
            hinge_losses[i, j] = loss_j
    hinge_loss_per_sample[i] = np.max(hinge_losses[i])
# åˆ¤å®šå¼‚å¸¸ï¼šå‡è®¾é˜ˆå€¼ä¸º 1ï¼Œè¶…è¿‡æ­¤å€¼å³è®¤ä¸ºæ˜¯å¼‚å¸¸
anomalies = np.where(hinge_loss_per_sample > 1)[0]
soft_anomalies = np.where((hinge_loss_per_sample > 0) & (hinge_loss_per_sample <= 1))[0]
correct_class = np.where(hinge_loss_per_sample == 0)[0]
# è¾“å‡ºè®­ç»ƒé›†ä¸­outliersä¸­å…·æœ‰è¾ƒé«˜hingeæŸå¤±çš„æ ·æœ¬ç´¢å¼•
# è®­ç»ƒæ•°æ®ä¸­çš„å¼‚å¸¸å€¼ï¼Œå¯¼è‡´SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬
inter_anomalies = list(set(train_outliers_index) & set(anomalies))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä½†æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®ä¸è¶³é˜ˆå€¼1
inter_soft_anomalies = list(set(train_outliers_index) & set(soft_anomalies))
# æµ‹è¯•æ•°æ®ä¸­çš„æ½œåœ¨å¼‚å¸¸å€¼ï¼Œæœªå¯¼è‡´SVMåˆ†ç±»é”™è¯¯ï¼Œä¸”æ­£ç¡®åˆ†ç±»çš„é¢„æµ‹å€¼ä¸Žå‰©ä½™é”™è¯¯åˆ†ç±»çš„æœ€å¤§é¢„æµ‹å€¼ç›¸å·®è¶…è¿‡é˜ˆå€¼1
inter_correct_class = list(set(train_outliers_index) & set(correct_class))

print("*" * 100)
print("è®­ç»ƒé›†ä¸­SVMå…·æœ‰è¾ƒé«˜hingeæŸå¤±å‡½æ•°çš„æ ·æœ¬æ•°é‡ï¼š", len(anomalies))
# print("è®­ç»ƒé›†ä¸­SVMçš„hingeæŸå¤±å‡½æ•°é«˜äºŽ1çš„æ ·æœ¬ç´¢å¼•ï¼š", anomalies)
print("è®­ç»ƒé›†ä¸­SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ•°é‡ï¼š", len(wrong_classified_indices))
# print("è®­ç»ƒé›†ä¸­SVMåˆ†ç±»é”™è¯¯çš„æ ·æœ¬ç´¢å¼•ï¼š", wrong_classified_indices)
intersection = np.intersect1d(anomalies, wrong_classified_indices)
diff_elements = np.setdiff1d(wrong_classified_indices, intersection)
print("åˆ†ç±»é”™è¯¯çš„æ ·æœ¬ä¸­æœªè¢«hingeé˜ˆå€¼å¤§äºŽ1è¯†åˆ«çš„æ ·æœ¬ç´¢å¼•ï¼š", diff_elements)
print("hingeæŸå¤±å¤§äºŽ1çš„æ ·æœ¬å’Œåˆ†ç±»é”™è¯¯çš„æ ·æœ¬çš„äº¤é›†æ•°é‡ï¼š", len(intersection))
print("è¯¥äº¤é›†æ‰€å åˆ†ç±»é”™è¯¯çš„æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(intersection)/len(wrong_classified_indices))
print("*" * 100)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°é«˜äºŽé˜ˆå€¼1çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_anomalies)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°åœ¨0å’Œé˜ˆå€¼1ä¹‹é—´çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_soft_anomalies)
print("è®­ç»ƒé›†çš„å¼‚å¸¸å€¼ä¸­æŸå¤±å‡½æ•°ä¸º0çš„æ ·æœ¬ç´¢å¼•ï¼š", inter_correct_class)

# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå¤„ç†åŽé‡æ–°è®­ç»ƒSVMæ¨¡åž‹

# SECTION åŽŸå§‹æ•°æ®ä¸­çš„svmåˆ†ç±»å‡†ç¡®åº¦
print("*" * 100)
print("åŽŸå§‹è®­ç»ƒé›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train, svm_model.predict(X_train))))
print("åŽŸå§‹æµ‹è¯•é›†ä¸­SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­hingeæŸå¤±å‡½æ•°é«˜äºŽ1ï¼Œä¸”è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
# ç”Ÿæˆå¸ƒå°”ç´¢å¼•ï¼Œä¸ºè¦åˆ é™¤çš„è¡Œåˆ›å»ºå¸ƒå°”å€¼æ•°ç»„
mask = np.ones(len(X_train), dtype=bool)
mask[inter_anomalies] = False
# ä½¿ç”¨å¸ƒå°”ç´¢å¼•åˆ é™¤é‚£äº›æ—¢è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼ï¼Œåˆä½¿å¾—hingeæŸå¤±é«˜äºŽ1çš„æ ·æœ¬
X_train_split = X_train[mask]
y_train_split = y_train[mask]
# é‡æ–°è®­ç»ƒSVMæ¨¡åž‹
svm_model_split = svm.SVC()
svm_model_split.fit(X_train_split, y_train_split)
print("*" * 100)
print("åŽ»é™¤åŒæ—¶æ»¡è¶³å¼‚å¸¸å’ŒæŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_split, svm_model_split.predict(X_train_split))))
print("åŽ»é™¤åŒæ—¶æ»¡è¶³å¼‚å¸¸å’ŒæŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_split.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­hingeæŸå¤±å‡½æ•°é«˜äºŽ1çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
mask_h = np.ones(len(X_train), dtype=bool)
mask_h[anomalies] = False
X_train_h = X_train[mask_h]
y_train_h = y_train[mask_h]
svm_model_h = svm.SVC()
svm_model_h.fit(X_train_h, y_train_h)
print("*" * 100)
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_h, svm_model_h.predict(X_train_h))))
print("åŽ»é™¤æŸå¤±é«˜äºŽé˜ˆå€¼çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_h.predict(X_test))))
print("*" * 100)

# SECTION èˆå¼ƒæŽ‰SVMè®­ç»ƒæ•°æ®ä¸­è¢«åˆ¤å®šä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬ï¼Œé‡æ–°åœ¨å¤„ç†åŽçš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒSVM
mask_o = np.ones(len(X_train), dtype=bool)
mask_o[train_outliers_index] = False
X_train_o = X_train[mask_o]
y_train_o = y_train[mask_o]
svm_model_o = svm.SVC()
svm_model_o.fit(X_train_o, y_train_o)
print("*" * 100)
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„è®­ç»ƒé›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_train_o, svm_model_o.predict(X_train_o))))
print("åŽ»é™¤åˆ¤å®šä¸ºå¼‚å¸¸çš„æ ·æœ¬åŽçš„æµ‹è¯•é›†SVMåˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, svm_model_o.predict(X_test))))
print("*" * 100)